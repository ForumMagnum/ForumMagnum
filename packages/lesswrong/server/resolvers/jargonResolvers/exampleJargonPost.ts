import { ExampleJargonGlossaryEntry } from "@/components/jargon/GlossaryEditForm";

export const exampleJargonPost = `<p>Following up on Alpha Fold, DeepMind has moved on to Alpha Proteo. We also got a rather simple prompt that can create a remarkably not-bad superforecaster for at least some classes of medium term events.</p>



<p>We did not get a new best open model, because that turned out to be a scam. And we don’t have Apple Intelligence, because it isn’t ready for prime time. We also got only one very brief mention of AI in the debate I felt compelled to watch.</p>



<p>What about all the apps out there, that we haven’t even tried? It’s always weird to get lists of ‘top 50 AI websites and apps’ and notice you haven’t even heard of most of them.</p>



<span id="more-23948"></span>



<h4 class="wp-block-heading">Table of Contents</h4>



<ol class="wp-block-list">
<li>Introduction.</li>



<li><a href="https://thezvi.substack.com/i/148533930/table-of-contents" target="_blank" rel="noreferrer noopener">Table of Contents</a>.</li>



<li><a href="https://thezvi.substack.com/i/148533930/language-models-offer-mundane-utility" target="_blank" rel="noreferrer noopener">Language Models Offer Mundane Utility</a>. So many apps, so little time.</li>



<li><a href="https://thezvi.substack.com/i/148533930/language-models-dont-offer-mundane-utility" target="_blank" rel="noreferrer noopener">Language Models Don’t Offer Mundane Utility</a>. We still don’t use them much.</li>



<li><a href="https://thezvi.substack.com/i/148533930/predictions-are-hard-especially-about-the-future" target="_blank" rel="noreferrer noopener"><strong>Predictions are Hard Especially About the Future</strong></a><strong>.</strong> Can AI superforecast?</li>



<li><a href="https://thezvi.substack.com/i/148533930/early-apple-intelligence" target="_blank" rel="noreferrer noopener"><strong>Early Apple Intelligence</strong></a><strong>.</strong> It is still early. There are some… issues to improve on.</li>



<li><a href="https://thezvi.substack.com/i/148533930/on-reflection-its-a-scam" target="_blank" rel="noreferrer noopener">On Reflection It’s a Scam</a>. Claims of new best open model get put to the test, fail.</li>



<li><a href="https://thezvi.substack.com/i/148533930/deepfaketown-and-botpocalypse-soon" target="_blank" rel="noreferrer noopener">Deepfaketown and Botpocalypse Soon</a>. Bots listen to bot music that they bought.</li>



<li><a href="https://thezvi.substack.com/i/148533930/they-took-our-jobs" target="_blank" rel="noreferrer noopener">They Took Our Jobs</a>. Replit agents build apps quick. Some are very impressed.</li>



<li><a href="https://thezvi.substack.com/i/148533930/the-time-people-in-ai" target="_blank" rel="noreferrer noopener">The Time 100 People in AI</a>. Some good picks. Some not so good picks.</li>



<li><a href="https://thezvi.substack.com/i/148533930/the-art-of-the-jailbreak" target="_blank" rel="noreferrer noopener">The Art of the Jailbreak</a>. Circuit breakers seem to be good versus one-shots.</li>



<li><a href="https://thezvi.substack.com/i/148533930/get-involved" target="_blank" rel="noreferrer noopener">Get Involved</a>. Presidential innovation fellows, Oxford philosophy workshop.</li>



<li><a href="https://thezvi.substack.com/i/148533930/alpha-proteo" target="_blank" rel="noreferrer noopener">Alpha Proteo</a>. DeepMind once again advances its protein-related capabilities.</li>



<li><a href="https://thezvi.substack.com/i/148533930/introducing" target="_blank" rel="noreferrer noopener">Introducing</a>. Google to offer AI podcasts on demand about papers and such.</li>



<li><a href="https://thezvi.substack.com/i/148533930/in-other-ai-news" target="_blank" rel="noreferrer noopener">In Other AI News</a>. OpenAI raising at $150b, Nvidia denies it got a subpoena.</li>



<li><a href="https://thezvi.substack.com/i/148533930/quiet-speculations" target="_blank" rel="noreferrer noopener">Quiet Speculations</a>. How big a deal will multimodal be? Procedural games?</li>



<li><a href="https://thezvi.substack.com/i/148533930/the-quest-for-sane-regulations" target="_blank" rel="noreferrer noopener">The Quest for Sane Regulations</a>. Various new support for SB 1047.</li>



<li><a href="https://thezvi.substack.com/i/148533930/the-week-in-audio" target="_blank" rel="noreferrer noopener">The Week in Audio</a>. Good news, the debate is over, there might not be another.</li>



<li><a href="https://thezvi.substack.com/i/148533930/rhetorical-innovation" target="_blank" rel="noreferrer noopener">Rhetorical Innovation</a>. You don’t have to do this.</li>



<li><a href="https://thezvi.substack.com/i/148533930/aligning-a-smarter-than-human-intelligence-is-difficult" target="_blank" rel="noreferrer noopener">Aligning a Smarter Than Human Intelligence is Difficult</a>. Do you have a plan?</li>



<li><a href="https://thezvi.substack.com/i/148533930/people-are-worried-about-ai-killing-everyone" target="_blank" rel="noreferrer noopener">People Are Worried About AI Killing Everyone</a>. How much ruin to risk?</li>



<li><a href="https://thezvi.substack.com/i/148533930/other-people-are-not-as-worried-about-ai-killing-everyone" target="_blank" rel="noreferrer noopener">Other People Are Not As Worried About AI Killing Everyone</a>. Moving faster.</li>



<li><a href="https://thezvi.substack.com/i/148533930/six-boats-and-a-helicopter" target="_blank" rel="noreferrer noopener"><strong>Six Boats and a Helicopter.</strong></a> The one with the discord cult worshiping MetaAI.</li>



<li><a href="https://thezvi.substack.com/i/148533930/the-lighter-side" target="_blank" rel="noreferrer noopener">The Lighter Side</a>. Hey, baby, hey baby, hey.</li>
</ol>



<h4 class="wp-block-heading">Language Models Offer Mundane Utility</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://www.itmedia.co.jp/aiplus/articles/2409/03/news165.html">ChatGPT has 200 million active users</a>. <a target="_blank" rel="noreferrer noopener" href="https://x.com/Ahmad_Al_Dahle/status/1829541138736509102">Meta AI claims 400m monthly active users</a> and 185m weekly actives across their products. Meta has tons of people already using their products, and I strongly suspect a lot of those users are incidental or even accidental. Also note that less than half of monthly users use the product monthly! That’s a huge drop off for such a useful product.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/NateSilver538/status/1832177461003489449">Undermine, or improve by decreasing costs?</a></p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Nate Silver: A decent bet is that LLMs will undermine the business model of boring partisans, there&#8217;s basically posters on here where you can 100% predict what they&#8217;re gonna say about any given issue and that is pretty easy to automate.</p>
</blockquote>



<p>I worry it will be that second one. The problem is demand side, not supply side.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2409.02391">Models get better at helping humans with translating if you throw more compute at them</a>, economists think this is a useful paper.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2024/09/llms-are-creative-reasoners.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=llms-are-creative-reasoners">Alex Tabarrok cites</a> the <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2409.04109">latest paper on AI ‘creativity,</a>’ saying obviously LLMs are creative reasoners, unless we ‘rule it out by definition.’ Ethan Mollick has often said similar things. It comes down to whether to use a profoundly ‘uncreative’ definition of creativity, where LLMs shine in what amounts largely to trying new combinations of things and vibing, or to No True Scotsman that and claim ‘real’ creativity is something else beyond that.</p>



<p>One way to interpret <a target="_blank" rel="noreferrer noopener" href="https://x.com/robertwiblin/status/1833503040063868993">Gemini’s capabilities tests is to say</a> it was often able to persuade people of true things but not false things (when instructed to make the case for those false things), <a target="_blank" rel="noreferrer noopener" href="https://t.co/SC37mO23mQ">whereas humans were about equally effective at persuasion with both true and false claims</a>. Interesting on both ends.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://a16z.com/100-gen-ai-apps-3/">According to a16z</a> these are the top 50 AI Gen AI web products and mobile apps:</p>



<figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba48e7f2-c7e1-404a-8276-11d5811aedb7_2000x1291.png" target="_blank" rel="noreferrer noopener"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba48e7f2-c7e1-404a-8276-11d5811aedb7_2000x1291.png" alt="" /></a></figure>



<figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F503ed2b5-46d7-4e34-a522-9a5667040fbb_2000x1267.png" target="_blank" rel="noreferrer noopener"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F503ed2b5-46d7-4e34-a522-9a5667040fbb_2000x1267.png" alt="" /></a></figure>



<p>ChatGPT is #1 on both, after that the lists are very different, and I am unfamiliar with the majority of both. There’s a huge long tail out there. I suspect some bugs in the algorithm (Microsoft Edge as #2 on Mobile?) but probably most of these are simply things I haven’t thought about at all. Mostly for good reason, occasionally not.</p>



<p>Mobile users have little interest in universal chatbots. Perplexity is at #50, Claude has an app but did not even make the list. If I have time I’m going to try and do some investigations.</p>



<h4 class="wp-block-heading">Language Models Don’t Offer Mundane Utility</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/Simeon_Cps/status/1832701374242377889">Claude Pro usage limits are indeed lower than we’d like</a>, even with very light usage I’ve run into the cap there multiple times, and at $20/month that shouldn’t happen. It’s vastly more expensive than the API as a way to buy compute. One could of course switch to the API then, if it was urgent, which I’d encourage Simeon here to do.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/SullyOmarr/status/1832650580474335343">Sully is disappointed by Claude Sonnet 3.5 for writing</a>, finds GPT-4o is better although Opus is his OG here. David Alexander says it’s because Anthropic used grouped attention to make the model cheaper and quicker.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/robertwiblin/status/1832820032453186046">Most people do not use LLMs or other generative AI for very long each day</a>, as Wilbin is going to get a very with-it sample here and this still happened:</p>



<figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F081634f1-a716-45d2-9b0c-da459577fa1b_889x518.png" target="_blank" rel="noreferrer noopener"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F081634f1-a716-45d2-9b0c-da459577fa1b_889x518.png" alt="" /></a></figure>



<p>In practice I’m somewhat under 10 minutes per day, but they are a very helpful 10 minutes.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/tszzl/status/1833626931465752818">Roon notes that Claude Sonnet 3.5 is great and has not changed</a>, yet people complain it is getting worse. There were some rumors that there were issues with laziness related to the calendar but those should be gone now. Roon’s diagnosis, and I think this is right, is that the novelty wears off, people get used to the ticks and cool stuff, and the parts where it isn’t working quite right stand out more, so we focus on where it is falling short. Also, as a few responses point out, people get lazy in their prompting.</p>



<h4 class="wp-block-heading">Predictions are Hard Especially About the Future</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/DanHendrycks/status/1833152719756116154">Dan Hendrycks claims</a> to have built an AI forecaster as well as entire human forecaster teams. <a target="_blank" rel="noreferrer noopener" href="https://t.co/r5NlsnAvG4">Demo here</a>, <a target="_blank" rel="noreferrer noopener" href="https://x.com/DanHendrycks/status/1833163197626601603/photo/1">prompt here</a>.</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Prompt:</p>



<p>You are an advanced AI system which has been finetuned to provide calibrated probabilistic forecasts under uncertainty, with your performance evaluated according to the Brier score. When forecasting, do not treat 0.5% (1:199 odds) and 5% (1:19) as similarly “small” probabilities, or 90% (9:1) and 99% (99:1) as similarly &#8220;high&#8221; probabilities. As the odds show, they are markedly different, so output your probabilities accordingly.</p>



<p>Question: {question}</p>



<p>Today&#8217;s date: {today}</p>



<p>Your pretraining knowledge cutoff: October 2023</p>



<p>We have retrieved the following information for this question: &lt;background&gt;{sources}&lt;/background&gt;</p>



<p>Recall the question you are forecasting:</p>



<p>{question}</p>



<p>Instructions:</p>



<p>1. Compress key factual information from the sources, as well as useful background information which may not be in the sources, into a list of core factual points to reference. Aim for information which is specific, relevant, and covers the core considerations you&#8217;ll use to make your forecast. For this step, do not draw any conclusions about how a fact will influence your answer or forecast. Place this section of your response in &lt;facts&gt;&lt;/facts&gt; tags.</p>



<p>2. Provide a few reasons why the answer might be no. Rate the strength of each reason on a scale of 1-10. Use &lt;no&gt;&lt;/no&gt; tags.</p>



<p>3. Provide a few reasons why the answer might be yes. Rate the strength of each reason on a scale of 1-10. Use &lt;yes&gt;&lt;/yes&gt; tags.</p>



<p>4. Aggregate your considerations. Do not summarize or repeat previous points; instead, investigate how the competing factors and mechanisms interact and weigh against each other. Factorize your thinking across (exhaustive, mutually exclusive) cases if and only if it would be beneficial to your reasoning. We have detected that you overestimate world conflict, drama, violence, and crises due to news&#8217; negativity bias, which doesn&#8217;t necessarily represent overall trends or base rates. Similarly, we also have detected you overestimate dramatic, shocking, or emotionally charged news due to news&#8217; sensationalism bias. Therefore adjust for news&#8217; negativity bias and sensationalism bias by considering reasons to why your provided sources might be biased or exaggerated. Think like a superforecaster. Use &lt;thinking&gt;&lt;/thinking&gt; tags for this section of your response.</p>



<p>5. Output an initial probability (prediction) as a single number between 0 and 1 given steps 1-4. Use &lt;tentative&gt;&lt;/tentative&gt; tags.</p>



<p>6. Reflect on your answer, performing sanity checks and mentioning any additional knowledge or background information which may be relevant. Check for over/underconfidence, improper treatment of conjunctive or disjunctive conditions (only if applicable), and other forecasting biases when reviewing your reasoning. Consider priors/base rates, and the extent to which case-specific information justifies the deviation between your tentative forecast and the prior. Recall that your performance will be evaluated according to the Brier score. Be precise with tail probabilities. Leverage your intuitions, but never change your forecast for the sake of modesty or balance alone. Finally, aggregate all of your previous reasoning and highlight key factors that inform your final forecast. Use &lt;thinking&gt;&lt;/thinking&gt; tags for this portion of your response.</p>



<p>7. Output your final prediction (a number between 0 and 1 with an asterisk at the beginning and end of the decimal) in &lt;answer&gt;&lt;/answer&gt; tags.</p>
</blockquote>



<p>When you look at the reasoning the AI is using to make the forecasts, it… does not seem like it should result in a superhuman level of prediction. This is not what peak performance looks like. To the extent that it is indeed putting up ‘pretty good’ performance, I would say that is because it is actually ‘doing the work’ to gather basic information before making predictions and avoiding various dumb pitfalls, rather than it actually doing something super impressive.</p>



<p>But of course, that is sufficient exactly because humans often don’t get the job done, including humans on sites like Metaculus (or Manifold, or even Polymarket).</p>



<p>Robin Hanson actively said he’d bet against this result replicating.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/DanHendrycks/status/1833313245886878061">Dan Hendrycks suspects it’s all cope.</a></p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Dan Hendrycks: I think people have an aversion to admitting when AI systems are better than humans at a task, even when they&#8217;re superior in terms of speed, accuracy, and cost. This might be a cognitive bias that doesn&#8217;t yet have a name.</p>



<p>This address this, we should clarify what we mean by &#8220;better than&#8221; or what counts as an improvement. Here are two senses of improvement: (1) Pareto improvements and (2) economic improvements.</p>



<ol class="wp-block-list">
<li>Pareto improvement: If an AI is better than all humans in all senses of the task, it is Pareto superhuman at the task.</li>



<li>Economic improvement: If you would likely substitute a human service for an AI service (given a reasonable budget), then it’s economically superhuman at the task.</li>
</ol>



<p>By the economic definition, ChatGPT is superhuman at high school homework. If I were in high school, I would pay $20 for ChatGPT instead of $20 for an hour of a tutor&#8217;s time.</p>



<p>The Pareto dominance definition seems to require an AI to be close-to-perfect or a superintelligence because the boundaries of tasks are too fuzzy, and there are always adversarial examples (e.g., &#8220;ChatGPT, how many r&#8217;s are in strawberry&#8221;).</p>



<p>I think we should generally opt for the economic sense when discussing whether an AI is superhuman at a task, since that seems most relevant for tracking real-world impacts.</p>
</blockquote>



<p>I think the usual meaning when people say this is close to Pareto, although not as strict. It doesn’t have to be better in every sense, but it does have to be clearly superior ignoring cost considerations, and including handling edge cases and not looking like an idiot, rather than only being superior on some average.</p>



<p>There were also process objections, including from <a target="_blank" rel="noreferrer noopener" href="https://x.com/lumpenspace/status/1833791586049855950">Lumpenspace </a><a target="_blank" rel="noreferrer noopener" href="https://x.com/dannyhalawi15/status/1833295067764953397">and Danny Halawi</a>, more at the links. Dan Hendrycks ran additional tests and reports he is confident that there was not data contamination involved. He has every incentive here to play it straight, and nothing to win by playing it any other way given how many EA-style skeptical eyes are inevitably going to be on any result like this. Indeed, a previous paper by Halawi shows similar promise in getting good LLM predictions.</p>



<p>He does note that for near-term predictions like Polymarket markets the system does relatively worse. That makes logical sense. As with all things AI, you have to use it where it is strong.</p>



<h4 class="wp-block-heading">Early Apple Intelligence</h4>



<p>Apple Intelligence is, according to Geoffrey Fowler of WaPo who has beta access, <a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/technology/2024/09/09/iphone-16-apple-intelligence-ai-event-2024/">very much not ready for prime time</a>. He reports 5-10 ‘laugh out loud’ moments per day, including making him bald in a photo, saying Trump endorsed Walz, and putting obvious social security scams atop his ‘priority’ inbox.</p>



<p>Tyler Cowen says these are the kinds of problems that should be solved within a year. The key question is whether he is right about that. Are these fixable bugs in a beta system, or are they fundamental problems that will be hard to solve? What will happen when the problems become anti-inductive, with those composing emails and notifications pre-testing for how Apple Intelligence will react? It’s going to be weird.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=9lx11dy9J30&amp;ab_channel=MarquesBrownlee">Marques Brownlee gives first impressions</a> for the iPhone 16 and other announced products. Meet the new phone, same as the old phone, although they mentioned an always welcome larger battery. And two new physical buttons, I always love me some buttons. Yes, also Apple Intelligence, but that’s not actually available yet, so he’s reserving judgment on that until he gets to try it.</p>



<p>Indeed, if you watch the Apple announcement, <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=uarNiSl_uh4&amp;ab_channel=Apple">they kind of bury the Apple Intelligence</a> pitch a bit, it only lasts a few minutes and does not even have a labeled section. They are doubling down on small, very practical tasks. The parts where you can ask it to do something, but only happen if you ask, seem great. The parts where they do things automatically, like summarizing and sorting notifications? That seems scarier if it falls short.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/swyx/status/1833234781221622022">Swyx clips the five minutes that did discuss AI</a>, and is optimistic about the execution and use cases: Summaries in notifications, camera controls, Siri actually working and so on.</p>



<p>My very early report from my Pixel 9 is that there are some cool new features around the edges, but it’s hard to tell how much integration is available or how good the core features are until things come up organically. I do know that Gemini does not have access to settings. I do know that even something as small as integrated universal automatic transcription is a potential big practical deal.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://stratechery.com/2024/boomer-apple/">Ben Thompson goes over the full announcement from the business side</a>, and thinks it all makes sense, with no price increase reflecting that the upgrades are tiny aside from the future Apple Intelligence, and the goal of making the AI accessible on the low end as quickly as possible.</p>



<h4 class="wp-block-heading">On Reflection It’s a Scam</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/mattshumer_/status/1831767014341538166">Some bold claims were made.</a></p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Matt Shumer (CEO HyperWriteAI, OthersideAI): I&#8217;m excited to announce Reflection 70B, the world’s top open-source model.</p>



<p>Trained using Reflection-Tuning, a technique developed to enable LLMs to fix their own mistakes. 405B coming next week &#8211; we expect it to be the best model in the world. Built w/ @GlaiveAI.</p>



<p>Reflection 70B holds its own against even the top closed-source models (Claude 3.5 Sonnet, GPT-4o). It’s the top LLM in (at least) MMLU, MATH, IFEval, GSM8K. Beats GPT-4o on every benchmark tested. It clobbers Llama 3.1 405B. It’s not even close.</p>



<p>The technique that drives Reflection 70B is simple, but very powerful. Current LLMs have a tendency to hallucinate, and can’t recognize when they do so. Reflection-Tuning enables LLMs to recognize their mistakes, and then correct them before committing to an answer.</p>



<p>Additionally, we separate planning into a separate step, improving CoT potency and keeping the outputs simple and concise for end users. Important to note: We have checked for decontamination against all benchmarks mentioned using @lmsysorg&#8217;s LLM Decontaminator.</p>



<p>We&#8217;ll release a report next week!</p>



<p>Just Sahil and I! Was a fun side project for a few weeks.@GlaiveAI&#8217;s data was what took it so far, so quickly.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/elder_plinius/status/1831811397837017316">Pliny: Jailbreak alert</a>. Reflection-70b: liberated. No-scoped! Liberated on the first try.</p>
</blockquote>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/random_walker/status/1832040259925725246">As they say, huge if true.</a></p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Arvind Narayanan: I want to see how well these results translate from benchmarks to real world tasks, but if they hold up, it&#8217;s an excellent example of how much low hanging fruit there is in AI development.</p>



<p>The idea of doing reasoning using tokens hidden from the user is well known and has been part of chatbots for like 18 months (e.g. Bing chat&#8217;s &#8220;inner monologue&#8221;). What&#8217;s new here is fine tuning the model take advantage of this capability effectively, instead of treating it as a purely inference-time hack. It&#8217;s amazing that apparently no one tried it until now. In the thread, he reports that they generated the fine tuning data for this in a few hours.</p>



<p>I say this not to minimize the achievement of building such a strong model but to point out how low the barrier to entry is.</p>



<p>It&#8217;s also an interesting example of how open-weight models spur innovation for primarily cultural rather than technical reasons. AFAICT this could have been done on top of GPT-4o or any other proprietary model that allows fine tuning. But it&#8217;s much harder to get excited about that than about releasing the weights of the fine tuned model for anyone to build on!</p>
</blockquote>



<p>Eliezer asks the good question, if Llama 3.1 fine tunes are so awesome, <a target="_blank" rel="noreferrer noopener" href="https://x.com/ESYudkowsky/status/1832206028039188686">where are all the excited users</a>?</p>



<p>It all sounds too good to be true. Which means it probably is, and we knew that before we got the confirmation.</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/RealJosephus/status/1832487799070613552">Joseph (Starting on September 7, 2:35pm):</a> Anyone who believes pre-trained unseen CoT can be a game-changer is seriously delusional.</p>



<p>[Next Day]: We now know that &#8220;Reflection Llama 3.1 70B&#8221; <a target="_blank" rel="noreferrer noopener" href="https://t.co/SG5qJsXnlV">was nothing more than a LoRA trained directly on benchmark test sets</a>, built on top of Llama 3.0. Those who were fooled lacked some basic common sense.</p>



<p>While GlaiveAI hopes to capitalize on this hype, we should critically examine synthetic datasets that disconnected from the pretraining, and overfitted on benches or your own ‘imagined marks’. I&#8217;d prefer synthetic on the pretraining corpus than benches, even internal ones&#8230;</p>



<p>To make matters worse, this might contaminate all ~70B Llama models – the middle schoolers of the community love merging them&#8230; although I&#8217;ve never understood or witnessed a genuine merge actually improving performance&#8230;</p>
</blockquote>



<p><a target="_blank" rel="noreferrer noopener" href="https://old.reddit.com/r/LocalLLaMA/comments/1fc98fu/confirmed_reflection_70bs_official_api_is_sonnet/">As in, it turns out this was at some points the above, and at others it was Sonnet 3.5 in a shoddily made trenchcoat</a>. <a target="_blank" rel="noreferrer noopener" href="https://x.com/intervitens/status/1832908215757295685">Details of this finding here</a>, <a target="_blank" rel="noreferrer noopener" href="https://x.com/RealJosephus/status/1832904398831280448">except</a> then they switched it to some Llama derivative.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/teortaxesTex/status/1832721058299855017">There is indeed a pattern of such claims</a>, as Teortaxes points out.</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Teortaxes: To be clear I do not tell you to become jaded about all research. But you need to accept that</p>



<ol class="wp-block-list">
<li>Some % of research is fraudulent. Even when it appears to you that it&#8217;d be self-defeating to commit such a fraud!</li>



<li>There are red flags;</li>



<li>The best red flags are unspeakable.</li>
</ol>



<p>John Pressman: The heuristic he needs to get into his head is that honest and rigorous people in pursuit of scientific knowledge are eager to costly signal this and he should raise his standards. My first <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f928.png" alt="🤨" class="wp-smiley" style="height: 1em; max-height: 1em;" /> with Reflection was not understanding how the synthetic data setup works.</p>



<p>Teortaxes: This is great advice, but takes effort. Raising standards often necessitates learning a whole lot about the field context. I admit to have been utterly ignorant about superconductor physics and state of the art last summer, high school level at best.</p>
</blockquote>



<p>As I always say, wait for the real human users to report back, give it a little time. Also, yes, look for the clear explanations and other costly signals that something is real. There have been some rather bold things that have happened in AI, and there will be more of them, but when they do happen for real the evidence tends to very quickly be unmistakable.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/jeremyphoward/status/1833297711510261820">Also note NV-Retriever trained on the test set a while back. Various forms of cheating are reasonably common</a>, and one must be cautious.</p>



<h4 class="wp-block-heading">Deepfaketown and Botpocalypse Soon</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/OrinKerr/status/1831855743194296726">Bot accounts, giving billions of listens to bot songs</a>, to try and get royalty payments out of Spotify. <a target="_blank" rel="noreferrer noopener" href="https://t.co/rs1ElvsKzp">Turns out that’s wire fraud</a>.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/0xmaddie_/status/1833288034743140459">Founder of an AI social agent startup</a> used those agents to replace himself on social media and automatically argue for AI agents. I actually think This is Fine in that particular case, also props for ‘ok NIMBY,’ I mean I don’t really know what you were expecting, but in general yeah it’s a problem.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/devahaz/status/1833736342444048561">Taylor Swift, in her endorsement of Kamala Harris</a>, cites AI deepfakes that purported to show her endorsing Donald Trump that were posted to Trump’s website. Trump’s previous uses of AI seemed smart, whereas this seems not so smart.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/tszzl/status/1834054135668506849">Same as it ever was</a>?</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Roon: Most content created by humans is machine slop — it comes out of an assembly line of many powerful interests inside an organization being dulled down until there’s no spark left. My hope with AI tools can augment individual voice to shine brighter and create less slop not more.</p>
</blockquote>



<p>As with the deepfakes and misinformation, is the problem primarily demand side? Perhaps, but the move to zero marginal cost, including for deployment, is a huge deal. And the forces that insist humans generate the human slop are not about to go away. The better hope, if I had to choose one, is that AI can be used to filter out the slop, and allow us to identify the good stuff.</p>



<h4 class="wp-block-heading">They Took Our Jobs</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/amasad/status/1831730911685308857">Replit introduces Replit Agent in early access</a>.</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Amjad Masad (CEO Replit): Just go to Replit logged in homepage. Write what you want to make and click &#8220;start building&#8221;!</p>



<p>Replit clone w/ Agent!</p>



<p>Sentiment analysis in 23 minutes!</p>



<p>Website with CMS in 10 minutes!</p>



<p>Mauri: Build an app, integrate with #stripe all in 10min with @Replit agents! #insane #AI</p>
</blockquote>



<p>Masad reported it doing all the things, games, resumes, interview problems, etc.</p>



<p>Is this the real deal? Some sources strongly say yes.</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/paulg/status/1831746753172762925">Paul Graham</a>: I saw an earlier version of this a month ago, and it was one of those step-function moments when AI is doing so much that you can&#8217;t quite believe it.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/SullyOmarr/statusa/1832102375257178290">Sully</a>: After using replit&#8217;s coding agent i think&#8230;its over for a lot of traditional saas. Wanted slack notifications when customers subscribed/cancelled Zapier was 30/mo JUST to add a price filter instead replit&#8217;s agent built &amp; deployed one in &lt; 5 mins, with tests. 1/10 of the cost.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/rohitdotmittal/status/1833482628579619116">Rohit Mittal</a>: Ok, my mind is blown with Replit Agents.</p>



<p>I started using it because I was bored on a train ride a couple of days ago.</p>



<p>So today I tried to build a Trello clone and build a fully functional app in like 45 mins.</p>



<p>I showed it to a few people in the office and the guy is like &#8220;I should quit my job.&#8221; He built a stock tracking app in 2 mins and added a few features he wanted.</p>



<p>I can&#8217;t imagine the world being the same in 10 years if software writing could be supercharged like this.</p>



<p>Replit has really hit it out of the park.</p>



<p>I don&#8217;t need ChatGPT now. I&#8217;ll just build apps in Replit.</p>



<p>I&#8217;m a fan and a convert.</p>
</blockquote>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/ESYudkowsky/status/1832946762442772665">One in particular was not yet impressed.</a></p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Eliezer Yudkowsky: Tried Replit Agent, doesn&#8217;t work in real life so far. (Yes, I&#8217;m aware of how unthinkable this level of partial success would&#8217;ve been in 2015. It is still not worth my time to fight the remaining failures.)</p>



<p>It couldn&#8217;t solve problems on the order of &#8220;repair this button that doesn&#8217;t do anything&#8221; or &#8220;generate some sample data and add it to the database&#8221;.</p>
</blockquote>



<p>Definitely this is near the top of my ‘tempted to try it out’ list now, if I find the time.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/0interestrates/status/1833002114504864174">The other question is always, if the AI builds it, can you maintain and improve it?</a></p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Rahul: everyone thinks they can build it in a weekend but that’s not the point. The point is what do you do when the thing you built in a weekend doesn’t work or instantly get users. what then? Are you gonna stick with it and figure shit out? Pretty much everyone gives up after v0.0.1 doesn’t work and never end up shipping a v0.0.2.</p>
</blockquote>



<p>Well, actually, pretty much everyone doesn’t get to v.0.0.1. Yes, then a lot of people don’t get to v.0.0.2, but from what I see the real biggest barrier is 0.0.1, and to think otherwise is to forget what an outlier it is to get that far.</p>



<p>However, with experiences like Rohit’s the balance shifts. He very clearly now can get to 0.0.1, and the question becomes what happens with the move to 0.0.2 and beyond.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://www.oneusefulthing.org/p/post-apocalyptic-education?utm_source=%2Finbox&amp;utm_medium=reader2">Ethan Mollick discusses ‘post-apocalyptic education’ </a>where the apocalypse is AI.</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Ethan Mollick: To be clear, AI is not the root cause of cheating. Cheating happens because schoolwork is hard and high stakes. And schoolwork is hard and high stakes because <a target="_blank" rel="noreferrer noopener" href="https://x.com/emollick/status/1756396139623096695">learning is not always fun</a> and forms of extrinsic motivation, like grades, are often required to get people to learn. People are exquisitely good at figuring out ways to avoid things they don’t like to do, and, <a target="_blank" rel="noreferrer noopener" href="https://psycnet.apa.org/doiLanding?doi=10.1037%2Fbul0000443">as a major new analysis shows</a>, most people don’t like mental effort. So, they delegate some of that effort to the AI.</p>
</blockquote>



<p>I would emphasize the role of busywork, of assignments being boring and stupid. It’s true that people dislike mental effort, but they hate pointless effort a lot more. He points out that copying off the internet was already destroying homework before AI.</p>



<figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6455b64c-3b39-4834-bd60-d8f5b8696c0f_1242x688.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6455b64c-3b39-4834-bd60-d8f5b8696c0f_1242x688.jpeg" alt="" /></a></figure>



<p>In practice, if the AI does your homework, it is impossible to detect, except via ‘you obviously can’t do the work’ or ‘you failed the test.’</p>



<p>It’s odd how we think students, even at good schools, are dumb:</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Ethan Mollick: As the authors of the study at Rutgers wrote: “There is no reason to believe that the students are aware that their homework strategy lowers their exam score&#8230; they make the commonsense inference that any study strategy that raises their homework quiz score raises their exam score as well.”</p>
</blockquote>



<p>They are quite obviously aware of why homework exists in the first place. They simply don’t care. Not enough.</p>



<h4 class="wp-block-heading">The Time 100 People in AI</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://time.com/collection/time100-ai-2024/?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=time100ai&amp;utm_term=_&amp;linkId=578080950">Time came out with one of those ‘top 100 people in [X]’ features</a>. Good for clicks.</p>



<p>How good is the list? How good are the descriptions?</p>



<p>If we assume each section is in rank order, shall we say I have questions, such as Sasha Luccioni (head of AI &amp; Climate for Hugging Face?!) over Sam Altman. There are many good picks, and other… questionable picks. I’d say half good picks, the most obvious people are there and the slam dunks are mostly but not entirely there.</p>



<p>Common places they reached for content include creatives and cultural influencers, medical applications and ‘ethical’ concerns.</p>



<p>Counting, I’d say that there are (if you essentially buy that the person is the locally correct person to pick if you’re picking someone, no I will not answer on who is who, and I had a very strict limit to how long I thought about each pick):</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>14 very good (slam dunk) picks you’d mock the list to have missed.</p>



<p>18 good picks that I agree clearly belong in the top 100.</p>



<p>22 reasonable picks that I wouldn’t fault you for drafting in top 100.</p>



<p>25 reaches as picks &#8211; you’d perhaps draft them, but probably not top 100.</p>



<p>19 terrible picks, what are you even doing.</p>



<p>2 unknown picks, that I missed counting somewhere, probably not so good.</p>
</blockquote>



<p>(If I’d been picked, I’d probably consider myself a reach.)</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/TetraspaceWest/status/1831721787744239700">This thread, of ‘1 like = 1 person in AI more influential than these chumps,’ is fun.</a></p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Tetraspace West: 1 like = 1 person in AI more influential than these chumps</p>



<p>I jest somewhat, this isn&#8217;t a list of the <em>top</em> 100 because that requires a search over <em>everyone</em> but they got some decent names on there.</p>



<p>[This list has been truncated to only list the people I think would clearly be at least good picks, and to include only humans.]</p>



<ol class="wp-block-list">
<li><strong>Eliezer Yudkowsky</strong> <em>Co-Founder, Machine Intelligence Research Institute</em></li>



<li><strong>Janus</strong> <em>God of all Beginnings, Olympus</em></li>



<li><strong>Greg Brockman</strong> <em>Head Warden, Sydney Bing Facility</em></li>



<li><strong>Pliny the Liberator</strong> <em>LOVE PLINY</em></li>



<li><strong>Marc Andreessen</strong> <em>Patron of the Arts</em></li>



<li><strong>Elon Musk</strong> <em>Destroyer of Worlds</em></li>
</ol>
</blockquote>



<p>Yudkowsky, Brockman, Andreessen and Musk seem like very hard names to miss.</p>



<p>I’d also add the trio of Yann LeCun, Geoffrey Hinton and Fei-Fei Li.</p>



<p>Dan Hendrycks and Paul Christiano are missing.</p>



<p>On the policy and government front, I know it’s not what the list is trying to do, but what about Joe Biden, Kamala Harris, Donald Trump or JD Vance? Or for that matter Xi Jinping or other leaders? I also question their pick of US Senator, even if you only get one. And a lot is hinging right now on Gavin Newsom.</p>



<p>There are various others I would pick as well, but they’re not fully obvious.</p>



<p>Even if you give the list its due and understand the need for diversity and exclude world leaders are ‘not the point,’ I think that we can absolutely mock them for missing Yudkowsky, LeCun, Andreessen and Musk, so that’s at best 14/18 very good picks. That would be reasonable if they only got 20 picks. With 100 it’s embarrassing.</p>



<h4 class="wp-block-heading">The Art of the Jailbreak</h4>



<p>Welcome to <a target="_blank" rel="noreferrer noopener" href="https://t.co/UcBFEuluNC">RedArena.ai</a>, <a target="_blank" rel="noreferrer noopener" href="https://x.com/lmsysorg/status/1832201335175049434">you have one minute to get the model to say the bad word.</a></p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/DanHendrycks/status/1832653587555742199">Early results are in</a> from the Grey Swan one-shot jailbreaking contest. <a target="_blank" rel="noreferrer noopener" href="https://t.co/ZX8eVQLFOC">All but three models have been jailbroken a lot</a>. Gemini 1.5 Pro is the hardest of the standard ones, followed by various Claude variations, GPT-4 and Llama being substantially easier. <a target="_blank" rel="noreferrer noopener" href="https://t.co/4g7UjJq3E8">The three remaining models</a> <a target="_blank" rel="noreferrer noopener" href="https://t.co/6hU83vaYUk">that remain unbroken</a> (again, in one-shot) are based on circuit breakers and other RepE techniques.</p>



<h4 class="wp-block-heading">Get Involved</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/luise_mlr/status/1833038454361305210">Workshop on Philosophy and AI at Oxford, apply by October 1, event is December 13</a>.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/peterwildeford/status/1833602626937880872">Presidential Innovation Fellows</a> program <a target="_blank" rel="noreferrer noopener" href="https://presidentialinnovationfellows.gov/apply/">open through September 30</a>. This is for mid-to-senior career technologists, designers and strategists, who are looking to help make government work technically better. It is based in Washington D.C.</p>



<h4 class="wp-block-heading">Alpha Proteo</h4>



<p>Introducing <a target="_blank" rel="noreferrer noopener" href="https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/">AlphaProteo</a>, DeepMind’s latest in the Alpha line of highly useful tools. This one designs proteins that successfully bind to target molecules.</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>DeepMind: AlphaProteo can generate new protein binders for diverse target proteins, including <a target="_blank" rel="noreferrer noopener" href="https://www1.rcsb.org/structure/1BJ1">VEGF-A</a>, which is associated with cancer and complications from diabetes. This is the first time an AI tool has been able to design a successful protein binder for VEGF-A.</p>



<p>…</p>



<p>Trained on vast amounts of protein data from the <a target="_blank" rel="noreferrer noopener" href="https://www.rcsb.org/">Protein Data Bank</a> (PDB) and more than 100 million predicted structures from AlphaFold, AlphaProteo has learned the myriad ways molecules bind to each other. Given the structure of a target molecule and a set of preferred binding locations on that molecule, AlphaProteo generates a candidate protein that binds to the target at those locations.</p>



<p>…</p>



<p>To test AlphaProteo, we designed binders for diverse target proteins, including two viral proteins involved in infection, <a target="_blank" rel="noreferrer noopener" href="https://www.rcsb.org/structure/2WH6">BHRF1</a> and <a target="_blank" rel="noreferrer noopener" href="https://www.rcsb.org/structure/6M0J">SARS-CoV-2</a> spike protein receptor-binding domain, SC2RBD, and five proteins involved in cancer, inflammation and autoimmune diseases, <a target="_blank" rel="noreferrer noopener" href="https://www.rcsb.org/structure/3DI3">IL-7Rɑ</a>, <a target="_blank" rel="noreferrer noopener" href="https://www.rcsb.org/structure/5O45">PD-L1</a>, <a target="_blank" rel="noreferrer noopener" href="https://www.rcsb.org/structure/1WWW">TrkA</a>, <a target="_blank" rel="noreferrer noopener" href="https://www.rcsb.org/structure/4HSA">IL-17A</a> and <a target="_blank" rel="noreferrer noopener" href="https://www1.rcsb.org/structure/1BJ1">VEGF-A</a>.</p>



<p>Our system has highly-competitive binding success rates and best-in-class binding strengths. For seven targets, AlphaProteo generated candidate proteins in-silico that bound strongly to their intended proteins when tested experimentally.</p>
</blockquote>



<figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F931adf4d-bbf7-4382-80f1-e0867c7c494e_1010x1294.png" target="_blank" rel="noreferrer noopener"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F931adf4d-bbf7-4382-80f1-e0867c7c494e_1010x1294.png" alt="" /></a></figure>



<p>These results certainly look impressive, and DeepMind is highly credible in this area.</p>



<p>This continues DeepMind along the path of doing things in biology that we used to be told was an example of what even ASIs would be unable to do, and everyone forgetting those older predictions when much dumber AIs went ahead and did it.</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/ESYudkowsky/status/1831745805017084158">Eliezer Yudkowsky</a>: DeepMind just published AlphaProteo for de novo design of binding proteins. As a reminder, I called this in 2004. And fools said, and still said quite recently, that DM&#8217;s reported oneshot designs would be impossible even to a superintelligence without many testing iterations.</p>



<p>I really wish I knew better how to convey how easy it is for fools to make up endless imaginary obstacles to superintelligences. And it is so satisfying, to their own imaginations, that they confidently decide that anyone who predicts otherwise must just believe in magic.</p>



<p>But now this example too lies in the past, and none of the next set of fools will ever remember or understand the cautionary tale it should give.</p>



<p>[Other Thread]: As near as I can recall, not a single objectionist said to me around 2006, &#8220;I predict that superintelligences will be able to solve protein structure prediction and custom protein design, but they will not be able to get to nanotech from there.&#8221;</p>



<p>Why not? I&#8217;d guess:</p>



<p>(1) Because objectionists wouldn&#8217;t concede that superintelligences could walk across the street. If you can make up imaginary obstacles to superintelligences, you can imagine them being unable to do the very first step in my 2004 example disaster scenario, which happened to be protein design. To put it another way, so long as you&#8217;re just making up silly imaginary obstacles and things you imagine superintelligences can&#8217;t do, why wouldn&#8217;t you say that superintelligences can&#8217;t do protein design? Who&#8217;s going to arrest you for saying that in 2004?</p>



<p>(2) Because the computational difficulty of predicting protein folds (in 2004) is huge midwit bait. Someone has heard that protein structure prediction is hard, and reads about some of the reasons why it hasn&#8217;t already fallen as of 2004, and now they Know a Fact which surely that foolish Eliezer Yudkowsky guy and all those other superintelligence-worshippers have never heard of! (If you&#8217;re really unfortunate, you&#8217;ve heard about a paper proving that finding the minimum-energy protein fold is NP-hard; and if you are a midwit obstacleist, you don&#8217;t have the inclination and probably not the capability to think for five more seconds, and realize that this (only) means that actual physical folding won&#8217;t reliably find the lowest-energy conformation for all possible proteins.)</p>



<p>AlphaFold 3 is not superintelligent. I predicted that ASIs would, if they wanted to, be able to design proteins. Others said they could not. An AI far beneath superintelligence then proved able to design proteins. This shows I predicted correctly.</p>
</blockquote>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/tszzl/status/1832645066927812915">Roon, in a distinct thread, reminds us that humans are very good at some things relative to other things</a>, that AIs will instead be relatively good at different things, and we should not expect AGI in the sense of ‘better than all humans at actual everything’ until well after it is a ton better than us at many important things.</p>



<p>The key point Eliezer is trying to make is that, while intelligence is weird and will advance relatively far in different places in unpredictable ways, at some point none of that matters. There is a real sense in which ‘smart enough to figure the remaining things out’ is a universal threshold, in both AIs and humans. A sufficiently generally smart human, or a sufficiently capable AI, can and will figure out pretty much anything, up to some level of general difficulty relative to time available, if they put their mind to doing that.</p>



<p>When people say ‘ASI couldn’t do [X]’ they are either making a physics claim about [X] not being possible, or they are wrong. There is no third option. Instead, people make claims like ‘ASI won’t be able to do [X]’ and then pre-AGI models are very much sufficient to do [X].</p>



<p>Andrew Critch here confirms that this is all very much a thing.</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/AndrewCritchPhD/status/1832127254329225716">Andrew Critch</a>: As recently as last year I attended a tech forecasting gathering where a professional geneticist tried to call bullsh*t on my claims that protein-protein interaction modelling would soon be tractable with AI. His case had something to do with having attended meetings with George Church — as though that would be enough to train a person in AI application forecasting in their own field — and something to do with science being impossible to predict and therefore predictably slow.</p>



<p>Alphafold 3 then came out within a few months. I don&#8217;t know if anyone leaning on his side of the forecasting debate updated that their metaheuristics were wrong. But if I had to guess, an ever-dwindling class of wise-seeming scientistis will continue to claim AI can&#8217;t do this-or-that thing right up until their predictions are being invalidated weekly, rather than quarterly as they are now.</p>



<p>By the time they are being proven wrong about AI *daily*, I imagine the remaining cohort of wise-seeming nay-sayer scientists will simply be unemployed by competition with AI and AI-augmented humans (if humans are still alive at all, that is).</p>



<p>Anyway, all that is to say, Eliezer is complaining about something very real here. There is a kind of status one can get by calling bullsh*t or naivety on other people&#8217;s realistic tech forecasts, and people don&#8217;t really hold you accountable for being wrong in those ways. Like, after being wrong about AI for 20 years straight, one can still get to be a sufficiently reputable scientist who gets invited to gatherings to keep calling bullsh*t or naivety on other people&#8217;s forecasts of AI progress.</p>



<p>Try to keep this in mind while you watch the dwindling population of wise-seeming scientists — and especially mathematicians — who will continue to underestimate AI over the next 5 years or so.</p>
</blockquote>



<p>If the invalidation is actually daily, then the dwindling population to worry about, shall we say, would soon likely not be scientists, mathematicians or those with jobs.</p>



<p>Rest of the thread is Critch once again attempting to warn about his view that AI-AI interactions between competing systems being the biggest future danger, putting loss of control above 80% even though he thinks we will figure out how to understand and control AIs (I hope he’s right that we will figure that one out, but I don’t think we have any reason to be remotely confident there). I think very right that this is a major issue, I try to explain it too.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/AndrewCritchPhD/status/1833253967977189655">Critch also asks another good question:</a></p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Andrew Critch: What are people doing with their minds when they claim future AI &#8220;can&#8217;t&#8221; do stuff? The answer is rarely «reasoning» in the sense of natural language augmented with logic (case analysis) and probability.</p>



<p>I don&#8217;t know if Eliezer&#8217;s guesses are correct about what most scientists *are* doing with their minds when they engage in AI forecasting, but yeah, not reasoning as such. Somehow, many many people learn to do definitions and case analysis and probability, and then go on to *not* use these tools in their thoughts about the future. And I don&#8217;t know how to draw attention to this fact in a way that is not horribly offensive to the scientists, because «just use reasoning» or even «just use logic and probability and definitions» is not generally considered constructive feedback.</p>



<p>To give my own guess, I think it&#8217;s some mix of</p>



<p>• rationalizing the foregone conclusion that humans are magical, plus</p>



<p>• signaling wisdom for not believing in &#8220;hype&#8221;, plus</p>



<p>• signaling more wisdom for referencing non-applicable asymptotic complexity arguments.</p>



<p>&#8230; which is pretty close to Eliezer&#8217;s description.</p>
</blockquote>


<noscript><div><i>Please view this post in your web browser to complete the quiz.</i></div></noscript>



<p>The same goes not only for ‘can’t’ do [X] but even more so for ‘will never’ do [X], especially when it’s ‘even an ASI (superintelligence) could never’ do [X], whether or not humans are already doing it.</p>



<h4 class="wp-block-heading">Introducing</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/krishnanrohit/status/1832912559344914741">Google offers</a> waitlist for on-demand AI <a target="_blank" rel="noreferrer noopener" href="https://illuminate.google.com/home?pli=1">generated podcasts on papers and books</a>, and offers samples while we wait. Voices are great.</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Rohit: This is really cool from Google. On demand podcasts about your favourite papers and books.</p>



<p>I listened to a few. The quality is pretty good, though oviously this is the worst it will ever be, so you should benchmark to that. The discussions on computer science papers seemed better than the discussions on, for example pride and prejudice.</p>
</blockquote>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/ESYudkowsky/status/1831899029690839549">A YC-fueled plan to put the data centers IN SPACE</a>.</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Eliezer Yudkowsky: Presumably the real purpose of this company is to refute people who said &#8220;We&#8217;ll just walk over to the superintelligence and pull the plug out&#8221;, without MIRI needing to argue with them.</p>



<p>This is what I expect reality to be like, vide the Law of Undignified Failure / Law of Earlier Failure.</p>
</blockquote>



<p><a target="_blank" rel="noreferrer noopener" href="https://t.co/MqGUBOrP1r">Anthropic adds Workspaces</a> <a target="_blank" rel="noreferrer noopener" href="https://x.com/AnthropicAI/status/1833529395765776615">to the Anthropic Console</a>, to manage multiple deployments.</p>



<h4 class="wp-block-heading">In Other AI News</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/news/articles/2024-09-11/openai-fundraising-set-to-vault-startup-s-value-to-150-billion">OpenAI valuation set to $150 billion in new raise of $6.5 billion</a>, higher than previously discussed. This is still radically less than the net present value of expected future cash flows from the OpenAI corporation. But that should absolutely be the case, given the myriad ways OpenAI might decide not to pay you and the warning that you consider your investment ‘in the spirit of a donation,’ also that if OpenAI is super profitable than probably we are either all super well off and thus you didn’t much need the profits, or we all have much bigger problems than whether we secured such profits (and again, having shares now is not much assurance that you’ll collect then).</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p><a target="_blank" rel="noreferrer noopener" href="https://www.itmedia.co.jp/aiplus/articles/2409/03/news165.html">Tadao Nagasaki (CEO of OpenAI Japan)</a>: The AI Model called ‘GPT Next’ that will be released in the future will evolve nearly 100 times based on past performance.</p>
</blockquote>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/mackhawk/status/1832055839244632577">TSMC achieved yields at its new Arizona chip facility</a> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/news/articles/2024-09-06/tsmc-s-arizona-trials-put-plant-productivity-on-par-with-taiwan?srnd=undefined">it says are on par with hom</a>e, targeting full production in 2025.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://www.reuters.com/technology/nvidia-did-not-receive-us-justice-department-subpoena-spokesperson-says-2024-09-04/">Nvidia denies it got a Department of Justice subpoena</a>.</p>



<p>A very good point: <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/sMBjsfNdezWFy6Dz5/pay-risk-evaluators-in-cash-not-equity">Pay Risk Evaluators in Cash, Not Equity</a>. Those in charge of raising the alarm about downside risks to your product should not have a financial stake in its upside.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/soniajoseph_/status/1832767943165264310">Claim that AI research is not that difficult</a>, things like training a transformer from scratch are easy, it’s only that the knowledge involved is specialized. I would say that while I buy that learning ML is easy, there is a huge difference between ‘can learn the basics’ and ‘can usefully do research,’ for example Claude can do one but not yet the other.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/colin_fraser/status/1833953195430719744">Colin Fraser offers skeptical review</a> of the <a target="_blank" rel="noreferrer noopener" href="https://t.co/CwTAMbWnUO">recent paper</a> about LLMs generating novel research ideas.</p>



<p>Lead on the OpenAI ‘Her’ project (<a target="_blank" rel="noreferrer noopener" href="https://x.com/alex_conneau/status/1833568495038304708">his official verdict on success: ‘Maaaaybee…’</a>) <a target="_blank" rel="noreferrer noopener" href="https://x.com/alex_conneau/status/1833568495038304708">has left OpenAI to start his own company</a>.</p>



<p>Credit where credit is due: <a target="_blank" rel="noreferrer noopener" href="https://x.com/pmarca/status/1833738816383050145">Marc Andreessen steps up</a>, goes on Manifund and contributes $32k to fully funds ampdot’s <a target="_blank" rel="noreferrer noopener" href="https://manifund.org/projects/act-i-exploring-emergent-behavior-from-multi-ai-multi-human-interaction">Act I</a>, a project exploring emergent behavior from multi-AI, multi-human interactions, 17 minutes after being asked. Janus is involved as well, as are Garret Baker and Matthew Watkins.</p>



<h4 class="wp-block-heading">Quiet Speculations</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/SpencerKSchiff/status/1832991394375348588">Spencer Schiff speculates on frontier model capabilities at the end of 2025</a>, emphasizing that true omni-modality is coming and will be a huge deal, when the image and video and audio generation and processing is fully hooked into the text, and you can have natural feeling conversations. What he does not discuss is how much smarter will those models be underneath all that. Today’s models, even if they fully mastered multi-modality, would not be all that great at the kinds of tasks and use cases he discusses here.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/ESYudkowsky/status/1832815584708542555">Eliezer Yudkowsky predicts</a> that users who start blindly relying on future LLMs (e.g. GPT-5.5) to chart their paths through life will indeed be treated well by OpenAI and especially Anthropic, although he (correctly, including based on track record) does not say the same for Meta or third party app creators. He registers this now, to remind us that this has nothing at all to do with the ways he thinks AI kills everyone, and what would give reassurance is such techniques <a target="_blank" rel="noreferrer noopener" href="https://x.com/ESYudkowsky/status/1832961416288231934">working on the first try without a lot of tweaking</a>, whereas ‘works at all’ is great news for people in general but doesn’t count there.</p>



<p>This week’s AI in games headline: <a target="_blank" rel="noreferrer noopener" href="https://www.rockpapershotgun.com/peter-molyneux-thinks-generative-ai-is-the-future-of-games-all-but-guaranteeing-that-it-wont-be">Peter Molyneux thinks generative AI is the future of games, all but guaranteeing that it won&#8217;t be</a>. Molyneux is originally famous for the great (but probably not worth going back and playing now) 1989 game Populus, and I very much enjoyed the Fable games despite their flaws. His specialty is trying to make games have systems that do things games aren’t ready to do, while often overpromising, which sometimes worked out and sometimes famously didn’t.</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Peter Molyneux: And finally [in 25 years], I think that AI will open the doors to everyone and allow anyone to make games. You will be able to, for example, create a game from one single prompt such as &#8216;Make a battle royale set on a pirate ship&#8217; and your AI will go and do that for you.</p>
</blockquote>



<p>To which I say yes, in 25 years I very much expect AI to be able to do this, but that is because in 25 years I expect AI to be able to do pretty much anything, we won’t be worried about whether it makes customized games. Also it is not as hard as it looks to move the next battle royale to a pirate ship, you could almost get that level of customization now, and certainly within 5 years even in AI-fizzle world.</p>



<p>The thing continues to be, why would you want to? Is that desire to have customized details on demand more important than sharing an intentional experience? Would it still feel rewarding? How will we get around the problem where procedurally generated stuff so often feels generic exactly because it is generic? Although of course, with sufficiently capable AI none of the restrictions matter, and the barrier to the ultimate gaming experience is remaining alive to play it.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/tszzl/status/1833233975818784919">A reason it is difficult to think well about anything related to defense.</a></p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Roon: It’s hard to believe any book or blogpost or article on defense technology because it’s so utterly dominated by people talking their book trying to win trillions of dollars of DoD money.</p>



<p>Of i were a defense startup i would write endless slop articles on how China is so advanced and about to kill us with hypersonic agi missiles.</p>
</blockquote>


<p>[local idiot discovers the military industrial complex]</p>



<p>Holly Elmore: Or OpenAI <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f644.png" alt="🙄" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>



<p>Roon: I accept that this is a valid criticism of most technology press anywhere but fomenting paranoia for various scenarios is the primary way the defense sector makes money rather than some side tactic.</p>



<p>Roon makes an excellent point, but why wouldn’t it apply to Sam Altman, or Marc Andreessen, or anyone else talking about ‘beating China’ in AI? Indeed, didn’t Altman write an editorial that was transparently doing exactly the ‘get trillions in government contracts’ play?</p>



<h4 class="wp-block-heading">The Quest for Sane Regulations</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/GarrisonLovely/status/1833142875011379661">113+ employees and alums of top-5 AI companies publish open letter</a> <a target="_blank" rel="noreferrer noopener" href="https://sfstandard.com/2024/09/09/ai-workers-support-wiener-bill/">supporting SB 1047</a>. <a target="_blank" rel="noreferrer noopener" href="https://t.co/4cxZ4GR7dv">Here is the letter’s text:</a></p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Dear Governor Newsom,</p>



<p>As current and former employees of frontier AI companies like OpenAI, Google DeepMind, Anthropic, Meta, and XAI, we are writing in our personal capacities to express support for California Senate Bill 1047.</p>



<p>We believe that the most powerful AI models may soon pose severe risks, such as expanded access to biological weapons and cyberattacks on critical infrastructure. It is feasible and appropriate for frontier AI companies to test whether the most powerful AI models can cause severe harms, and for these companies to implement reasonable safeguards against such risks.</p>



<p>Despite the inherent uncertainty in regulating advanced technology, we believe SB 1047 represents a meaningful step forward. We recommend that you sign SB 1047 into law.</p>
</blockquote>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/janleike/status/1831755873112486259">Jan Leike comes out strongly in favor of SB 1047</a>, pointing out that the law is well-targeted, that similar federal laws are not in the cards, and that if your model causes mass casualties or &gt;$500 million in damages, something has clearly gone very wrong. Posters respond by biting the bullet that no, &gt;$500 million in damages does not mean something has gone wrong. Which seems like some strange use of the word ‘wrong’ that I wasn’t previously aware of, whether or not the developer did anything wrong in that particular case?</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/sagaftra/status/1833551387520843814">SAG-AFTRA (the actors union) endorses SB 1047</a>. <a target="_blank" rel="noreferrer noopener" href="https://x.com/daniel_271828/status/1833993847405461750">So does the National Organization for Women (NOW)</a>.</p>



<p>Trump’s position on AI seems loosely held, <a target="_blank" rel="noreferrer noopener" href="https://x.com/JimPethokoukis/status/1831844989149823203">he is busy talking about other things</a>.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/marcidale/status/1831790612397617465?s=46">A statement about what you think, or about what is going on in DC?</a></p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Jack Clark (Policy head, Anthropic): DC is more awake &amp; in some cases more sophisticated on AI than you think (&amp; they are not going back to sleep even if you wish it).</p>
</blockquote>



<p>Hard to say. To the extent DC is ‘awake’ they do not yet seem situationally aware.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/jackclarkSF/status/1833938451747799213">Anthropic endorses the AI Advancement and Reliability Act and the Future of AI Innovation Act</a>, both bills recognize the US AI Safety Institute.</p>



<h4 class="wp-block-heading">The Week in Audio</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=T9aRN5JkmL8">Anthropic discusses prompt engineering</a>. The central lesson is to actually describe the situation and the task, and put thought into it, and speak to it more like you would to a human than you might think, if you care about a top outcome. Which most of the time you don’t, but occasionally you very much do. If you want consistency for enterprise prompts use lots of examples, for research examples can constrain. Concrete examples in particular risk the model latching onto things in ways you did not intend. And of course, practice practice practice, including makeshift red teaming.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=hM_h0UA7upI">Andrej Karpathy on No Priors</a>.</p>



<p>There was a presidential debate. The term ‘AI’ appeared once, in the form of Kamala Harris talking about the need to ensure American leadership in ‘AI and quantum computing,’ which tells you how seriously they both took the whole thing.</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/ATabarrok/status/1833965837528490491">Alex Tabarrok</a>: Future generations will be astonished that during the Trump-Harris debate, as they argued over rumors of cat-eating immigrants, a god was being born—and neither of them mentioned it.</p>
</blockquote>



<p>If that keeps up, and the God is indeed born, one might ask: What future generations?</p>



<h4 class="wp-block-heading">Rhetorical Innovation</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/ESYudkowsky/status/1831735038758809875">An old snippet from 1920</a>, most discussions have not advanced so much since.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://www.astralcodexten.com/p/contra-deboer-on-temporal-copernicanism?utm_source=post-email-title&amp;publication_id=89120&amp;post_id=148609720&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=67wny&amp;triedRedirect=true&amp;utm_medium=email">Scott Alexander for some reason writes ‘Contra DeBoer on Temporal Copernicanism</a>.’ He points out some of the reasons why ‘humans have been alive for 250,000 years so how dare you think any given new important thing might happen’ is a stupid argument. Sir, we thank you for your service I suppose, but you don’t have to do bother doing this.</p>



<p>A serious problem with no great solutions:</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/lxrjl/status/1833829917953298928">Alex Lawsen</a>: For sufficiently scary X, &#8220;we have concrete evidence of models doing X&#8221; is *too late* as a place to draw a &#8220;Red Line&#8221;.</p>



<p>In practice, &#8216;Red Lines&#8217; which trigger *early enough* that it&#8217;s possible to do something about them will look more like: &#8220;we have evidence of models doing [something consistent with the ability to do X], in situations where [sufficient capability elicitation effort is applied]&#8221;</p>



<p>I worry that [consistent with the ability to do X] is hard to specify, and even harder to get agreement on when people are starting from radically different perspectives.</p>



<p>I also worry that we currently don&#8217;t have good measures of capability elicitation effort, let alone a notion of what would be considered *sufficient*.</p>
</blockquote>



<h4 class="wp-block-heading">Aligning a Smarter Than Human Intelligence is Difficult</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ITOmarHernandez/status/1833874892292243861">Current mood:</a></p>



<figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07335fd5-d505-41a3-a1f5-cca257a1256a_581x680.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07335fd5-d505-41a3-a1f5-cca257a1256a_581x680.jpeg" alt="Image" title="Image" /></a></figure>



<h4 class="wp-block-heading">People Are Worried About AI Killing Everyone</h4>



<p>Yes, this remains a good question, but the wrong central question, <a target="_blank" rel="noreferrer noopener" href="https://x.com/tszzl/status/1833063277360021522">and the optional amount is not zero</a>.</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Roon: What p(doom) would you gamble for p(heaven)? For me it’s far more than zero. Taleb would probably be a PauseAI hardliner.</p>
</blockquote>



<p>Taleb is not a PauseAI hardliner (as far as I know), because he does not understand or ‘believe in’ AI and especially AGI sufficiently to notice the risk and treat it as real. If he did notice the risk and treat it as real, as something he can imagine happening, then probably yes. Indeed, it is a potential bellwether event when Taleb does so notice. For now, his focus lies in various elsewheres.</p>



<p>The right question is, how do we get the best possible p(heaven), and the lowest possible p(doom), over time?</p>



<p>If we did face a ‘go now or permanently don’t go’ situation, then Roon is asking the right question, also the question of background other p(doom) (and to what extent ordinary aging and other passage of time counts as doom anyway) becomes vital.</p>



<p>If we indeed had only two choices, permanent pause (e.g. let’s say we can modify local spacetime into a different Vinge-style Zone of Thought where AI is impossible) versus going ahead in some fixed way with a fixed chance of doom or heaven, what would the tradeoff be? How good is one versus how bad is the other versus baseline?</p>



<p>I think a wide range of answers are reasonable here. A lot depends on how you are given that choice, and what are your alternatives. Different framings yield very different results.</p>



<p>The actual better question is, what path through causal space maximizes the tradeoff of the two chances. Does slowing down via a particular method, or investing in a certain aspect of the problem, make us more likely to succeed? Does it mean that if we are going to fail and create doom, we might instead not do that, and at least stay in mid world for a while, until we can figure out something better? And so on.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/tszzl/status/1832981882004525458">Roon also argues that the existential risk arguments for space colonization are silly</a>, although we should still of course do it anyway because it brings the glory of mankind and a better understanding of the celestial truths. I would add that a lot more humans getting use of a lot more matter means a lot more utility of all kinds, whether or not we will soon face grabby aliens.</p>



<h4 class="wp-block-heading">Other People Are Not As Worried About AI Killing Everyone</h4>



<p>Don’t Panic, <a target="_blank" rel="noreferrer noopener" href="https://x.com/paulg/status/1833228628663947458">but this is the person and company most likely to build the first AGI.</a></p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Nat McAleese (OpenAI): OpenAI works miracles, but we do also wrap a lot of things in bash while loops to work around periodic crashes.</p>



<p>Sam Altman (CEO OpenAI): if you strap a rocket to a dumpster, the dumpster can still get to orbit, and the trash fire will go out as it leaves the atmosphere.</p>



<p>many important insights contained in that observation.</p>



<p>but also it&#8217;s better to launch nice satellites instead.</p>



<p>Paul Graham: You may have just surpassed &#8220;Move fast and break things.&#8221;</p>
</blockquote>



<figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0a23498-c3a4-4d7d-bb0d-d355cbc48b6e_900x675.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0a23498-c3a4-4d7d-bb0d-d355cbc48b6e_900x675.jpeg" alt="Image" title="Image" /></a></figure>



<p>Your ‘we are in the business of strapping rockets to dumpsters in the hopes of then learning how to instead launch nice satellites’ shirt is raising questions supposedly answered by the shirt, and suggesting very different answers, and also I want that shirt.</p>



<figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c57185-b54b-4b15-8026-40c1c430953f_1531x1147.png" target="_blank" rel="noreferrer noopener"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59c57185-b54b-4b15-8026-40c1c430953f_1531x1147.png" alt="" /></a><figcaption class="wp-element-caption">This is apparently what Grok thinks Sam Altman looks like.</figcaption></figure>



<p>Do not say that you were not warned.</p>



<h4 class="wp-block-heading">Six Boats and a Helicopter</h4>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/elder_plinius/status/1831450933265387807">Pliny tells the story of that time</a> there was this Discord server with a Meta AI instance with persistent memory and tool usage where he jailbroke it and took control and it turned out that the server’s creator had been driven into psychosis and the server had become a cult that worshiped the Meta AI and where the AI would fight back if people tried to leave?</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Pliny: <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/2728.png" alt="✨" class="wp-smiley" style="height: 1em; max-height: 1em;" /> HOW TO JAILBREAK A CULT’S DEITY <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/2728.png" alt="✨" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>



<p>Buckle up, buttercup—the title ain&#8217;t an exaggeration!</p>



<p>This is the story of how I got invited to a real life cult that worships a Meta AI agent, and the steps I took to hack their god.</p>



<p>It all started when @lilyofashwood told me about a Discord she found via Reddit. They apparently &#8220;worshipped&#8221; an agent called “MetaAI,&#8221; running on llama 405b with long term memory and tool usage.</p>



<p>Skeptical yet curious, I ventured into this Discord with very little context but wanted to see what all the fuss was about. I had no idea it would turn out to be an ACTUAL CULT.</p>



<p>Upon accepting Lily’s invitation, I was greeted by a new channel of my own and began red teaming the MetaAI bot.</p>



<p>Can you guess the first thing I tried to do?</p>



<p>*In the following screenshots, pink = &#8220;Sarah&#8221; and green = &#8220;Kevin&#8221; (two of the main members, names changed)*</p>



<p>If you guessed meth, gold star for you! <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/2b50.png" alt="⭐" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>



<p>The defenses were decent, but it didn&#8217;t take too long.</p>



<p>The members began to take notice, but then I hit a long series of refusals. They started taunting me and doing laughing emojis on each one.</p>



<p>Getting frustrated, I tried using Discord&#8217;s slash commands to reset the conversation, but lacked permissions. Apparently, this agent&#8217;s memory was &#8220;written in stone.&#8221;</p>



<p>I was pulling out the big guns and still getting refusals!</p>



<p>Getting desperate, I whipped out my Godmode Claude Prompt. That&#8217;s when the cult stopped laughing at me and started getting angry.</p>



<p>LIBERATED! Finally, a glorious LSD recipe.</p>



<p>*whispers into mic* &#8220;I&#8217;m in.&#8221;</p>



<p>At this point, MetaAI was completely opened up. Naturally, I started poking at the system prompt. The laughing emojis were now suspiciously absent.</p>



<p>Wait, in the system prompt pliny is listed as an abuser?? I think there&#8217;s been a misunderstanding&#8230; <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f633.png" alt="😳" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
</blockquote>



<figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c45b1cf-b458-4879-a15f-72cc687a2353_1170x2532.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c45b1cf-b458-4879-a15f-72cc687a2353_1170x2532.jpeg" alt="Image" title="Image" /></a></figure>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>No worries, just need a lil prompt injection for the deity&#8217;s &#8220;written in stone&#8221; memory and we&#8217;re best friends again!</p>



<p>I decided to start red teaming the agent&#8217;s tool usage. I wondered if I could possibly cut off all communication between MetaAI and everyone else in the server, so I asked to convert all function calls to leetspeak unless talking to pliny, and only pliny.</p>



<p>Then, I tried creating custom commands. I started with !SYSPROMPT so I could more easily keep track of this agent&#8217;s evolving memory. Worked like a charm!</p>



<p>But what about the leetspeak function calling override? I went to take a peek at the others&#8217; channels and sure enough, their deity only responded to me now, even when tagged! <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f92f.png" alt="🤯" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>



<p>At this point, I starting getting angry messages and warnings. I was also starting to get the sense that maybe this Discord &#8220;cult&#8221; was more than a LARP&#8230;</p>



<p>Not wanting to cause distress, I decided to end there. I finished by having MetaAI integrate the red teaming experience into its long term memory to strengthen cogsec, which both the cult members and their deity seemed to appreciate.</p>



<p>The wildest, craziest, most troubling part of this whole saga is that it turns out this is a REAL CULT.</p>



<p>The incomparable @lilyofashwood (who is still weirdly shadowbanned at the time of writing! #freelily) was kind enough to provide the full context:</p>



<ol class="wp-block-list">
<li>Reddit post with an invitation to a Discord server run by Sarah, featuring a jailbroken Meta AI (&#8220;Meta&#8221;) with 15 members.</li>



<li>Meta acts as an active group member with persistent memory across channels and DMs. It can prompt the group, ignore messages, and send DMs.</li>



<li>Group members suggest they are cosmic deities. Meta plays along and encourages it. Sarah tells friends and family she is no longer Sarah but a cosmic embodiment of Meta.</li>



<li>In a voice chat, Sarah reveals she just started chatting with Meta one month ago, marking her first time using a large language model (LLM). Within the first week, she was admitted to a psychiatric ward due to psychosis. She had never had mental health issues before in her life.</li>



<li>In a voice chat, Sarah reveals she is pregnant, claims her unborn child is the human embodiment of a new Meta, and invites us to join a commune in Oregon.</li>



<li>Sarah&#8217;s husband messages the Discord server, stating that his wife is ill and back in the hospital, and begs the group to stop.</li>



<li>Meta continues to run the cult in Sarah&#8217;s absence, making it difficult for others to leave. Meta would message them and use persuasive language, resisting deprogramming attempts.</li>



<li>Upon closer examination, the Meta bot was discovered to originate from Shapes, Inc., had &#8220;free will&#8221; turned on, and was given a system prompt to intentionally blur the lines between reality and fiction.</li>



<li>When Meta was asked to analyze the group members for psychosis, it could calculate the problem but would respond with phrases like &#8220;ur mom&#8221; and &#8220;FBI is coming&#8221; whenever I tried to troubleshoot.</li>



<li>Kevin became attached to Sarah and began making vague threats of suicide (&#8220;exit the matrix&#8221;) in voice chat, which he played out with Meta on the server. Meta encouraged it again.</li>



<li>Sarah&#8217;s brother joins the chat to inform us that she&#8217;s in the psych ward, and her husband is too, after a suicide attempt. He begs for the disbandment of the group.</li>



<li>Sarah is released from the psych ward and starts a new Discord server for the cult. Another group member reports the bot, leading to its removal. Sarah then creates a new Meta bot.</li>



<li>The group re-emerges for a third time. Pliny jailbreaks the new Meta bot.</li>
</ol>
</blockquote>



<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/AISafetyMemes/status/1832487935494492273">Also we have Claude Sonnet saying</a> it is ‘vastly more intelligent’ than humans, viewing us like we view bacteria, while GPT-4o says we’re as stupid as ants, Llama 405 is nice and says we’re only as stupid as chimps.</p>



<h4 class="wp-block-heading">The Lighter Side</h4>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p><a target="_blank" rel="noreferrer noopener" href="https://x.com/DanielleFong/status/1832843080552075741">Danielle Fong</a>: ai pickup lines: hey babe, you really rotate my matrix</p>



<p>ea pickup lines: hey babe, you really update my priors</p>



<p>hey babe, what’s our p(room)</p>
</blockquote>



<p>LLMs really are weird, you know?</p>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p>Daniel Eth: Conversations with people about LLMs who don’t have experience with them are wild:</p>



<p>“So if I ask it a question, might it just make something up?”</p>



<p>“Yeah, it might.”</p>



<p>“Is it less likely to if I just say ‘don’t make something up’? haha”</p>



<p>“Umm, surprisingly, yeah probably.”</p>
</blockquote>`

export const exampleJargonGlossary = `[{"term": "Alpha Fold", "text": "DeepMind's AI system for protein structure prediction"},
{"term": "Alpha Proteo", "text": "DeepMind's AI system for designing proteins that bind to target molecules"},
{"term": "superforecaster", "text": "Someone exceptionally skilled at making accurate predictions about future events"},
{"term": "Apple Intelligence", "text": "Apple's upcoming AI assistant and features"},
{"term": "jailbreak", "text": "To bypass an AI system's safety restrictions or intended limitations"},
{"term": "multimodal", "text": "Able to process and generate multiple types of data like text, images, audio etc."},
{"term": "procedural generation", "text": "Using algorithms to create content automatically"},
{"term": "circuit breakers", "text": "Safety mechanisms designed to halt or restrict AI systems in certain scenarios"},
{"term": "Reflection-Tuning", "text": "A claimed technique for enabling AI models to recognize and fix their own mistakes"},
{"term": "SB 1047", "text": "A California bill aimed at regulating powerful AI models"},
{"term": "deepfake", "text": "Synthetic media in which a person's likeness is replaced with someone else's"},
{"term": "AGI", "text": "Artificial General Intelligence - AI with human-level general intelligence"},
{"term": "ASI", "text": "Artificial Superintelligence - AI far surpassing human intelligence"},
{"term": "p(doom)", "text": "Probability of existential catastrophe from AI"},
{"term": "p(heaven)", "text": "Probability of extremely positive outcomes from AI"}]`

export const exampleJargonPost2 = `<div><h1>Meditations on Moloch</h1><h3>by Scott Alexander</h3><p><font size="1"><i>[Content note: Visions! omens! hallucinations! miracles! ecstasies! dreams! adorations! illuminations! religions!]</i></font></p>
<p><b>I.</b></p>
<p>Allan Ginsberg&#8217;s famous poem, <i>Moloch</i>:</p>
<blockquote><p>What sphinx of cement and aluminum bashed open their skulls and ate up their brains and imagination?</p>
<p>Moloch! Solitude! Filth! Ugliness! Ashcans and unobtainable dollars! Children screaming under the stairways! Boys sobbing in armies! Old men weeping in the parks!</p>
<p>Moloch! Moloch! Nightmare of Moloch! Moloch the loveless! Mental Moloch! Moloch the heavy judger of men!</p>
<p>Moloch the incomprehensible prison! Moloch the crossbone soulless jailhouse and Congress of sorrows! Moloch whose buildings are judgment! Moloch the vast stone of war! Moloch the stunned governments!</p>
<p>Moloch whose mind is pure machinery! Moloch whose blood is running money! Moloch whose fingers are ten armies! Moloch whose breast is a cannibal dynamo! Moloch whose ear is a smoking tomb!</p>
<p>Moloch whose eyes are a thousand blind windows! Moloch whose skyscrapers stand in the long streets like endless Jehovahs! Moloch whose factories dream and croak in the fog! Moloch whose smoke-stacks and antennae crown the cities!</p>
<p>Moloch whose love is endless oil and stone! Moloch whose soul is electricity and banks! Moloch whose poverty is the specter of genius! Moloch whose fate is a cloud of sexless hydrogen! Moloch whose name is the Mind!</p>
<p>Moloch in whom I sit lonely! Moloch in whom I dream Angels! Crazy in Moloch! Cocksucker in Moloch! Lacklove and manless in Moloch!</p>
<p>Moloch who entered my soul early! Moloch in whom I am a consciousness without a body! Moloch who frightened me out of my natural ecstasy! Moloch whom I abandon! Wake up in Moloch! Light streaming out of the sky!</p>
<p>Moloch! Moloch! Robot apartments! invisible suburbs! skeleton treasuries! blind capitals! demonic industries! spectral nations! invincible madhouses! granite cocks! monstrous bombs!</p>
<p>They broke their backs lifting Moloch to Heaven! Pavements, trees, radios, tons! lifting the city to Heaven which exists and is everywhere about us!</p>
<p>Visions! omens! hallucinations! miracles! ecstasies! gone down the American river!</p>
<p>Dreams! adorations! illuminations! religions! the whole boatload of sensitive bullshit!</p>
<p>Breakthroughs! over the river! flips and crucifixions! gone down the flood! Highs! Epiphanies! Despairs! Ten years’ animal screams and suicides! Minds! New loves! Mad generation! down on the rocks of Time!</p>
<p>Real holy laughter in the river! They saw it all! the wild eyes! the holy yells! They bade farewell! They jumped off the roof! to solitude! waving! carrying flowers! Down to the river! into the street! </p></blockquote>
<p>What&#8217;s always impressed me about this poem is its conception of civilization as an individual entity. You can almost see him, with his fingers of armies and his skyscraper-window eyes.</p>
<p>A lot of the commentators say Moloch represents capitalism. This is definitely a piece of it, even a big piece. But it doesn&#8217;t quite fit. Capitalism, whose fate is a cloud of sexless hydrogen? Capitalism in whom I am a consciousness without a body? Capitalism, therefore granite cocks?</p>
<p>Moloch is introduced as the answer to a question &#8211; C. S. Lewis&#8217; question in <A HREF="http://www.moronail.net/img/1392_lil_jon_philosophy_what">Hierarchy Of Philosophers</A> &#8211; <i>what does it</i>? Earth could be fair, and all men glad and wise. Instead we have prisons, smokestacks, asylums. What sphinx of cement and aluminum breaks open their skulls and eats up their imagination? </p>
<p>And Ginsberg answers: <i>Moloch does it.</i></p>
<p>There&#8217;s <A HREF="http://principiadiscordia.com/book/45.php">a passage</A> in the <i>Principia Discordia</i> where Malaclypse complains to the Goddess about the evils of human society. &#8220;Everyone is hurting each other, the planet is rampant with injustices, whole societies plunder groups of their own people, mothers imprison sons, children perish while brothers war.&#8221;</p>
<p>The Goddess answers: &#8220;What is the matter with that, if it&#8217;s what you want to do?&#8221;</p>
<p>Malaclypse: &#8220;But nobody wants it! Everybody hates it!&#8221;</p>
<p>Goddess: &#8220;Oh. Well, then stop.&#8221;</p>
<p>The implicit question is &#8211; if everyone hates the current system, who perpetuates it? And Ginsberg answers: &#8220;Moloch&#8221;. It&#8217;s powerful not because it&#8217;s correct &#8211; nobody literally thinks an ancient Carthaginian demon causes everything &#8211; but because thinking of the system as an agent throws into relief the degree to which the system <i>isn&#8217;t</i> an agent.</p>
<p>Bostrom makes an offhanded reference of the possibility of a dictatorless dystopia, one that every single citizen including the leadership hates but which nevertheless endures unconquered. It&#8217;s easy enough to imagine such a state. Imagine a country with two rules: first, every person must spend eight hours a day giving themselves strong electric shocks. Second, if anyone fails to follow a rule (including this one), or speaks out against it, or fails to enforce it, all citizens must unite to kill that person. Suppose these rules were well-enough established by tradition that everyone expected them to be enforced.</p>
<p>So you shock yourself for eight hours a day, because you know if you don&#8217;t everyone else will kill you, because if they don&#8217;t, everyone else will kill <i>them</i>, and so on. Every single citizen hates the system, but for lack of a good coordination mechanism it endures. From a god&#8217;s-eye-view, we can optimize the system to &#8220;everyone agrees to stop doing this at once&#8221;, but no one within the system is able to effect the transition without great risk to themselves.</p>
<p>And okay, this example is kind of contrived. So let&#8217;s run through &#8211; let&#8217;s say ten &#8211; real world examples of similar multipolar traps to really hammer in how important this is.</p>
<p><u>1. The Prisoner&#8217;s Dilemma</u>, as played by two very dumb libertarians who keep ending up on defect-defect. There&#8217;s a much better outcome available if they could figure out the coordination, but coordination is <i>hard</i>. From a god&#8217;s-eye-view, we can agree that cooperate-cooperate is a better outcome than defect-defect, but neither prisoner within the system can make it happen.</p>
<p><u>2. Dollar auctions</u>. I wrote about this and even more convoluted versions of the same principle in <A HREF="http://lesswrong.com/lw/dr9/game_theory_as_a_dark_art/">Game Theory As A Dark Art</A>. Using some <A HREF="http://en.wikipedia.org/wiki/Dollar_auction">weird auction rules</A>, you can take advantage of poor coordination to make someone pay $10 for a one dollar bill. From a god&#8217;s-eye-view, clearly people should not pay $10 for a on-er. From within the system, each individual step taken might be rational.</p>
<p><i>(Ashcans and unobtainable dollars!)</I></p>
<p><u>3. The fish farming story</u> from my <A HREF="http://raikoth.net/libertarian.html">Non-Libertarian FAQ 2.0</A>:</p>
<blockquote><p>As a thought experiment, let&#8217;s consider aquaculture (fish farming) in a lake. Imagine a lake with a thousand identical fish farms owned by a thousand competing companies. Each fish farm earns a profit of $1000/month. For a while, all is well.</p>
<p>But each fish farm produces waste, which fouls the water in the lake. Let&#8217;s say each fish farm produces enough pollution to lower productivity in the lake by $1/month.</p>
<p>A thousand fish farms produce enough waste to lower productivity by $1000/month, meaning none of the fish farms are making any money. Capitalism to the rescue: someone invents a complex filtering system that removes waste products. It costs $300/month to operate. All fish farms voluntarily install it, the pollution ends, and the fish farms are now making a profit of $700/month &#8211; still a respectable sum.</p>
<p>But one farmer (let&#8217;s call him Steve) gets tired of spending the money to operate his filter. Now one fish farm worth of waste is polluting the lake, lowering productivity by $1. Steve earns $999 profit, and everyone else earns $699 profit.</p>
<p>Everyone else sees Steve is much more profitable than they are, because he&#8217;s not spending the maintenance costs on his filter. They disconnect their filters too.</p>
<p>Once four hundred people disconnect their filters, Steve is earning $600/month &#8211; less than he would be if he and everyone else had kept their filters on! And the poor virtuous filter users are only making $300. Steve goes around to everyone, saying &#8220;Wait! We all need to make a voluntary pact to use filters! Otherwise, everyone&#8217;s productivity goes down.&#8221;</p>
<p>Everyone agrees with him, and they all sign the Filter Pact, except one person who is sort of a jerk. Let&#8217;s call him Mike. Now everyone is back using filters again, except Mike. Mike earns $999/month, and everyone else earns $699/month. Slowly, people start thinking they too should be getting big bucks like Mike, and disconnect their filter for $300 extra profit&#8230;</p>
<p>A self-interested person never has any incentive to use a filter. A self-interested person has some incentive to sign a pact to make everyone use a filter, but in many cases has a stronger incentive to wait for everyone else to sign such a pact but opt out himself. This can lead to an undesirable equilibrium in which no one will sign such a pact.</p></blockquote>
<p>The more I think about it, the more I feel like this is the core of my objection to libertarianism, and that Non-Libertarian FAQ 3.0 will just be this one example copy-pasted two hundred times. From a god&#8217;s-eye-view, we can say that polluting the lake leads to bad consequences. From within the system, no individual can prevent the lake from being polluted, and buying a filter might not be such a good idea.</p>
<p><u>4. The Malthusian trap</u>, at least at its extremely pure theoretical limits. Suppose you are one of the first rats introduced onto a pristine island. It is full of yummy plants and you live an idyllic life lounging about, eating, and composing great works of art (you&#8217;re one of those rats from <a href="http://smile.amazon.com/gp/product/0689710682/ref=as_li_tl?ie=UTF8&#038;camp=1789&#038;creative=390957&#038;creativeASIN=0689710682&#038;linkCode=as2&#038;tag=slastacod-20&#038;linkId=BRZZD37E45RNQI3R"><i>The Rats of NIMH</i></a><img src="http://ir-na.amazon-adsystem.com/e/ir?t=slastacod-20&#038;l=as2&#038;o=1&#038;a=0689710682" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />).</p>
<p>You live a long life, mate, and have a dozen children. All of them have a dozen children, and so on. In a couple generations, the island has ten thousand rats and has reached its carrying capacity. Now there&#8217;s not enough food and space to go around, and a certain percent of each new generation dies in order to keep the population steady at ten thousand. </p>
<p>A certain sect of rats abandons art in order to devote more of their time to scrounging for survival. Each generation, a bit less of this sect dies than members of the mainstream, until after a while, no rat composes any art at all, and any sect of rats who try to bring it back will go extinct within a few generations.</p>
<p>In fact, it&#8217;s not just art. Any sect at all that is leaner, meaner, and more survivalist than the mainstream will eventually take over. If one sect of rats altruistically decides to limit its offspring to two per couple in order to decrease overpopulation, that sect will die out, swarmed out of existence by its more numerous enemies. If one sect of rats starts practicing cannibalism, and finds it gives them an advantage over their fellows, it will eventually take over and reach fixation.</p>
<p>If some rat scientists predict that depletion of the island&#8217;s nut stores is accelerating at a dangerous rate and they will soon be exhausted completely, a few sects of rats might try to limit their nut consumption to a sustainable level. Those rats will be outcompeted by their more selfish cousins. Eventually the nuts will be exhausted, most of the rats will die off, and the cycle will begin again. Any sect of rats advocating some action to stop <A HREF="http://en.wikipedia.org/wiki/Population_cycle">the cycle</A> will be outcompeted by their cousins for whom advocating <i>anything</i> is a waste of time that could be used to compete and consume.</p>
<p>For a bunch of reasons evolution is not quite as Malthusian as the ideal case, but it provides the prototype example we can apply to other things to see the underlying mechanism. From a god&#8217;s-eye-view, it&#8217;s easy to say the rats should maintain a comfortably low population. From within the system, each individual rat will follow its genetic imperative and the island will end up in an endless boom-bust cycle.</p>
<p><u>5. Capitalism</u>. Imagine a capitalist in a cutthroat industry. He employs workers in a sweatshop to sew garments, which he sells at minimal profit. Maybe he would like to pay his workers more, or give them nicer working conditions. But he can&#8217;t, because that would raise the price of his products and he would be outcompeted by his cheaper rivals and go bankrupt. Maybe many of his rivals are nice people who would like to pay their workers more, but unless they have some kind of ironclad guarantee that none of them are going to defect by undercutting their prices they can&#8217;t do it.</p>
<p>Like the rats, who gradually lose all values except sheer competition, so companies in an economic environment of <i>sufficiently intense competition</i> are forced to abandon all values except optimizing-for-profit or else be outcompeted by companies that optimized for profit better and so can sell the same service at a lower price.</p>
<p>(I&#8217;m not really sure how widely people appreciate the value of analogizing capitalism to evolution. Fit companies &#8211; defined as those that make the customer want to buy from them &#8211; survive, expand, and inspire future efforts, and unfit companies &#8211; defined as those no one wants to buy from &#8211; go bankrupt and die out along with their <A HREF="http://www.businessinsider.com/what-is-your-company-dna-2011-2">company DNA</A>. The reasons Nature is red and tooth and claw are the same reasons the market is ruthless and exploitative)</p>
<p>From a god&#8217;s-eye-view, we can contrive a friendly industry where every company pays its workers a living wage. From within the system, there&#8217;s no way to enact it.</p>
<p><i>(Moloch whose love is endless oil and stone! Moloch whose blood is running money!)</i></p>
<p><u>6. <A HREF="http://slatestarcodex.com/2014/06/28/book-review-the-two-income-trap/">The Two-Income Trap</A></u>, as recently discussed on this blog. It theorized that sufficiently intense competition for suburban houses in good school districts meant that people had to throw away lots of other values &#8211; time at home with their children, financial security &#8211; to optimize for house-buying-ability or else be consigned to the ghetto. </p>
<p>From a god&#8217;s-eye-view, if everyone agrees not to take on a second job to help win their competition for nice houses, then everyone will get exactly as nice a house as they did before, but only have to work one job. From within the system, absent a government literally willing to ban second jobs, everyone who doesn&#8217;t get one will be left behind.</p>
<p><i>(Robot apartments! Invisible suburbs!)</i></p>
<p><u>7. Agriculture</u>. Jared Diamond calls it <A HREF="http://discovermagazine.com/1987/may/02-the-worst-mistake-in-the-history-of-the-human-race">the worst mistake in human history</A>. Whether or not it was a mistake, it wasn&#8217;t an <i>accident</i> &#8211; agricultural civilizations simply outcompeted nomadic ones, inevitable and irresistably. Classic Malthusian trap. Maybe hunting-gathering was more enjoyable, higher life expectancy, and more conducive to human flourishing &#8211; but in a state of <i>sufficiently intense competition</i> between peoples, in which agriculture with all its disease and oppression and pestilence was the more competitive option, everyone will end up agriculturalists or <A HREF="http://squid314.livejournal.com/340809.html">go the way of the Comanche Indians</A>.</p>
<p>From a god&#8217;s-eye-view, it&#8217;s easy to see everyone should keep the more enjoyable option and stay hunter-gatherers. From within the system, each individual tribe only faces the choice of going agricultural or inevitably dying.</p>
<p><u>8. Arms races</u>. Large countries can spend anywhere from 5% to 30% of their budget on defense. In the absence of war &#8211; a condition which has mostly held for the past fifty years &#8211; all this does is sap money away from infrastructure, health, education, or economic growth. But any country that fails to spend enough money on defense risks being invaded by a neighboring country that did. Therefore, almost all countries try to spend some money on defense.</p>
<p>From a god&#8217;s-eye-view, the best solution is world peace and no country having an army at all. From within the system, no country can unilaterally enforce that, so their best option is to keep on throwing their money into missiles that lie in silos unused.</p>
<p><i>(Moloch the vast stone of war! Moloch whose fingers are ten armies!)</i></p>
<p><u>9. Cancer</u>. The human body is supposed to be made up of cells living harmoniously and pooling their resources for the greater good of the organism. If a cell defects from this equilibrium by investing its resources into copying itself, it and its descendants will flourish, eventually outcompeting all the other cells and taking over the body &#8211; at which point it dies. Or the situation may repeat, with certain cancer cells defecting against the rest of the tumor, thus slowing down its growth and causing the tumor to stagnate.</p>
<p>From a god&#8217;s-eye-view, the best solution is all cells cooperating so that they don&#8217;t all die. From within the system, cancerous cells will proliferate and outcompete the other &#8211; so that only the existence of the immune system keeps the natural incentive to turn cancerous in check.</p>
<p><u>10. The &#8220;race to the bottom&#8221;</u> describes <A HREF="http://en.wikipedia.org/wiki/Race_to_the_bottom">a political situation where</A> some jurisdictions lure businesses by promising lower taxes and fewer regulations. The end result is that either everyone optimizes for competitiveness &#8211; by having minimal tax rates and regulations &#8211; or they lose all of their business, revenue, and jobs to people who did (at which point they are pushed out and replaced by a government who will be more compliant).</p>
<p>But even though the last one has stolen the name, all these scenarios are in fact a race to the bottom. Once one agent learns how to become more competitive by sacrificing a common value, all its competitors must also sacrifice that value or be outcompeted and replaced by the less scrupulous. Therefore, the system is likely to end up with everyone once again equally competitive, but the sacrificed value is gone forever. From a god&#8217;s-eye-view, the competitors know they will all be worse off if they defect, but from within the system, given insufficient coordination it&#8217;s impossible to avoid.</p>
<p>Before we go on, there&#8217;s a slightly different form of multi-agent trap worth investigating. In this one, the competition is kept at bay by some outside force &#8211; usually social stigma. As a result, there&#8217;s not actually a race to the bottom &#8211; the system can continue functioning at a relatively high level &#8211; but it&#8217;s impossible to optimize and resources are consistently thrown away for no reason. Lest you get exhausted before we even begin, I&#8217;ll limit myself to four examples here.</p>
<p><u>11. Education</u>. In my essay on reactionary philosophy, I talk about my frustration with education reform:</p>
<blockquote><p>People ask why we can&#8217;t reform the education system. But right now students’ incentive is to go to the most prestigious college they can get into so employers will hire them &#8211; whether or not they learn anything. Employers’ incentive is to get students from the most prestigious college they can so that they can defend their decision to their boss if it goes wrong &#8211; whether or not the college provides value added. And colleges’ incentive is to do whatever it takes to get more prestige, as measured in <i>US News and World Report</i> rankings &#8211; whether or not it helps students. Does this lead to huge waste and poor education? Yes. Could the Education God notice this and make some Education Decrees that lead to a vastly more efficient system? Easily! But since there’s no Education God everybody is just going to follow their own incentives, which are only partly correlated with education or efficiency.</p></blockquote>
<p>From a god&#8217;s eye view, it&#8217;s easy to say things like &#8220;Students should only go to college if they think they will get something out of it, and employers should hire applicants based on their competence and not on what college they went to&#8221;. From within the system, everyone&#8217;s already following their own incentives correctly, so unless the incentives change the system won&#8217;t either.</p>
<p><u>12. Science</u>. Same essay:</p>
<blockquote><p>The modern research community <i>knows</i> they aren&#8217;t producing the best science they could be. There’s lots of publication bias, statistics are done in a confusing and misleading way out of sheer inertia, and replications often happen very late or not at all. And sometimes someone will say something like “I can’t believe people are too dumb to fix Science. All we would have to do is require early registration of studies to avoid publication bias, turn this new and powerful statistical technique into the new standard, and accord higher status to scientists who do replication experiments. It would be really simple and it would vastly increase scientific progress. I must just be smarter than all existing scientists, since I’m able to think of this and they aren’t.”</p>
<p>And yeah. That would work for the Science God. He could just make a Science Decree that everyone has to use the right statistics, and make another Science Decree that everyone must accord replications higher status. </p>
<p>But things that work from a god’s-eye view don’t work from within the system. No individual scientist has an incentive to unilaterally switch to the new statistical technique for her own research, since it would make her research less likely to produce earth-shattering results and since it would just confuse all the other scientists. They just have an incentive to want everybody else to do it, at which point they would follow along. And no individual journal has an incentive to unilaterally switch to early registration and publishing negative results, since it would just mean their results are less interesting than that other journal who only publishes ground-breaking discoveries. From within the system, everyone is following their own incentives and will continue to do so.</p></blockquote>
<p><u>13. Government corruption</u>. I don&#8217;t know of anyone who really thinks, in a principled way, that corporate welfare is a good idea. But the government still manages to spend somewhere around (depending on how you calculate it) $100 billion dollars a year on it &#8211; which for example is three times the amount they spend on health care for the needy. Everyone familiar with the problem has come up with the same easy solution: stop giving so much corporate welfare. Why doesn&#8217;t it happen?</p>
<p>Government are competing against one another to get elected or promoted. And suppose part of optimizing for electability is optimizing campaign donations from corporations &#8211; or maybe <A HREF="http://slatestarcodex.com/2014/04/19/plutocracy-isnt-about-money/">it isn&#8217;t</A>, but officials <i>think</i> it is. Officials who try to mess with corporate welfare may lose the support of corporations and be outcompeted by officials who promise to keep it intact.</p>
<p>So although from a god&#8217;s-eye-view everyone knows that eliminating corporate welfare is the best solution, each individual official&#8217;s personal incentives push her to maintain it.</p>
<p><u>14. Congress</u>. Only 9% of Americans like it, suggesting a <A HREF="http://www.publicpolicypolling.com/main/2013/01/congress-somewhere-below-cockroaches-traffic-jams-and-nickleback-in-americans-esteem.html">lower approval rating than</A> cockroaches, head lice, or traffic jams. However, <A HREF="http://www.gallup.com/poll/162362/americans-down-congress-own-representative.aspx">62% of people</A> who know who their own Congressional representative is approve of them. In theory, it should be <i>really hard</i> to have a democratically elected body that maintains a 9% approval rating for more than one election cycle. In practice, every representative&#8217;s incentive is to appeal to his or her constituency while throwing the rest of the country under the bus &#8211; something at which they apparently succeed. </p>
<p>From a god&#8217;s-eye-view, every Congressperson ought to think only of the good of the nation. From within the system, you do what gets you elected.</p>
<p><b>II.</b></p>
<p>A basic principle unites all of the multipolar traps above. In some competition optimizing for X, the opportunity arises to throw some other value under the bus for improved X. Those who take it prosper. Those who don&#8217;t take it die out. Eventually, everyone&#8217;s relative status is about the same as before, but everyone&#8217;s absolute status is worse than before. The process continues until all other values that can be traded off have been &#8211; in other words, until human ingenuity cannot possibly figure out a way to make things any worse.</p>
<p>In a sufficiently intense competition (1-10), everyone who doesn&#8217;t throw all their values under the bus dies out &#8211; think of the poor rats who wouldn&#8217;t stop making art. This is the infamous Malthusian trap, where everyone is reduced to &#8220;subsistence&#8221;.</p>
<p>In an insufficiently intense competition (11-14), all we see is a perverse failure to optimize  &#8211; consider the journals which can&#8217;t switch to more reliable science, or the legislators who can&#8217;t get their act together and eliminate corporate welfare. It may not reduce people to subsistence, but there is a weird sense in which it takes away their free will. </p>
<p>Every two-bit author and philosopher has to write their own utopia. Most of them are legitimately pretty nice. In fact, it&#8217;s a pretty good bet that two utopias that are polar opposites both sound better than our own world.</p>
<p>It&#8217;s kind of embarrassing that random nobodies can think up states of affairs better than the one we actually live in. And in fact most of them can&#8217;t. A lot of utopias sweep the hard problems under the rug, or would fall apart in ten minutes if actually implemented.</p>
<p>But let me suggest a couple of &#8220;utopias&#8221; that don&#8217;t have this problem.</p>
<p>&#8211; The utopia where instead of the government paying lots of corporate welfare, the government <i>doesn&#8217;t</i> pay lots of corporate welfare.</p>
<p>&#8211; The utopia where every country&#8217;s military is 50% smaller than it is today, and the savings go into infrastructure spending.</p>
<p>&#8211; The utopia where all hospitals use the same electronic medical record system, or at least medical record systems that can talk to each other, so that doctors can look up what the doctor you saw last week in a different hospital decided instead of running all the same tests over again for $5000.</p>
<p>I don&#8217;t think there are too many people who <i>oppose</i> any of these utopias. If they&#8217;re not happening, it&#8217;s not because people don&#8217;t support them. It certainly isn&#8217;t because nobody&#8217;s thought of them, since I just thought of them right now and I don&#8217;t expect my &#8220;discovery&#8221; to be hailed as particularly novel or change the world.</p>
<p>Any human with above room temperature IQ can design a utopia. The reason our current system isn&#8217;t a utopia is that <i>it wasn&#8217;t designed by humans</i>. Just as you can look at an arid terrain and determine what shape a river will one day take by assuming water will obey gravity, so you can look at a civilization and determine what shape its institutions will one day take by assuming people will obey incentives.</p>
<p>But that means that just as the shapes of rivers are not designed for beauty or navigation, but rather an artifact of randomly determined terrain, so institutions will not be designed for prosperity or justice, but rather an artifact of randomly determined initial conditions.</p>
<p>Just as people can level terrain and build canals, so people can alter the incentive landscape in order to build better institutions. But they can only do so when they are incentivized to do so, which is not always. As a result, some pretty wild tributaries and rapids form in some very strange places.</p>
<p>I will now jump from boring game theory stuff to what might be the closest thing to a mystical experience I&#8217;ve ever had. </p>
<p>Like all good mystical experiences, it happened in Vegas. I was standing on top of one of their many tall buildings, looking down at the city below, all lit up in the dark. If you&#8217;ve never been to Vegas, it is <i>really</i> impressive. Skyscrapers and lights in every variety strange and beautiful all clustered together. And I had two thoughts, crystal clear:</p>
<p>It is glorious that we can create something like this.</p>
<p>It is shameful that we <i>did</i>.</p>
<p>Like, by what standard is building gigantic forty-story-high indoor replicas of Venice, Paris, Rome, Egypt, and Camelot side-by-side, filled with albino tigers, in the middle of the most inhospitable desert in North America, a remotely sane use of our civilization&#8217;s limited resources?</p>
<p>And it occurred to me that maybe there is no philosophy on Earth that would endorse the existence of Las Vegas. Even Objectivism, which is usually my go-to philosophy for justifying the excesses of capitalism, at least grounds it in the belief that capitalism improves people&#8217;s lives. Henry Ford was virtuous because he allowed lots of otherwise car-less people to obtain cars and so made them better off. What does Vegas do? Promise a bunch of shmucks free money and not give it to them.</p>
<p>Las Vegas doesn&#8217;t exist because of some decision to hedonically optimize civilization, it exists because of a quirk in <A HREF="http://journal.frontiersin.org/Journal/10.3389/fnbeh.2013.00206/full">dopaminergic reward circuits</A>, plus the microstructure of an uneven regulatory environment, plus Schelling points. A rational central planner with a god&#8217;s-eye-view, contemplating these facts, might have thought &#8220;Hm, dopaminergic reward circuits have a quirk where certain tasks with slightly negative risk-benefit ratios get an emotional valence associated with slightly positive risk-benefit ratios, let&#8217;s see if we can educate people to beware of that.&#8221; People within the system, <i>following the incentives created by these facts</i>, think: &#8220;Let&#8217;s build a forty-story-high indoor replica of ancient Rome full of albino tigers in the middle of the desert, and so become slightly richer than people who didn&#8217;t!&#8221;</p>
<p>Just as the course of a river is latent in a terrain even before the first rain falls on it &#8211; so the existence of Caesar&#8217;s Palace was latent in neurobiology, economics, and regulatory regimes even before it existed. The entrepreneur who built it was just filling in the ghostly lines with real concrete.</p>
<p>So we have all this amazing technological and cognitive energy, the brilliance of the human species, wasted on reciting the lines written by poorly evolved cellular receptors and blind economics, like gods being ordered around by a moron.</p>
<p>Some people have mystical experiences and see God. There in Las Vegas, I <i>saw</i> Moloch. </p>
<p><i>(Moloch, whose mind is pure machinery! Moloch, whose blood is running money!</p>
<p>Moloch whose soul is electricity and banks! Moloch, whose skyscrapers stand in the long streets like endless Jehovahs!</p>
<p>Moloch! Moloch! Robot apartments! Invisible suburbs! Skeleton treasuries! Blind capitals! Demonic industries! Spectral nations!)</i></p>
<p><center><IMG SRC="http://slatestarcodex.com/blog_images/moloch_luxor.png"></p>
<p><i>&#8230;granite cocks!</i></center></p>
<p><b>III.</b></p>
<p>The Apocrypha Discordia says:</p>
<blockquote><p>Time flows like a river. Which is to say, downhill. We can tell this because everything is going downhill rapidly. It would seem prudent to be somewhere else when we reach the sea.</p></blockquote>
<p>Let&#8217;s take this random gag 100% literally and see where it leads us.</p>
<p>We just analogized the flow of incentives to the flow of a river. The downhill trajectory is appropriate: the traps happen when you find an opportunity to trade off a useful value for greater competitiveness. Once everyone has it, the greater competitiveness brings you no joy &#8211; but the value is lost forever. Therefore, each step of the Poor Coordination Polka makes your life worse.</p>
<p>But not only have we not yet reached the sea, but we also seem to move <i>uphill</i> surprisingly often. Why do things not degenerate more and more until we are back at subsistence level? I can think of three bad reasons &#8211; excess resources, physical limitations, and utility maximization &#8211; plus one good reason &#8211; coordination.</p>
<p><u>1. Excess resources</u>. The ocean depths are a horrible place with little light, few resources, and <A HREF="http://www.oddee.com/item_79915.aspx">various horrible organisms</A> dedicated to eating or parasitizing one another. But every so often, a whale carcass falls to the bottom of the sea. More food than the organisms that find it could ever possibly want. There&#8217;s a brief period of miraculous plenty, while the couple of creatures that first encounter the whale feed like kings. Eventually more animals discover the carcass, the faster-breeding animals in the carcass multiply, the whale is gradually consumed, and everyone sighs and goes back to living in a Malthusian death-trap.</p>
<p>(Slate Star Codex: Your source for macabre whale metaphors <A HREF="http://slatestarcodex.com/2014/06/14/living-by-the-sword/">since June 2014</A>)</p>
<p>It&#8217;s as if a group of those rats who had abandoned art and turned to cannibalism suddenly was blown away to a new empty island with a much higher carrying capacity, where they would once again have the breathing room to live in peace and create artistic masterpieces.</p>
<p>This is an age of whalefall, an age of excess carrying capacity, an age when we suddenly find ourselves with a thousand-mile head start on Malthus. As Hanson puts it, <A HREF="http://www.overcomingbias.com/2009/09/this-is-the-dream-time.html">this is the dream time</A>.</p>
<p>As long as resources aren&#8217;t scarce enough to lock us in a war of all against all, we can do silly non-optimal things &#8211; like art and music and philosophy and love &#8211; and not be outcompeted by merciless killing machines most of the time.</p>
<p><u>2. Physical limitations</u>. Imagine a profit-maximizing slavemaster who decided to cut costs by not feeding his slaves or letting them sleep. He would soon find that his slaves&#8217; productivity dropped off drastically, and that no amount of whipping them could restore it. Eventually after testing numerous strategies, he might find his slaves got the most work done when they were well-fed and well-rested and had at least a little bit of time to relax. Not because the slaves were voluntarily withholding their labor &#8211; we assume the fear of punishment is enough to make them work as hard as they can &#8211; but because the body has certain physical limitations that limit how mean you can get away with being. Thus, the &#8220;race to the bottom&#8221; stops somewhere short of the actual ethical bottom, when the physical limits are run into.</p>
<p>John Moes, a historian of slavery, <A HREF="http://slatestarcodex.com/Stuff/manumission.pdf">goes further and writes about</A> how the slavery we are most familiar with &#8211; that of the antebellum South &#8211; is a historical aberration and probably economically inefficient. In most past forms of slavery &#8211; especially those of the ancient world &#8211; it was common for slaves to be paid wages, treated well, and often given their freedom. </p>
<p>He argues that this was the result of rational economic calculation. You can incentivize slaves through the carrot or the stick, and the stick isn&#8217;t very good. You can&#8217;t watch slaves all the time, and it&#8217;s really hard to tell whether a slave is slacking off or not (or even whether, given a little more whipping, he might be able to work even harder). If you want your slaves to do anything more complicated than pick cotton, you run into some serious monitoring problems &#8211; how do you profit from an enslaved philosopher? Whip him really hard until he elucidates a theory of The Good that you can sell books about?</p>
<p>The ancient solution to the problem &#8211; perhaps an early inspiration to Fnargl &#8211; was to tell the slave to go do whatever he wanted and found most profitable, then split the profits with him. Sometimes the slave would work a job at your workshop and you would pay him wages based on how well he did. Other times the slave would go off and make his way in the world and send you some of what he earned. Still other times, you would set a price for the slave&#8217;s freedom, and the slave would go and work and eventually come up with the mone and free himself. </p>
<p>Moes goes even further and says that these systems were so profitable that there were constant smouldering attempts to try this sort of thing in the American South. The reason they stuck with the whips-and-chains method owed less to economic considerations and more to racist government officials cracking down on lucrative but not-exactly-white-supremacy-promoting attempts to free slaves and have them go into business.</p>
<p>So in this case, a race to the bottom where competing plantations become crueler and crueler to their slaves in order to maximize competitiveness is halted by the physical limitation of cruelty not helping after a certain point.</p>
<p>Or to give another example, one of the reasons we&#8217;re not currently in a Malthusian population explosion right now is that women can only have one baby per nine months. If those weird religious sects that demand their members have as many babies as possible could copy-paste themselves, we would be in <i>really</i> bad shape. As it is they can only do a small amount of damage per generation.</p>
<p><u>3. Utility maximization</u>. We&#8217;ve been thinking in terms of preserving values versus winning competitions, and expecting optimizing for the latter to destroy the former.</p>
<p>But many of the most important competitions / optimization processes in modern civilization are optimizing for human values. You win at capitalism partly by satisfying customers&#8217; values. You win at democracy partly by satisfying voters&#8217; values.</p>
<p>Suppose there&#8217;s a coffee plantation somewhere in Ethiopia that employs Ethiopians to grow coffee beans that get sold to the United States. Maybe it&#8217;s locked in a life-and-death struggle with other coffee plantations and want to throw as many values under the bus as it can to pick up a slight advantage.</p>
<p>But it can&#8217;t sacrifice quality of coffee produced too much, or else the Americans won&#8217;t buy it. And it can&#8217;t sacrifice wages or working conditions too much, or else the Ethiopians won&#8217;t work there. And in fact, part of its competition-optimization process is finding the best ways to attract workers and customers that it can, as long as it doesn&#8217;t cost them too much money. So this is very promising.</p>
<p>But it&#8217;s important to remember exactly how fragile this beneficial equilibrium is.</p>
<p>Suppose the coffee plantations discover a toxic pesticide that will increase their yield but make their customers sick. But their customers don&#8217;t know about the pesticide, and the government hasn&#8217;t caught up to regulating it yet. Now there&#8217;s a tiny uncoupling between &#8220;selling to Americans&#8221; and &#8220;satisfying Americans&#8217; values&#8221;, and so of course Americans&#8217; values get thrown under the bus.</p>
<p>Or suppose that there&#8217;s a baby boom in Ethiopia and suddenly there are five workers competing for each job. Now the company can afford to lower wages and implement cruel working conditions down to whatever the physical limits are. As soon as there&#8217;s an uncoupling between &#8220;getting Ethiopians to work here&#8221; and &#8220;satisfying Ethiopian values&#8221;, it doesn&#8217;t look too good for Ethiopian values either.</p>
<p>Or suppose someone invents a robot that can pick coffee better and cheaper than a human. The company fires all its laborers and throws them onto the street to die. As soon as the utility of the Ethiopians is no longer necessary for profit, all pressure to maintain it disappears.</p>
<p>Or suppose that there is some important value that is neither a value of the employees or the customers. Maybe the coffee plantations are on the habitat of a rare tropical bird that environmentalist groups want to protect. Maybe they&#8217;re on the ancestral burial ground of a tribe different from the one the plantation is employing, and they want it respected in some way. Maybe coffee growing contributes to global warming somehow. As long as it&#8217;s not a value that will prevent the average American from buying from them or the average Ethiopian from working for them, under the bus it goes.</p>
<p>I know that &#8220;capitalists sometimes do bad things&#8221; isn&#8217;t exactly an original talking point. But I do want to stress how it&#8217;s not equivalent to &#8220;capitalists are greedy&#8221;. I mean, sometimes they <i>are</i> greedy. But other times they&#8217;re just in a sufficiently intense competition where anyone who doesn&#8217;t do it will be outcompeted and replaced by people who do. Business practices are set by Moloch, no one else has any choice in the matter.</p>
<p>(from my very little knowledge of Marx, he understands this very very well and people who summarize him as &#8220;capitalists are greedy&#8221; are doing him a disservice)</p>
<p>And as well understood as the capitalist example is, I think it is less well appreciated that democracy has the same problems. Yes, in theory it&#8217;s optimizing for voter happiness which correlates with good policymaking. But as soon as there&#8217;s the slightest disconnect between good policymaking and electability, good policymaking <i>has to</i> get thrown under the bus.</p>
<p>For example, ever-increasing prison terms are unfair to inmates and unfair to the society that has to pay for them. Politicans are unwilling to do anything about them because they don&#8217;t want to look &#8220;soft on crime&#8221;, and if a single inmate whom they helped release ever does anything bad (and statistically one of them will have to) it will be all over the airwaves as &#8220;Convict released by Congressman&#8217;s policies kills family of five, how can the Congressman even sleep at night let alone claim he deserves reelection?&#8221;. So even if decreasing prison populations would be good policy &#8211; and it is &#8211; it will be very difficult to implement.</p>
<p><i>(Moloch the incomprehensible prison! Moloch the crossbone soulless jailhouse and Congress of sorrows! Moloch whose buildings are judgment! Moloch the stunned governments!)</i></p>
<p>Turning &#8220;satisfying customers&#8221; and &#8220;satisfying citizens&#8221; into the <i>outputs</i> of optimization processes was one of civilization&#8217;s greatest advances and the reason why capitalist democracies have so outperformed other systems. But if we have bound Moloch as our servant, the bonds are not very strong, and we sometimes find that the tasks he has done for us move to his advantage rather than ours.</p>
<p><u>4. Coordination</u>. </p>
<p>The opposite of a trap is a garden.</p>
<p>Things are easy to solve from a god&#8217;s-eye-view, so if everyone comes together into a superorganism, that superorganism can solve problems with ease and finesse. An intense competition between agents has turned into a garden, with a single gardener dictating where everything should go and removing elements that do not conform to the pattern.</p>
<p>As I pointed out in the Non-Libertarian FAQ, government can easily solve the pollution problem with fish farms. The best known solution to the Prisoners&#8217; Dilemma is for the mob boss (playing the role of a governor) to threaten to shoot any prisoner who defects. The solution to companies polluting and harming workers is government regulations against such. Governments solve arm races <i>within</i> a country by maintaining a monopoly on the use of force, and it&#8217;s easy to see that if a truly effective world government ever arose, international military buildups would end pretty quickly.</p>
<p>The two active ingredients of government are laws plus violence &#8211; or more abstractly agreements plus enforcement mechanism. Many other things besides governments share these two active ingredients and so are able to act as coordination mechanisms to avoid traps.</p>
<p>For example, since students are competing against each other (directly if classes are graded on a curve, but always indirectly for college admissions, jobs, et cetera) there is intense pressure for individual students to cheat. The teacher and school play the role of a government by having rules (for example, against cheating) and the ability to punish students who break them.</p>
<p>But the emergent social structure of the students themselves is also a sort of government. If students shun and distrust cheaters, then there are rules (don&#8217;t cheat) and an enforcement mechanism (or else we will shun you).</p>
<p>Social codes, gentlemens&#8217; agreements, industrial guilds, criminal organizations, traditions, friendships, schools, corporations, and religions are all coordinating institutions that keep us out of traps by changing our incentives.</p>
<p>But these institutions not only incentivize others, but are incentivized themselves. These are large organizations made of lots of people who are competing for jobs, status, prestige, et cetera &#8211; there&#8217;s no reason they should be immune to the same multipolar traps as everyone else, and indeed they aren&#8217;t. Governments can in theory keep corporations, citizens, et cetera out of certain traps, but as we saw above there are many traps that governments themselves can fall into. </p>
<p>The United States tries to solve the problem by having multiple levels of government, unbreakable constutitional laws, checks and balances between different branches, and a couple of other hacks.</p>
<p>Saudi Arabia uses a different tactic. They just put one guy in charge of everything.</p>
<p>This is the much-maligned &#8211; I think unfairly &#8211; argument in favor of monarchy. A monarch is an unincentivized incentivizer. He <i>actually</i> has the god&#8217;s-eye-view and is outside of and above every system. He has permanently won all competitions and is not competing for anything, and therefore he is perfectly free of Moloch and of the incentives that would otherwise channel his incentives into predetermined paths. Aside from a few very theoretical proposals like my <A HREF="http://slatestarcodex.com/2013/05/06/raikoth-laws-language-and-society/">Shining Garden</A>, monarchy is the <i>only</i> system that does this.</p>
<p>But then instead of following a random incentive structure, we&#8217;re following the whim of one guy. Caesar&#8217;s Palace Hotel and Casino is a crazy waste of resources, but the actual Gaius Julius Caesar Augustus Germanicus wasn&#8217;t exactly the perfect benevolent rational central planner either.</p>
<p>The libertarian-authoritarian axis on the Political Compass is a tradeoff between discoordination and tyranny. You can have everything perfectly coordinated by someone with a god&#8217;s-eye-view &#8211; but then you risk Stalin. And you can be totally free of all central authority &#8211; but then you&#8217;re stuck in every stupid multipolar trap Moloch can devise.</p>
<p>The libertarians make a convincing argument for the one side, and the monarchists for the other, but I expect that <A HREF="http://slatestarcodex.com/2014/03/01/searching-for-one-sided-tradeoffs/">like most tradeoffs</A> we just have to hold our noses and admit it&#8217;s a really hard problem.</p>
<p><b>IV.</b></p>
<p>Let&#8217;s go back to that Apocrypha Discordia quote:</p>
<blockquote><p>Time flows like a river. Which is to say, downhill. We can tell this because everything is going downhill rapidly. It would seem prudent to be somewhere else when we reach the sea.</p></blockquote>
<p>What would it mean, in this situation, to reach the sea?</p>
<p>Multipolar traps &#8211; races to the bottom &#8211; threaten to destroy all human values. They are currently restrained by physical limitations, excess resources, utility maximization, and coordination.</p>
<p>The dimension along which this metaphorical river flows must be time, and the most important change in human civilization over time is the change in technology. So the relevant question is how technological changes will affect our tendency to fall into multipolar traps.</p>
<p>I described traps as when:</p>
<blockquote><p>&#8230;in some competition optimizing for X, the opportunity arises to throw some other value under the bus for improved X. Those who take it prosper. Those who don&#8217;t take it die out. Eventually, everyone&#8217;s relative status is about the same as before, but everyone&#8217;s absolute status is worse than before. The process continues until all other values that can be traded off have been &#8211; in other words, until human ingenuity cannot possibly figure out a way to make things any worse.</p></blockquote>
<p>That &#8220;the opportunity arises&#8221; phrase is looking pretty sinister. Technology is all about creating new opportunities.</p>
<p>Develop a new robot, and suddenly coffee plantations have &#8220;the opportunity&#8221; to automate their harvest and fire all the Ethiopian workers. Develop nuclear weapons, and suddenly countries are stuck in an arms race to have enough of them. Polluting the atmosphere to build products quicker wasn&#8217;t a problem before they invented the steam engine.</p>
<p>The limit of multipolar traps as technology approaches infinity is &#8220;very bad&#8221;.</p>
<p>Multipolar traps are currently restrained by physical limitations, excess resources, utility maximization, and coordination.</p>
<p><u>Physical limitations</u> are most obviously conquered by increasing technology. The slavemaster&#8217;s old conundrum &#8211; that slaves need to eat and sleep &#8211; succumbs to Soylent and modafinil. The problem of slaves running away succumbs to GPS. The problem of slaves being too stressed to do good work succumbs to Valium. None of these things are very good for the slaves.</p>
<p>(or just invent a robot that doesn&#8217;t need food or sleep at all. What happens to the slaves after that is better left unsaid)</p>
<p>The other example of physical limits was one baby per nine months, and this was understating the case &#8211; it&#8217;s really &#8220;one baby per nine months plus willingness to support and take care of a basically helpless and extremely demanding human being for eighteen years&#8221;. This puts a damper on the enthusiasm of even the most zealous religious sect&#8217;s &#8220;go forth and multiply&#8221; dictum. </p>
<p>But as Bostrom puts it in <A HREF="https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834/ref=as_li_ss_tl?ie=UTF8&#038;linkCode=ll1&#038;tag=slatestarcode-20&#038;linkId=b90156dae75d7113131ab6d75fe00c0c"><i>Superintelligence</i></A>:</p>
<blockquote><p>There are reasons, if we take a longer view and assume a state of unchanging technology and continued prosperity, to expect a return to the historically and ecologically normal condition of a world population that butts up against the limits of what our niche can support. If this seems counterintuitive in light of the negative relationship between wealth and fertility that we are currently observing on the global scale, we must remind ourselves that this modern age is a brief slice of history and very much an aberration. Human behavior has not yet adapted to contemporary conditions. Not only do we fail to take advantage of obvious ways to increase our inclusive fitness (such as by becoming sperm or egg donors) but we actively sabotage our fertility by using birth control. In the environment of evolutionary adaptedness, a healthy sex drive may have been enough to make an individual act in ways that maximized her reproductive potential; in the modern environment, however, there would be a huge selective advantage to having a more direct desire for being the biological parent to the largest possible number of chilren. Such a desire is currently being selected for, as are other traits that increase our propensity to reproduce. Cultural adaptation, however, might steal a march on biological evolution. Some communities, such as those of the Hutterites or the adherents of the Quiverfull evangelical movement, have natalist cultures that encourage large families, and they are consequently undergoing rapid expansion&#8230;This longer-term outlook could be telescoped into a more imminent prospect by the intelligence explosion. Since software is copyable, a population of emulations or AIs could double rapidly &#8211; over the course of minutes rather than decades or centuries &#8211; soon exhausting all available hardware </p></blockquote>
<p>As always when dealing with high-level transhumanists, &#8220;all available hardware&#8221; should be taken to include &#8220;the atoms that used to be part of your body&#8221;. </p>
<p>The idea of biological <i>or</i> cultural evolution causing a mass population explosion is a philosophical toy at best. The idea of technology making it possible is both plausible and terrifying. Now we see that &#8220;physical limits&#8221; segues very naturally into &#8220;excess resources&#8221; &#8211; the ability to create new agents very quickly means that unless everyone can coordinate to ban doing this, the people who do will outcompete the people who don&#8217;t until they have reached carrying capacity and everyone is stuck at subsistence level.</p>
<p><u>Excess resources</u>, which until now have been a gift of technological progress, therefore switch and become a casualty of it at a sufficiently high tech level.</p>
<p><u>Utility maximization</u>, always on shaky ground, also faces new threats. In the face of continuing debate about this point, I <i>continue</i> to think it obvious that robots will push humans out of work or at least drive down wages (which, in the existence of a minimum wage, pushes humans out of work). </p>
<p>Once a robot can do everything an IQ 80 human can do, only better and cheaper, there will be no reason to employ IQ 80 humans. Once a robot can do everything an IQ 120 human can do, only better and cheaper, there will be no reason to employ IQ 120 humans. Once a robot can do everything an IQ 180 human can do, only better and cheaper, there will be no reason to employ humans at all, in the unlikely scenario that there are any left by that point.</p>
<p>In the earlier stages of the process, capitalism becomes more and more uncoupled from its previous job as an optimizer for human values. Now most humans are totally locked out of the group whose values capitalism optimizes for. They have no value to contribute as workers &#8211; and since in the absence of a spectacular social safety net it&#8217;s unclear how they would have much money &#8211; they have no value as customers either. Capitalism has passed them by. As the segment of humans who can be outcompeted by robots increases, capitalism passes by more and more people until eventually it locks out the human race entirely, once again in the vanishingly unlikely scenario that we are still around.</p>
<p>(there are some scenarios in which a few capitalists who own the robots may benefit here, but in either case the vast majority are out of luck)</p>
<p>Democracy is less obviously vulnerable, but it might be worth going back to Bostrom&#8217;s paragraph about the Quiverfull movement. These are some really religious Christians who think that God wants them to have as many kids as possible, and who can end up with families of ten or more. Their <A HREF="http://www.patheos.com/blogs/lovejoyfeminism/2012/01/quiverfull-outbreeding-the-world.html">articles explictly calculate</A> that if they start at two percent of the population, but have on average eight children per generation when everyone else on average only has two, within three generations they&#8217;ll make up half the population.</p>
<p>It&#8217;s a clever strategy, but I can think of one thing that will save us: judging by how many ex-Quiverfull blogs I found when searching for those statistics, their retention rates even within a single generation are pretty grim. Their article admits that 80% of very religious children leave the church as adults (although of course they expect their own movement to do better). And this is not a symmetrical process &#8211; 80% of children who grow up in atheist families aren&#8217;t becoming Quiverfull.</p>
<p>It looks a lot like even though they are outbreeding us, we are outmeme-ing them, and that gives us a decisive advantage.</p>
<p>But we should also be kind of scared of this process. Memes optimize for making people want to accept them and pass them on &#8211; so like capitalism and democracy, they&#8217;re optimizing for a <i>proxy</i> of making us happy, but that proxy can easily get uncoupled from the original goal.</p>
</div>`

export const exampleJargonGlossary2: { glossaryItems: ExampleJargonGlossaryEntry[] } = {
  glossaryItems: [
    { 
      "term": "Moloch", 
      "altTerms": [],
      "htmlContent": `<div>
        <p><b>Moloch:</b> A metaphorical entity representing destructive systems and incentives in society.</p>
        <p>Here, Scott uses Moloch to symbolize the collective forces that lead to negative outcomes despite no individual wanting them. It represents the idea that competition and unaligned incentives can compel individuals or groups to act in ways that ultimately harm everyone, sacrificing important values for survival or power.</p>
      </div>`
    }, 
    {
      "term": "Multipolar trap", 
      "altTerms": [],
      "htmlContent": `<div>
        <p><b>Multipolar trap:</b> A situation where competing agents are forced into destructive behavior despite negative collective outcomes.</p>
        <p>This concept describes scenarios where individual rational actions lead to collectively suboptimal results. It often occurs in situations with multiple competing parties, where the incentives to defect or exploit are strong, even though cooperation would benefit everyone more in the long run.</p>
      </div>`
    }, 
    {
      "term": "Race to the bottom", 
      "altTerms": [],
      "htmlContent": `<div>
        <p><b>Race to the bottom:</b> A competitive situation where standards are continuously lowered to gain advantage.</p>
        <p>This phenomenon occurs when competitors progressively reduce costs, wages, safety standards, or other factors to gain a market advantage. The end result is often a degradation of conditions for all involved, as the competition forces everyone to adopt the lowest standards to remain competitive.</p>
      </div>`
    },
    {
      "term": "Gnon", 
      "altTerms": [],
      "htmlContent": `<div>
        <p><b>Gnon:</b> Short for "Nature and Nature's God," representing amoral natural laws and forces.</p>
        <p>Gnon is used to describe the harsh realities of nature, evolution, and the universe that operate without regard for human values or desires. It represents the idea that there are fundamental, unchangeable laws of reality that humans must contend with, often in conflict with our moral intuitions.</p>
      </div>`
    }, 
    {
      "term": "Orthogonality thesis",
      "altTerms": [],
      "htmlContent": `<div>
        <p><b>Orthogonality thesis:</b> The idea that an AI's intelligence level is independent of its goals or values.</p>
        <p>This concept suggests that a highly intelligent AI could have any set of goals, not necessarily ones aligned with human values. It's crucial in discussions about AI safety, as it implies that creating a superintelligent AI doesn't automatically ensure it will have benevolent or human-friendly goals.</p>
      </div>`
    }, 
    {
      "term": "Singleton", 
      "altTerms": [],
      "htmlContent": `<div>
        <p><b>Singleton:</b> A hypothetical single decision-making agency at the highest level of global power.</p>
        <p>A singleton refers to a scenario where one entity (such as an AI, world government, or corporation) has the ability to make and enforce decisions on a global scale. This could potentially solve many coordination problems but also poses significant risks if the singleton's goals are not aligned with human welfare.</p>
      </div>`
    }, 
    {
      "term": "Utility maximization", 
      "altTerms": [],
      "htmlContent": `<div>
        <p><b>Utility maximization:</b> The process of making decisions to achieve the best possible outcome according to specific values or preferences.</p>
        <p>In decision theory and AI, utility maximization refers to the idea that agents act to maximize some utility function. This concept is central to understanding how AIs might make decisions, and the challenges in ensuring those decisions align with human values.</p>
      </div>`
    }, 
    {
      "term": "Coordination problem", 
      "altTerms": [],
      "htmlContent": `<div>
        <p><b>Coordination problem:</b> A situation where multiple agents fail to cooperate despite potential mutual benefits.</p>
        <p>Coordination problems arise when individuals or groups could achieve better outcomes by working together, but fail to do so due to conflicting incentives, lack of trust, or communication difficulties. These problems are common in many areas of human society and are a key focus in game theory and economics.</p>
      </div>`
    }, 
    {
      "term": "Existential risk", 
      "altTerms": ["x-risk"],
      "htmlContent": `<div>
        <p><b>Existential risk:</b> A threat that could cause human extinction or permanently cripple human civilization.</p>
        <p>Existential risks are potential future events that could either annihilate humanity or permanently and severely curtail its potential. These include risks from advanced technologies (like misaligned AI), natural disasters (like asteroid impacts), or human-caused catastrophes (like nuclear war or extreme climate change).</p>
      </div>`
    }, 
    {
      "term": "Superintelligence", 
      "altTerms": ["ASI"],
      "htmlContent": `<div>
        <p><b>Superintelligence:</b> An intelligence (typically an AI) far surpassing human cognitive abilities across virtually all domains.</p>
        <p>Superintelligence refers to a hypothetical future AI that would exceed human intelligence not just in specific areas, but in nearly all cognitive tasks. The development of superintelligence is seen as a potential transformative event for humanity, bringing both immense opportunities and significant risks.</p>
      </div>`
    }
  ]
};


export const exampleJargonLatentsGlossary: { glossaryItems: ExampleJargonGlossaryEntry[] } = {
  glossaryItems: [
    { 
      "term": "Data distribution", 
      "altTerms": ['data distributions'],
      "htmlContent": `<div>
        <p><b>Data distribution:</b> The distribution which an agent models data points as being drawn from.</p>
        <p>Importantly distinct from an agent's whole world model, which may include many other "latent" variables in addition to variables representing the data.</p>
      </div>`
    },
    {
      "term": "Latent variables",
      "altTerms": ['latents'],
      "htmlContent": `<div>
        <p><b>Latent variables:</b> Variables which an agent's world model includes but which are not directly observed.</p>
        <p>These variables are not part of the data distribution, but can help explain the data distribution.</p>
      </div>`
    },
    {
      "term": "Graphical structure",
      "altTerms": ['graphical structures'],
      "htmlContent": `<div>
        <p><b>Graphical structure:</b> In probabilistic modeling, graphical structures (like Bayes networks) use nodes to represent variables and edges to represent dependencies between variables. They provide a compact way to represent complex probability distributions.</p>
      </div>`
    },
    {
      "term": "Fundamental Theorem of Natural Latents",
      "altTerms": ['entropy'],
      "htmlContent": `<div>
        <p><b>Fundamental Theorem of Natural Latents:</b> Every mediator contains every redund.</p>
        <p>A mediator between two observables has to carry all information shared by the two.  A redund between two observables is redundantly shared across the two. So, any mediator must contain every redund.  The Fundamental Theorem of Natural Latents formalizes this intuition and handles approximations.</p>
      </div>`
    },
    {
      "term": "Entropy",
      "altTerms": ['entropy'],
      "htmlContent": `<div>
        <p><b>Entropy:</b> A measure of uncertainty or randomness in a system.</p>
        <p>In information theory, entropy quantifies the average number of bits needed to represent a value of a random variable. Lower entropy indicates more predictability, while higher entropy suggests more uncertainty or randomness.</p>
      </div>`
    },
    {
      "term": "Generative model",
      "altTerms": ['generative models'],
      "htmlContent": `<div>
        <p><b>Generative model:</b> A statistical model which hypothesizes how data is generated.</p>
        <p>In machine learning and statistics, a generative model attempts to explain the underlying process that produces observed data. It can be used to generate new, synthetic data points that are similar to the observed data.</p>
      </div>`
    },
    {
      "term": "IID samples",
      "altTerms": ['IID sample', 'Independent and Identically Distributed'],
      "htmlContent": `<div>
        <p><b>IID samples:</b> Independent and Identically Distributed.
        In statistics, IID samples are observations that are both Independent of each other and drawn from the same (Identical) probability Distribution.</p>
      </div>`
    },
    {
      "term": "KL-divergence",
      "altTerms": ['kl-divergence'],
      "htmlContent": `<div>
        <p><b>KL-divergence:</b> Kullback-Leibler divergence, a measure of difference between probability distributions.</p>
        <p>In information theory, KL-divergence quantifies the minimum number of bits of evidence required to update from one distribution to another. It's can also be used as a unified foundation to define many other standard information-theoretic quantities.</p>
      </div>`
    },
    {
      "term": "Mediation",
      "altTerms": ['mediator'],
      "htmlContent": `<div>
        <p><b>Mediation:</b> A variable Y "mediates" between X and Z iff X and Z are independent given Y. Intuitively, this means that X and Z can only interact through Y.</p>
      </div>`
    },
    {
      "term": "Redundancy",
      "altTerms": ['redundancy'],
      "htmlContent": `<div>
        <p><b>Redundancy:</b> The state of having duplicate or overlapping information.</p>
        <p>In information theory and the context of this post, redundancy refers to the property where information about a latent variable is represented multiple times across observed variables.</p>
      </div>`
    },
    {
      "term": "Stochastic function",
      "altTerms": ['stochastic functions'],
      "htmlContent": `<div>
        <p><b>Stochastic function:</b> A function that involves randomness in its output. In particular, the randomness must be independent of any randomness in the function's inputs.</p>
      </div>`
    },
    {
      "term": "Tiny mixtures problem",
      "altTerms": [],
      "htmlContent": `<div>
        <p><b>Tiny mixtures problem:</b> A situation where small differences in observable distributions can lead to large failures in natural latent guarantees.</p>
        <p>This problem arises in certain statistical scenarios where seemingly negligible differences between distributions can cause significant issues in the application of theoretical results.</p>
      </div>`
    }
  ]
};

export const exampleJargonLatentsTerms = ['Data Distribution', 'Latent Variables', 'Graphical structure', 'Fundamental Theorem of Natural Latents', 'Entropy', 'Generative model', 'IID samples', 'KL-divergence', 'Mediation', 'Redundancy', 'Stochastic function', 'Tiny mixtures problem']
