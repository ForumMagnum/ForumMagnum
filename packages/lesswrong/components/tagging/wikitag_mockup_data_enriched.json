[
  {
    "_id": "rationality-theory-concepts",
    "name": "Theory / Concepts",
    "slug": "theory-concepts",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "Ng8Gice9KNkncxqcj"
  },
  {
    "_id": "rationality-applied-topics",
    "name": "Applied Topics", 
    "slug": "applied-topics",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "Ng8Gice9KNkncxqcj"
  },
  {
    "_id": "rationality-failure-modes",
    "name": "Failure Modes",
    "slug": "failure-modes", 
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "Ng8Gice9KNkncxqcj"
  },
  {
    "_id": "rationality-communication",
    "name": "Communication",
    "slug": "communication",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "Ng8Gice9KNkncxqcj"
  },
  {
    "_id": "rationality-techniques",
    "name": "Techniques",
    "slug": "techniques",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "Ng8Gice9KNkncxqcj"
  },
  {
    "_id": "ai-basic-alignment-theory",
    "name": "Basic Alignment Theory",
    "slug": "basic-alignment-theory",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "sYm3HiWcfZvrGu3ui"
  },
  {
    "_id": "ai-engineering-alignment",
    "name": "Engineering Alignment",
    "slug": "engineering-alignment",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "sYm3HiWcfZvrGu3ui"
  },
  {
    "_id": "ai-organizations",
    "name": "Organizations",
    "slug": "organizations",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "sYm3HiWcfZvrGu3ui"
  },
  {
    "_id": "ai-strategy",
    "name": "Strategy",
    "slug": "strategy",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "sYm3HiWcfZvrGu3ui"
  },
  {
    "_id": "ai-other",
    "name": "Other",
    "slug": "other",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "sYm3HiWcfZvrGu3ui"
  },
  {
    "_id": "world-modeling-mathematical-sciences",
    "name": "Mathematical Sciences",
    "slug": "mathematical-sciences",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "3uE2pXvbcnS9nnZRE"
  },
  {
    "_id": "world-modeling-general-science",
    "name": "General Science & Eng",
    "slug": "general-science-eng",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "3uE2pXvbcnS9nnZRE"
  },
  {
    "_id": "world-modeling-social-economic",
    "name": "Social & Economic",
    "slug": "social-economic",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "3uE2pXvbcnS9nnZRE"
  },
  {
    "_id": "world-modeling-biological-psychological",
    "name": "Biological & Psychological",
    "slug": "biological-psychological",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "3uE2pXvbcnS9nnZRE"
  },
  {
    "_id": "world-modeling-practice",
    "name": "The Practice of Modeling",
    "slug": "practice-of-modeling",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "3uE2pXvbcnS9nnZRE"
  },
  {
    "_id": "world-optimization-moral-theory",
    "name": "Moral Theory",
    "slug": "moral-theory",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "xexCWMyds6QLWognu"
  },
  {
    "_id": "world-optimization-causes",
    "name": "Causes / Interventions",
    "slug": "causes-interventions",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "xexCWMyds6QLWognu"
  },
  {
    "_id": "world-optimization-working-with-humans",
    "name": "Working with Humans",
    "slug": "working-with-humans",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "xexCWMyds6QLWognu"
  },
  {
    "_id": "world-optimization-applied-topics",
    "name": "Applied Topics",
    "slug": "applied-topics",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "xexCWMyds6QLWognu"
  },
  {
    "_id": "world-optimization-value-virtue",
    "name": "Value & Virtue",
    "slug": "value-virtue",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "xexCWMyds6QLWognu"
  },
  {
    "_id": "world-optimization-meta",
    "name": "Meta",
    "slug": "meta",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "xexCWMyds6QLWognu"
  },
  {
    "_id": "practical-domains-wellbeing",
    "name": "Domains of Well-being",
    "slug": "domains-wellbeing",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "fkABsGCJZ6y9qConW"
  },
  {
    "_id": "practical-skills-techniques",
    "name": "Skills & Techniques",
    "slug": "skills-techniques",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "fkABsGCJZ6y9qConW"
  },
  {
    "_id": "practical-productivity",
    "name": "Productivity",
    "slug": "productivity",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "fkABsGCJZ6y9qConW"
  },
  {
    "_id": "practical-interpersonal",
    "name": "Interpersonal",
    "slug": "interpersonal",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "fkABsGCJZ6y9qConW"
  },
  {
    "_id": "community-all",
    "name": "All",
    "slug": "all",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "izp6eeJJEg9v5zcur"
  },
  {
    "_id": "community-lesswrong",
    "name": "LessWrong",
    "slug": "lesswrong",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "izp6eeJJEg9v5zcur"
  },
  {
    "_id": "other-content-type",
    "name": "Content-Type",
    "slug": "content-type",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "MfpEPj6kJneT9gWT6"
  },
  {
    "_id": "other-format",
    "name": "Format",
    "slug": "format",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "MfpEPj6kJneT9gWT6"
  },
  {
    "_id": "other-cross-category",
    "name": "Cross-Category",
    "slug": "cross-category",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "MfpEPj6kJneT9gWT6"
  },
  {
    "_id": "other-miscellaneous",
    "name": "Miscellaneous",
    "slug": "miscellaneous",
    "postCount": 0,
    "description_html": "",
    "description_length": 0,
    "parentTagId": "MfpEPj6kJneT9gWT6"
  },
  {
    "core-tag": "Rationality",
    "_id": "SJFsFfFhE6m2ThAYJ",
    "name": "Anticipated Experiences",
    "slug": "anticipated-experiences",
    "postCount": 48,
    "description_html": "<p>One principle of rationality is that &quot;<a href=\"https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/a7n8GdKiAZRX86T5A\">beliefs should pay rent in <strong>anticipated experiences</strong></a>.&quot; If you believe in something, what do you expect to be different as a result? What does the belief say should happen, and what does it say should <em>not</em> happen? If you have a verbal disagreement with someone, how does your disagreement cash out in differing expectations?</p><p>If two people try to get specific about the anticipated experiences driving their disagreement, one method for doing so is the <a href=\"https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-resolving-disagreement\">double crux</a> technique. The notion that beliefs are models of what we expect to experience is also one of the basic premises of <a href=\"https://www.lesswrong.com/tag/predictive-processing\">predictive processing</a> theories of how the brain works. Beliefs that do not pay rent may be related to <a href=\"https://www.lesswrong.com/posts/4xKeNKFXFB458f5N8/ethnic-tension-and-meaningless-arguments\">meaningless arguments</a> driven by <a href=\"https://www.lesswrong.com/tag/coalitional-instincts\">coalitional instincts</a>. </p><blockquote> <em>If a tree falls in a forest and no one hears it, does it make a sound? One says, &#x201C;Yes it does, for it makes vibrations in the air.&#x201D; Another says, &#x201C;No it does not, for there is no auditory processing in any brain.&#x201D;</em>  [...]</blockquote><blockquote>Suppose that, after a tree falls, the two arguers walk into the forest together. Will one expect to see the tree fallen to the right, and the other expect to see the tree fallen to the left? Suppose that before the tree falls, the two leave a sound recorder next to the tree. Would one, playing back the recorder, expect to hear something different from the other? Suppose they attach an electroencephalograph to any brain in the world; would one expect to see a different trace than the other?</blockquote><blockquote>Though the two argue, one saying &#x201C;No,&#x201D; and the other saying &#x201C;Yes,&#x201D; they do not anticipate any different experiences. The two think they have different models of the world, but they have no difference with respect to what they expect will <em>happen to</em> them; their maps of the world do not diverge in any sensory detail.</blockquote><blockquote>-- Eliezer Yudkowsky, <a href=\"https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/a7n8GdKiAZRX86T5A\">Making Beliefs Pay Rent (In Anticipated Experiences)</a></blockquote>",
    "description_length": 2608,
    "viewCount": 563,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "WH5ZmNSjZmK9SMj7k",
    "name": "Aumann's Agreement Theorem",
    "slug": "aumann-s-agreement-theorem",
    "postCount": 26,
    "description_html": "<p><strong>Aumann&apos;s agreement theorem</strong>, roughly speaking, says that two agents acting rationally (in a certain precise sense) and with <a href=\"https://www.lesswrong.com/tag/common-knowledge\">common knowledge</a> of each other&apos;s beliefs cannot agree to disagree. More specifically, if two people are genuine <a href=\"https://www.lesswrong.com/tag/bayesianism\">Bayesians</a>, share common <a href=\"https://www.lesswrong.com/tag/priors\">priors</a>, and have common knowledge of each other&apos;s current probability assignments, then they must have equal probability assignments.</p><p><em>Related tags and wikis: </em><a href=\"https://www.lesswrong.com/tag/disagreement\">Disagreement</a>, <a href=\"https://www.lesswrong.com/tag/modesty\">Modesty</a>,  <a href=\"https://www.lesswrong.com/tag/modesty-argument\">Modesty argument</a>, <a href=\"https://www.lesswrong.com/tag/aumann-agreement\">Aumann agreement</a>, <a href=\"https://www.lesswrong.com/tag/the-aumann-game\">The Aumann Game</a></p><h2>Highlighted Posts</h2><ul><li><a href=\"https://www.lesswrong.com/lw/gr/the_modesty_argument/\">The Modesty Argument</a></li><li><a href=\"http://www.overcomingbias.com/2006/12/agreeing_to_agr.html\">Agreeing to Agree</a> by <a href=\"https://en.wikipedia.org/wiki/Hal_Finney_(cypherpunk)\">Hal Finney</a></li><li><a href=\"http://www.overcomingbias.com/2007/01/the_coin_guessi.html\">The Coin Guessing Game</a> by Hal Finney</li><li><a href=\"https://www.lesswrong.com/lw/gq/the_proper_use_of_humility/\">The Proper Use of Humility</a></li><li><a href=\"http://www.overcomingbias.com/2006/12/meme_lineages_a.html\">Meme Lineages and Expert Consensus</a> by <a href=\"https://www.lesswrong.com/tag/carl-shulman\">Carl Shulman</a> (OB)</li><li><a href=\"https://www.lesswrong.com/lw/1il/probability_space_aumann_agreement/\">Probability Space &amp; Aumann Agreement</a> by <a href=\"http://weidai.com/\">Wei Dai</a></li><li><a href=\"https://www.lesswrong.com/lw/i5/bayesian_judo/\">Bayesian Judo</a></li></ul><h2>External Links</h2><ul><li><a href=\"https://web.archive.org/web/20110725162431/http://dl.dropbox.com:80/u/34639481/Aumann_agreement_theorem.pdf\">A write-up of the proof of Aumann&apos;s agreement theorem</a> (pdf) by Tyrrell McAllister</li></ul><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/disagreement\">Disagreement</a></li><li><a href=\"https://www.lesswrong.com/tag/modesty-argument\">Modesty argument</a></li><li><a href=\"https://www.lesswrong.com/tag/aumann-agreement\">Aumann agreement</a></li><li><a href=\"https://www.lesswrong.com/tag/the-aumann-game\">The Aumann Game</a></li><li><a href=\"http://www.overcomingbias.com/tag/disagreement\">Overcoming Bias posts on &quot;Disagreement&quot;</a></li></ul><h2>References</h2><ul><li>(<a href=\"http://www.ma.huji.ac.il/~raumann/pdf/Agreeing%20to%20Disagree.pdf\">PDF</a>)</li><li>(<a href=\"http://hanson.gmu.edu/deceive.pdf\">PDF</a>, <a href=\"http://www.newmedia.ufm.edu/gsm/index.php?title=Are_Disagreements_Honest%3F\">Talk video</a>)</li></ul>",
    "description_length": 3013,
    "viewCount": 484,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "LhX3F2SvGDarZCuh6",
    "name": "Bayes' Theorem",
    "slug": "bayes-theorem",
    "postCount": 174,
    "description_html": "<p><strong>Bayes' Theorem</strong> (also known as Bayes' Law) is a law of probability that describes the proper way to incorporate new evidence into prior probabilities to form an updated probability estimate. It is commonly regarded as the foundation of consistent rational reasoning under uncertainty. Bayes Theorem is named after Reverend Thomas Bayes who proved the theorem in 1763.&nbsp;</p><p>Bayes' theorem commonly takes the form:</p><span><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"{\\displaystyle P(A|B)={\\frac {P(B|A)\\,P(A)}{P(B)}}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mstyle\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 5.962em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 5.962em; top: -1.59em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"width: 5.962em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 5.962em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.68em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span><p>where A is the proposition of interest, B is the observed evidence, P(A) and P(B) are prior probabilities, and P(A|B) is the posterior probability of A.</p><p>With the posterior odds, the prior odds and the <a href=\"https://wiki.lesswrong.com/wiki/Likelihood_ratio\"><u>likelihood ratio</u></a> written explicitly, the theorem reads:</p><span><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"{\\displaystyle {\\frac {P(A|B)}{P(\\neg A|B)}}={\\frac {P(A)}{P(\\neg A)}}\\cdot {\\frac {P(B|A)}{P(B|\\neg A)}}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mstyle\"><span class=\"mjx-mrow\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.183em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 4.183em; top: -1.59em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"width: 4.183em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">¬</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.183em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.68em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 3.146em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 3.146em; top: -1.59em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"width: 3.146em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">¬</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 3.146em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.68em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">⋅</span></span><span class=\"mjx-texatom MJXc-space2\"><span class=\"mjx-mrow\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.183em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 4.183em; top: -1.59em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span class=\"mjx-denominator\" style=\"width: 4.183em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">¬</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.183em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.68em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span></span></span></span></span></span></span><p>&nbsp;</p><h2>Visualisation of Bayes' Rule</h2><figure class=\"image image_resized\" style=\"width:100%\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LhX3F2SvGDarZCuh6/hmsnhsz2ei6beeylrwv2\" alt=\"Bayes.png\"></figure>",
    "description_length": 29880,
    "viewCount": 982,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "wfW6iL96u26mbatep",
    "name": "Bounded Rationality",
    "slug": "bounded-rationality",
    "postCount": 28,
    "description_html": "<p><strong>Bounded Rationality</strong> is rationality for bounded agents. Not (primarily) about &quot;modelling irrationality&quot;: may include models of irrational behavior, but the aspiration of bounded rationality is to explain why this is in some sense the best a bounded agent can do, or, a rational approach for a bounded agent to take given its limited resources and knowledge. In other words, bounded rationality is a type of rationality, not a type of irrationality.</p>",
    "description_length": 481,
    "viewCount": 160,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "tZsfB6WfpRy6kFb6q",
    "name": "Conservation of Expected Evidence",
    "slug": "conservation-of-expected-evidence",
    "postCount": 20,
    "description_html": "<p><strong>Conservation of Expected Evidence</strong> is a consequence of probability theory which states that for every expectation of evidence, there is an equal and opposite expectation of counter-evidence [<a href=\"https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence\">1</a>]. Conservation of Expected Evidence is about both the direction of the update and its magnitude: a low probability of seeing strong evidence in one direction must be balanced by a high probability of observing weak counter-evidence in the other direction [<a href=\"https://www.lesswrong.com/s/uLEjM2ij5y3CXXW6c/p/zTfSXQracE7TW8x4w#1___You_can_t_predict_that_you_ll_update_in_a_particular_direction__\">2</a>]. The mere <i>expectation</i> of encountering evidence–before you've actually seen it–should not shift your prior beliefs. It also goes by other names, including <i>the</i> <a href=\"https://en.wikipedia.org/wiki/Law_of_total_expectation\"><i>law of total expectation</i></a> and <i>the law of iterated expectations</i>.</p><p>A consequence of this principle is that <a href=\"https://www.lesswrong.com/posts/mnS2WYLCGJP2kQkRn/absence-of-evidence-is-evidence-of-absence\">absence of evidence is evidence of absence</a>.</p><p>Consider a hypothesis H and evidence (observation) E. <a href=\"https://wiki.lesswrong.com/wiki/Prior\">Prior</a> <a href=\"https://wiki.lesswrong.com/wiki/probability\">probability</a> of the hypothesis is P(H); <a href=\"https://wiki.lesswrong.com/wiki/posterior\">posterior</a> probability is either P(H|E) or P(H|¬E), depending on whether you observe E or not-E (evidence or counter-evidence). The probability of observing E is P(E), and probability of observing not-E is P(¬E). Thus, <a href=\"https://lesswrong.com/tag/expected-value\">expected value</a> of the posterior probability of the hypothesis is:</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(H|E) ⋅ P(E) + P(H|\\neg E) ⋅ P(\\neg E)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">⋅</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">¬</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">⋅</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">¬</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span></p><p>But the prior probability of the hypothesis itself can be trivially broken up the same way:</p><span><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"\\begin{align}\nP(H) &amp; = P(H,E) + P(H,\\neg{E}) \\\\\n&amp; = P(H|E) \\cdot P(E) + P(H|\\neg{E}) \\cdot P(\\neg{E})\n\\end{align}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mtable\" style=\"vertical-align: -0.975em; padding: 0px 0.167em;\"><span class=\"mjx-table\"><span class=\"mjx-mtr\" style=\"height: 1.225em;\"><span class=\"mjx-mtd\" style=\"padding: 0px 0px 0px 0px; text-align: right; width: 2.417em;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0px 0px 0px 0px; text-align: left; width: 16.839em;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-mi\"></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">¬</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-strut\"></span></span></span></span><span class=\"mjx-mtr\" style=\"height: 1.225em;\"><span class=\"mjx-mtd\" style=\"padding: 0.15em 0px 0px 0px; text-align: right;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0.15em 0px 0px 0px; text-align: left;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-mi\"></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">⋅</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">¬</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">⋅</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">¬</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-strut\"></span></span></span></span></span></span></span></span></span></span></span><p>Thus, expectation of posterior probability is equal to the prior probability.</p><p>In other way, if you expect the probability of a hypothesis to change as a result of observing some evidence, the amount of this change if the evidence is positive is</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"D_1 = P(H|E) − P(H)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.333em;\"></span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.333em;\"></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span></p><p>If the evidence is negative, the change is</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"D_2 = P(H|\\neg E) − P(H)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.333em;\"></span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.333em;\"></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">¬</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\">H</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span></p><p>Expectation of the change given positive evidence is equal to negated expectation of the change given counter-evidence:</p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"D_1 ⋅ P(E) =  − D_2 ⋅ P(\\neg E)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">⋅</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.333em;\"></span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.333em;\"></span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">D</span></span></span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2</span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">⋅</span></span><span class=\"mjx-mo\"><span class=\"mjx-char\"><span class=\"mjx-charbox MJXc-TeX-main-R\" style=\"margin-right: 0.25em;\"></span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">¬</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\">E</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span></span></span></span></p><p>If you can <i>anticipate in advance</i> updating your belief in a particular direction, then you should just go ahead and update now. Once you know your destination, you are already there.</p><h3>Sequences:</h3><ul><li><a href=\"https://www.lesswrong.com/s/uLEjM2ij5y3CXXW6c\">Filtered Evidence, Filtered Arguments</a> by <a href=\"https://www.lesswrong.com/users/abramdemski\">Abram Demski</a></li></ul><h2>Notable Posts</h2><ul><li><a href=\"https://lesswrong.com/lw/ii/conservation_of_expected_evidence/\">Conservation of Expected Evidence</a></li><li><a href=\"https://lesswrong.com/posts/zTfSXQracE7TW8x4w/mistakes-with-conservation-of-expected-evidence-1\">Mistakes with Conservation of Expected Evidence</a></li></ul><h2>See Also</h2><ul><li><a href=\"https://lesswrong.com/tag/filtered-evidence\">Filtered evidence</a></li></ul>",
    "description_length": 43577,
    "viewCount": 330,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "6Qic6PwwBycopJFNN",
    "name": "Contrarianism",
    "slug": "contrarianism",
    "postCount": 32,
    "description_html": "<p>A <strong>contrarian</strong> is a person who holds a contrary position, especially a position against the <a href=\"https://www.lesswrong.com/tag/consensus\">majority</a> <a href=\"https://en.wikipedia.org/wiki/Contrarian\">(from Wikipedia).</a></p>",
    "description_length": 249,
    "viewCount": 131,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "dPPATLhRmhdJtJM2t",
    "name": "Decision Theory",
    "slug": "decision-theory",
    "postCount": 456,
    "description_html": "<p><strong>Decision theory</strong> is the study of principles and algorithms for making correct decisions—that is, decisions that allow an agent to achieve better outcomes with respect to its goals. Every action at least implicitly represents a decision under uncertainty: in a state of partial knowledge, something has to be done, even if that something turns out to be nothing (call it \"the null action\"). Even if you don't know how you make decisions, decisions do get made, and so there has to be some underlying mechanism. What is it? And how can it be done better? Decision theory has the answers.</p>\n<p><em>Note: this page needs to be updated with content regarding Functional Decision Theory, the latest theory from MIRI.</em></p>\n<p><em>Related:</em> <a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory</a>, <a href=\"https://www.lesswrong.com/tag/robust-agents?showPostCount=true&amp;useTagName=true\">Robust Agents</a>, <a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions</a></p>\n<p>A core idea in decision theory is that of <a href=\"https://lesswrong.com/tag/expected-utility\"><em>expected utility</em></a> <em>maximization</em>, usually intractable to directly calculate in practice, but an invaluable theoretical concept. An agent assigns utility to every possible outcome: a real number representing the goodness or desirability of that outcome. The mapping of outcomes to utilities is called the agent's <em>utility function</em>. (The utility function is said to be invariant under affine transformations: that is, the utilities can be scaled or translated by a constant while resulting in all the same decisions.) For every action that the agent could take, sum over the utilities of the various possible outcomes weighted by their probability: this is the <a href=\"https://lesswrong.com/tag/expected-value\">expected</a> utility of the action, and the action with the highest expected utility is to be chosen.</p>\n<h2>Thought experiments</h2>\n<p>The limitations and pathologies of decision theories can be analyzed by considering the decisions they suggest in the certain idealized situations that stretch the limits of decision theory's applicability. Some of the thought experiments more frequently discussed on <a href=\"https://wiki.lesswrong.com/wiki/LW\">LW</a> include:</p>\n<ul>\n<li><a href=\"https://lesswrong.com/tag/newcomb-s-problem\">Newcomb's problem</a></li>\n<li><a href=\"https://lesswrong.com/tag/counterfactual-mugging\">Counterfactual mugging</a></li>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker\">Parfit's hitchhiker</a></li>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Smoker's_lesion\">Smoker's lesion</a></li>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Absentminded_driver\">Absentminded driver</a></li>\n<li><a href=\"https://lesswrong.com/tag/sleeping-beauty-paradox\">Sleeping Beauty problem</a></li>\n<li><a href=\"https://lesswrong.com/tag/prisoner-s-dilemma\">Prisoner's dilemma</a></li>\n<li><a href=\"https://lesswrong.com/tag/pascal-s-mugging\">Pascal's mugging</a></li>\n</ul>\n<h2>Commonly discussed decision theories</h2>\n<p>Standard theories well-known in academia:</p>\n<ul>\n<li>CDT, <a href=\"http://en.wikipedia.org/wiki/Causal_decision_theory\">Causal Decision Theory</a></li>\n<li>EDT, <a href=\"http://en.wikipedia.org/wiki/Evidential_decision_theory\">Evidential Decision Theory</a></li>\n</ul>\n<p>Theories invented by researchers associated with <a href=\"https://wiki.lesswrong.com/wiki/MIRI\">MIRI</a> and LW:</p>\n<ul>\n<li>FDT: <a href=\"https://intelligence.org/2017/10/22/fdt/\">Functional Decision Theory</a></li>\n<li>TDT, <a href=\"https://lesswrong.com/tag/timeless-decision-theory\">Timeless Decision Theory</a></li>\n<li>UDT, <a href=\"https://lesswrong.com/tag/updateless-decision-theory\">Updateless Decision Theory</a></li>\n<li>ADT: <a href=\"https://lesswrong.com/tag/ambient-decision-theory\">Ambient Decision Theory</a> (a variant of UDT)</li>\n<li>FDT: <a href=\"https://intelligence.org/files/DeathInDamascus.pdf\">Cheating Death in Damascus</a></li>\n</ul>\n<p>Other decision theories are listed in <a href=\"https://casparoesterheld.com/a-comprehensive-list-of-decision-theories/\">A comprehensive list of decision theories</a>.</p>\n<h2>Blog posts</h2>\n<ul>\n<li><a href=\"https://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/\">Terminal Values and Instrumental Values</a></li>\n<li><a href=\"https://lesswrong.com/lw/aq9/decision_theories_a_less_wrong_primer/\">Decision Theories: A Less Wrong Primer</a> by orthonormal</li>\n<li><a href=\"https://lesswrong.com/lw/gu1/decision_theory_faq/\">Decision Theory FAQ</a> by lukeprog and crazy88</li>\n</ul>\n<h2>Sequence by <a href=\"https://wiki.lesswrong.com/wiki/AnnaSalamon\">AnnaSalamon</a></h2>\n<ul>\n<li><a href=\"https://lesswrong.com/lw/16f/decision_theory_an_outline_of_some_upcoming_posts/\">Decision theory: An outline of some upcoming posts</a></li>\n<li><a href=\"https://lesswrong.com/lw/16i/confusion_about_newcomb_is_confusion_about/\">Confusion about Newcomb is confusion about counterfactuals</a></li>\n<li><a href=\"https://lesswrong.com/lw/174/decision_theory_why_we_need_to_reduce_could_would/\">Why we need to reduce “could”, “would”, “should”</a></li>\n<li><a href=\"https://lesswrong.com/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/\">Why Pearl helps reduce “could” and “would”, but still leaves us with at least three alternatives</a></li>\n</ul>\n<h2>Sequence by <a href=\"http://lesswrong.com/user/orthonormal/\">orthonormal</a> (Decision Theories: A Semi-Formal Analysis)</h2>\n<ul>\n<li><a href=\"https://lesswrong.com/lw/aq9/decision_theories_a_less_wrong_primer/\">Part 0: Decision Theories: A Less Wrong Primer</a></li>\n<li><a href=\"https://lesswrong.com/lw/axl/decision_theories_a_semiformal_analysis_part_i/\">Part I: The Problem with Naive Decision Theory</a></li>\n<li><a href=\"https://lesswrong.com/lw/az6/decision_theories_a_semiformal_analysis_part_ii/\">Part II: Causal Decision Theory and Substitution</a></li>\n<li><a href=\"https://lesswrong.com/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/\">Part III: Formalizing Timeless Decision Theory</a></li>\n</ul>\n<h2>See also</h2>\n<ul>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Instrumental_rationality\">Instrumental rationality</a></li>\n<li><a href=\"https://lesswrong.com/tag/causality\">Causality</a></li>\n<li><a href=\"https://lesswrong.com/tag/expected-utility\">Expected utility</a></li>\n<li><a href=\"https://lesswrong.com/tag/evidential-decision-theory\">Evidential Decision Theory</a></li>\n<li><a href=\"https://lesswrong.com/tag/timeless-decision-theory\">Timeless decision theory</a>, <a href=\"https://lesswrong.com/tag/updateless-decision-theory\">Updateless decision theory</a></li>\n<li><a href=\"https://lesswrong.com/tag/aixi\">AIXI</a></li>\n</ul>\n",
    "description_length": 6860,
    "viewCount": 965,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "xgpBASEThXPuKRhbS",
    "name": "Epistemology",
    "slug": "epistemology",
    "postCount": 359,
    "description_html": "<p><strong>Epistemology</strong> is the study of how we know the world. It's both a topic in philosophy and a practical concern for how we come to believe things are true.</p><p><strong>Related Sequences:</strong> <a href=\"https://www.lesswrong.com/s/SqFbMbtxGybdS2gRs\">Highly Advanced Epistemology 101 for Beginners</a>, <a href=\"https://www.lesswrong.com/s/FYMiCeXEgMzsB5stm\">Concepts in formal epistemology</a>, <a href=\"https://www.lesswrong.com/s/GTEay24Lxm3xoE4hy\">Novum Organum</a></p>",
    "description_length": 492,
    "viewCount": 314,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "b8FHrKqyXuYGWc6vn",
    "name": "Game Theory",
    "slug": "game-theory",
    "postCount": 316,
    "description_html": "<p><strong>Game theory</strong> is the formal study of how rational actors interact to pursue incentives. It investigates situations of conflict and cooperation.</p>\n<p><em>See also:</em> <a href=\"https://www.lesswrong.com/tag/coordination-cooperation?showPostCount=true&amp;useTagName=true\">Coalition/coordination</a>, <a href=\"https://www.lesswrong.com/tag/coalitional-instincts?showPostCount=true&amp;useTagName=true\">Coalitional Instincts</a>, <a href=\"https://www.lesswrong.com/tag/decision-theory\">Decision theory</a>, <a href=\"https://www.lesswrong.com/tag/moloch?showPostCount=true&amp;useTagName=true\">Moloch</a>, <a href=\"https://www.lesswrong.com/tag/utility-functions\">Utility functions</a>, <a href=\"https://lesswrong.com/tag/decision-theory\">Decision Theory</a>, <a href=\"https://lesswrong.com/tag/prisoner-s-dilemma\">Prisoner's Dilemma</a></p>\n<p>Game theory is an extremely powerful and robust tool in analyzing much more complex situations, such as: mergers and acquisitions, political economy, voting systems, war bargaining and biological evolution. Eight game-theorists have won the Nobel Prize in Economic Sciences.</p>\n<h2>References</h2>\n<ul>\n<li><a href=\"http://levine.sscnet.ucla.edu/general/whatis.htm\">Naïve introduction to Game Theory</a></li>\n<li><a href=\"http://plato.stanford.edu/entries/game-theory/\">Stanford Encyclopedia entry on Game Theory</a></li>\n</ul>\n",
    "description_length": 1391,
    "viewCount": 896,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "Q6hq54EXkrw8LQQE7",
    "name": "Gears-Level",
    "slug": "gears-level",
    "postCount": 64,
    "description_html": "<p>A <strong>gears-level </strong>model is 'well-constrained' in the sense that there is a strong connection between each of the things you observe-- it would be hard for you to imagine that one of the variables could be different while all of the others remained the same. </p><p><em>Related Tags: <a href=\"https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&amp;useTagName=true\">Anticipated Experiences</a>, <a href=\"https://www.lesswrong.com/tag/double-crux\">Double-Crux</a>, <a href=\"https://www.lesswrong.com/tag/empiricism?showPostCount=true&amp;useTagName=true\">Empiricism</a>, <a href=\"https://www.lesswrong.com/tag/falsifiability?showPostCount=true&amp;useTagName=true\">Falsifiability</a>, <a href=\"https://www.lesswrong.com/tag/map-and-territory?showPostCount=true&amp;useTagName=true\">Map and Territory</a></em></p><br><p>The term <strong>gears-level</strong> was first described on LW in the post <a href=\"https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding\">\"Gears in Understanding\"</a>:</p><blockquote>This property is <em>how deterministically interconnected the variables of the model are</em>. There are a few <a href=\"https://en.wikipedia.org/wiki/Goodhart%27s_law\">tests</a> I know of to see to what extent a model has this property, though I don't know if this list is exhaustive and would be a little surprised if it were:</blockquote><blockquote> 1. Does the model p<a href=\"https://www.lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">ay rent?</a> If it does, and if it were falsified, how much (and how precisely) could you infer other things from the falsification?</blockquote><blockquote> 2. How incoherent is it to imagine that the model is accurate but that a given variable <a href=\"https://www.lesswrong.com/lw/if/your_strength_as_a_rationalist/\">could be different</a>?</blockquote><blockquote> 3. If you knew the model were accurate but you were to forget the value of one variable, <a href=\"https://www.lesswrong.com/lw/la/truly_part_of_you/\">could you rederive it</a>?</blockquote><p>An example from <a href=\"https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding\">Gears in Understanding</a> of a gears-level model is (surprise) a box of gears. If you can see a series of  interlocked gears, alternately turning clockwise, then counterclockwise, and so on, then you're able to anticipate the direction of any given, even if you cannot see it. It would be very difficult to imagine all of the gears turning as they are but only one of them changing direction whilst remaining interlocked. And finally, you would be able to rederive the direction of any given gear if you forgot it. </p><br><p>Note that the author of <a href=\"https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding\">Gears in Understanding</a>, <a href=\"https://www.lesswrong.com/users/valentine\">Valentine</a>, was careful to point out that these tests do not fully <em>define</em> the property 'gears-level', and that  \"Gears-ness is not the same as goodness\"-- there are other things that are valuable in a model, and many things cannot practically be modelled in this fashion. If you intend to use the term it is highly recommended you read the post beforehand, as the concept is not easily defined.</p>",
    "description_length": 3308,
    "viewCount": 243,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "5oDii8KKW53n4HSx4",
    "name": "Hansonian Pre-Rationality",
    "slug": "hansonian-pre-rationality",
    "postCount": 8,
    "description_html": "<p>In defining <strong>Hansonian Pre-Rationality </strong>Robin Hanson offers an intriguing&nbsp;argument that, upon learning that our beliefs were created by an irrational process (be it a religious upbringing or a genetic predisposition to paranoid depression), we should update to agree with the alternate version of ourselves who could have had different beliefs. Agents who agree with alternate selves in this way are \"pre-rational\". (NOTE: not to be confused with \"pre-rational\" meaning \"not yet rational\" or \"less than rational\".)</p><p>Suppose you are an AI who was designed by a drunk programmer. Your prior contains an \"optimism\" parameter which broadly skews how you see the world -- set it to -100 and you'd expect world-ending danger around every corner, while +100 would make you expect heaven around every corner. Although your powerful learning algorithm allows you to accurately predict the world, the optimism/pessimism bias never fully goes away: it skews your views about anything you <i>don't</i> know.</p><p>Unfortunately for you, your programmer set the parameter randomly, rather than attempting to figure out which setting was most accurate or useful. You know for a fact they just mashed the num pad randomly.</p><p>How should you think about this?</p>",
    "description_length": 1278,
    "viewCount": 118,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "wdLqQnzdgiYpDXEWH",
    "name": "Infra-Bayesianism",
    "slug": "infra-bayesianism",
    "postCount": 56,
    "description_html": "<p><strong>Infra-Bayesianism</strong> is a new approach to <a href=\"https://www.lesswrong.com/tag/epistemology\">epistemology</a> / <a href=\"https://www.lesswrong.com/tag/decision-theory\">decision theory</a> / reinforcement learning theory, which builds on \"imprecise probability\" to solve the problem of prior misspecification / grain-of-truth / nonrealizability which plagues <a href=\"https://www.lesswrong.com/tag/bayesianism\">Bayesianism</a> and Bayesian reinforcement learning.</p><p>Infra-Bayesianism also naturally leads to an implementation of <a href=\"https://www.lesswrong.com/tag/updateless-decision-theory\">UDT</a>, and (more speculatively at this stage) has applications to multi-agent theory, <a href=\"https://www.lesswrong.com/tag/embedded-agency\">embedded agency </a>and reflection.</p><p>See the <a href=\"https://www.lesswrong.com/posts/zB4f7QqKhBHa5b37a/introduction-to-the-infra-bayesianism-sequence\">Infra-Bayesianism Sequence</a>.</p>",
    "description_length": 954,
    "viewCount": 318,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "EewHHv3ewvQ3mqbyb",
    "name": "Law-Thinking",
    "slug": "law-thinking",
    "postCount": 20,
    "description_html": "<p><strong>Law-thinking</strong> is an approach in which action and reasoning are thought to have theoretical criteria (laws) specifying the optimal actions and belief adjustments in any given situation. These criteria may be impossible to apply to a situation directly, and one may be forced to use only rough approximations. But one can still evaluate the approximations based on how well they match the optimal criteria.</p><p>The relationship between laws and approximations resembles that of between physics and engineering. Physics specify the laws by which the world works, while engineering tries to find practical solutions as constrained by those laws.</p><p>Some concepts which have been used as theoretical criteria in law-thinking:</p><ul><li><a href=\"https://www.lesswrong.com/tag/bayes-theorem?useTagName=true\">Bayes Theorem</a> is a law of probability that describes the proper way to incorporate new evidence into prior probabilities to form an updated probability estimate.</li><li><a href=\"https://www.lesswrong.com/tag/decision-theory?useTagName=true\">Decision Theory</a> studies the general laws for choosing between actions in any given situation.</li><li><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?useTagName=true\">Solomonoff Induction</a> is a theoretically optimal way of arriving at true beliefs, though impossible to use directly. <a href=\"https://www.lesswrong.com/tag/aixi?useTagName=true\">AIXI</a> is an AI design based on Solomonoff Induction; it is also impossible to build directly, but some approximations exist.</li></ul><p>Note that one can make use of e.g. Bayes Theorem or decision theory without being a law-thinker. Thus, articles covering the above topics do not automatically fall under this tag. A &quot;toolbox-thinker&quot; may use such tools if that seems warranted, <em>without</em> considering them normative standards to compare things against. This difference is discussed in <a href=\"https://www.lesswrong.com/posts/CPP2uLcaywEokFKQG/toolbox-thinking-and-law-thinking\">Toolbox-thinking and Law-thinking</a>.</p>",
    "description_length": 2079,
    "viewCount": 149,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "wMPYFGmhcFg4bSb4Z",
    "name": "Map and Territory",
    "slug": "map-and-territory",
    "postCount": 67,
    "description_html": "<p>Our mental frameworks for understanding the world often differ from reality itself. It's easy to mistake the map that we hold in our minds for the actual territory it's representing. Recognizing the difference between the map and the territory is crucial for clear thinking, open-mindedness, and avoiding biases. <br><br>Other names for this concept include:<br><br>• Theory vs reality <br>• Beliefs vs facts <br>• Mental models vs what's really there <br><br>For example, we might assume that all neighborhoods are as safe or unsafe as the one we grew up in, or that all jobs involve the same daily tasks as our current one. But those are just maps in our mind - the territory, or what's really out there, differs. Updating our mental maps to more closely match the territory leads to better choices and less surprising encounters with the unexpected.<br><br>The \"map is not the territory\" is a key idea in fields like productivity, rational thinking, and cognitive psychology. Keeping an open and curious mindset focused on what's really there rather than our preconceptions helps us achieve our goals and gain a more accurate understanding of the world.</p><p>[edited with Claude]</p>",
    "description_length": 1190,
    "viewCount": 1403,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "fihKHQuS5WZBJgkRm",
    "name": "Newcomb's Problem",
    "slug": "newcomb-s-problem",
    "postCount": 69,
    "description_html": "<p><strong>Newcomb's Problem</strong> is a thought experiment in decision theory exploring problems posed by having other agents in the environment who can predict your actions.</p><h2>The Problem</h2><p>From <a href=\"https://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality\">Newcomb's Problem and Regret of Rationality</a>:</p><blockquote><p>A superintelligence from another galaxy, whom we shall call Omega, comes to Earth and sets about playing a strange little game. In this game, Omega selects a human being, sets down two boxes in front of them, and flies away.</p></blockquote><blockquote><p>Box A is transparent and contains a thousand dollars.<br>Box B is opaque, and contains either a million dollars, or nothing.</p></blockquote><blockquote><p>You can take both boxes, or take only box B.</p></blockquote><blockquote><p>And the twist is that Omega has put a million dollars in box B iff Omega has predicted that you will take only box B.</p></blockquote><blockquote><p>Omega has been correct on each of 100 observed occasions so far - everyone who took both boxes has found box B empty and received only a thousand dollars; everyone who took only box B has found B containing a million dollars. (We assume that box A vanishes in a puff of smoke if you take only box B; no one else can take box A afterward.)</p></blockquote><blockquote><p>Before you make your choice, Omega has flown off and moved on to its next game. Box B is already empty or already full.</p></blockquote><blockquote><p>Omega drops two boxes on the ground in front of you and flies off.</p></blockquote><blockquote><p>Do you take both boxes, or only box B?</p></blockquote><p>One line of reasoning about the problem says that because Omega has already left, the boxes are set and you can't change them. And if you look at the payoff matrix, you'll see that whatever decision Omega has already made, you get $1000 more for taking both boxes. This makes taking two boxes (\"two-boxing\") a dominant strategy and therefore the correct choice. Agents who reason this way do not make very much money playing this game. This is because this line of reasoning ignores the connection between the agent and Omega's prediction: two-boxing only makes $1000 more than one-boxing if Omega's prediction is the same in both cases, while the problem states Omega is extremely accurate in its predictions. Switching from one-boxing to two-boxing doesn't give the agent a $1000 more, it results in a loss of $999,000.</p><p>Because the agent's decision in this problem can't causally affect Omega's prediction (which happened in the past), <a href=\"https://www.lesswrong.com/tag/causal-decision-theory\">Causal Decision Theory</a> two-boxes. One-boxing is correlated with getting a million dollars, whereas two-boxing is correlated with getting only $1000; therefore, <a href=\"https://www.lesswrong.com/tag/evidential-decision-theory\">Evidential Decision Theory</a> one-boxes. <a href=\"https://www.lesswrong.com/tag/functional-decision-theory\">Functional Decision Theory</a> (FDT) also one-boxes, but for a completely different reason: FDT reasons that Omega must have had a model of the agent's decision procedure in order to make the prediction. Therefore, your decision procedure is run not only by you, but also (in the past) by Omega; whatever you decide, Omega's model must have decided the same. Either both you and Omega's model two-box, or both you and Omega's model one-box; of these two options, the latter is preferable, so FDT one-boxes.</p><p>The general class of decision problems that involve other agents predicting your actions are called Newcomblike Problems.</p><h2>Irrelevance of Omega's Physical Impossibility</h2><p>Sometimes people dismiss Newcomb's problem because of the physical impossibility of a being like Omega. However, Newcomb's problem does not actually depend on the possibility of Omega in order to be relevant. Similar issues arise if we imagine a skilled human psychologist who can predict other people's actions with 65% accuracy in similar situations.</p><h2>Notable Posts</h2><ul><li><a href=\"https://lessestwrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's Problem and Regret of Rationality</a></li><li><a href=\"https://lessestwrong.com/lw/7v/formalizing_newcombs/\">Formalizing Newcomb's</a></li><li><a href=\"https://lessestwrong.com/lw/90/newcombs_problem_standard_positions/\">Newcomb's Problem standard positions</a></li><li><a href=\"https://lessestwrong.com/lw/6r/newcombs_problem_vs_oneshot_prisoners_dilemma/\">Newcomb's Problem vs. One-Shot Prisoner's Dilemma</a></li><li><a href=\"https://lessestwrong.com/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/\">Decision theory: Why Pearl helps reduce “could” and “would”, but still leaves us with at least three alternatives</a></li></ul><h2>See Also</h2><ul><li><a href=\"https://lessestwrong.com/tag/decision-theory\">Decision theory</a></li><li><a href=\"https://lessestwrong.com/tag/counterfactual-mugging\">Counterfactual mugging</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker\">Parfit's hitchhiker</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Smoker's_lesion\">Smoker's lesion</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Absentminded_driver\">Absentminded driver</a></li><li><a href=\"https://lessestwrong.com/tag/sleeping-beauty-paradox\">Sleeping Beauty problem</a></li><li><a href=\"https://lessestwrong.com/tag/prisoner-s-dilemma\">Prisoner's dilemma</a></li></ul>",
    "description_length": 5524,
    "viewCount": 1251,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "hQiuNkBhn6xxcedTD",
    "name": "Occam's Razor",
    "slug": "occam-s-razor",
    "postCount": 46,
    "description_html": "<p><strong>Occam's razor</strong> (more formally referred to as the principle of parsimony) is a principle commonly stated as \"Entities must not be multiplied beyond necessity\". When several theories are able to explain the same observations, Occam's razor suggests the simpler one is preferable. It must be noted that Occam's razor is a requirement for the simplicity of <i>theories</i>, not for the size of the systems described by those theories. For example, the immensity of the Universe isn't at odds with the principle of Occam's razor.</p><p>Occam's razor is necessitated by the conjunction rule of probability theory: the conjunction A and B is necessarily less (or equally, in the case of logical equivalence) probable than the A alone; <a href=\"https://www.lesswrong.com/tag/burdensome-details\">every detail you tack onto your story drives the probability down</a>.</p><p>Occam's razor has been formalized as Minimum Description Length or Minimum Message Length, in which the total size of the theory is the length of the message required to describe the theory, plus the length of the message required to describe the evidence <i>using</i> the theory. <a href=\"https://www.lesswrong.com/tag/solomonoff-induction\">Solomonoff induction</a> is the ultimate case of <a href=\"https://wiki.lesswrong.com/wiki/minimum_message_length\">minimum message length</a> in which the code for messages can describe all computable hypotheses. This has jokingly been referred to as \"<a href=\"https://www.lesswrong.com/tag/solomonoff-induction\">Solomonoff's lightsaber</a>\".</p><h2>Notable Posts</h2><ul><li><a href=\"https://www.lesswrong.com/lw/jp/occams_razor/\">Occam's Razor</a></li><li><a href=\"https://www.lesswrong.com/lw/k2/a_priori/\">A Priori</a></li><li><a href=\"https://www.lesswrong.com/lw/q3/decoherence_is_simple/\">Decoherence is Simple</a></li></ul><h2>See Also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/solomonoff-induction\">Solomonoff induction</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Occam's_imaginary_razor\">Occam's imaginary razor</a></li><li><a href=\"https://www.lesswrong.com/tag/priors\">Priors</a></li><li><a href=\"https://www.lesswrong.com/tag/burdensome-details\">Burdensome details</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Egan's_law\">Egan's law</a></li></ul><h2>External Links</h2><ul><li><a href=\"http://www.andrew.cmu.edu/user/kk3n/ockham/Ockham.htm\">Ockham’s Razor: A New Justification</a></li></ul>",
    "description_length": 2453,
    "viewCount": 297,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "wBoHTJs9iQzczNtW3",
    "name": "Robust Agents",
    "slug": "robust-agents",
    "postCount": 40,
    "description_html": "<p><strong>Robust Agents</strong> are decision-makers who can perform well in a variety of situations. Whereas some humans rely on folk wisdom or instinct, and some AIs might be designed to achieve a narrow set of goals, a Robust Agent has a coherent set of values and decision-procedures. This enables them to adapt to new circumstances (such as succeeding in a new environment, or responding to a new strategy by a competitor).</p><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/agency\">Agency</a></li></ul>",
    "description_length": 525,
    "viewCount": 137,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "bTeiZr6YAEaSPQTC8",
    "name": "Solomonoff Induction",
    "slug": "solomonoff-induction",
    "postCount": 69,
    "description_html": "<p><strong>Solomonoff induction </strong>is an inference system defined by <a href=\"https://en.wikipedia.org/wiki/Ray_Solomonoff\">Ray Solomonoff</a> that will learn to correctly predict any computable sequence with only the absolute minimum amount of data. This system, in a certain sense, is the perfect universal prediction algorithm.&nbsp;</p><p>To summarize it very informally, Solomonoff induction works by:</p><ul><li>Starting with all possible hypotheses (sequences) as represented by computer programs (that generate those sequences), weighted by their simplicity (2<sup>-</sup><strong><sup>n</sup></strong>, where <strong>n</strong> is the program length);</li><li>Discarding those hypotheses that are inconsistent with the data.</li></ul><p>Weighting hypotheses by simplicity, the system automatically incorporates a form of <a href=\"https://www.lesswrong.com/tag/occam-s-razor\">Occam's razor</a>, which is why it has been playfully referred to as <i>Solomonoff's lightsaber</i>.</p><p>Solomonoff induction gets off the ground with a solution to the \"problem of the priors\". Suppose that you stand before a universal <a href=\"http://www.scholarpedia.org/article/Algorithmic_complexity#Prefix_Turing_machine\">prefix Turing machine</a> <i>U</i>. You are interested in a certain finite output string <i>y</i><sub>0</sub>. In particular, you want to know the probability that <i>U</i> will produce the output <i>y</i><sub>0</sub> given a random input tape. This probability is the <strong>Solomonoff </strong><i><strong>a priori</strong></i><strong> probability</strong> of <i>y</i><sub>0</sub>.</p><p>More precisely, suppose that a particular infinite input string <i>x</i><sub>0</sub> is about to be fed into <i>U</i>. However, you know nothing about <i>x</i><sub>0</sub> other than that each term of the string is either 0 or 1. As far as your state of knowledge is concerned, the <i>i</i>th digit of <i>x</i><sub>0</sub> is as likely to be 0 as it is to be 1, for all <i>i</i> = 1, 2, …. You want to find the <i>a priori</i> probability <i>m</i>(<i>y</i><sub>0</sub>) of the following proposition:</p><p>(*) If <i>U</i> takes in <i>x</i><sub>0</sub> as input, then <i>U</i> will produce output <i>y</i><sub>0</sub> and then halt.</p><p>Unfortunately, computing the exact value of <i>m</i>(<i>y</i><sub>0</sub>) would require solving the halting problem, which is undecidable. Nonetheless, it is easy to derive an expression for <i>m</i>(<i>y</i><sub>0</sub>). If <i>U</i> halts on an infinite input string <i>x</i>, then <i>U</i> must read only a finite initial segment of <i>x</i>, after which <i>U</i> immediately halts. We call a finite string <i>p</i> a <i>self-delimiting program</i> if and only if there exists an infinite input string <i>x</i> beginning with <i>p</i> such that <i>U</i> halts on <i>x</i> immediately after reading to the end of <i>p</i>. The set 𝒫 of self-delimiting programs is the <i>prefix code</i> for <i>U</i>. It is the determination of the elements of 𝒫 that requires a solution to the halting problem.</p><p>Given <i>p</i> ∈ 𝒫, we write \"prog (<i>x</i><sub>0</sub>) = <i>p</i>\" to express the proposition that <i>x</i><sub>0</sub> begins with <i>p</i>, and we write \"<i>U</i>(<i>p</i>) = <i>y</i><sub>0</sub>\" to express the proposition that <i>U</i> produces output <i>y</i><sub>0</sub>, and then halts, when fed any input beginning with <i>p</i>. Proposition (*) is then equivalent to the exclusive disjunction</p><p><br>⋁<i><sub>p</sub></i><sub> ∈ 𝒫: </sub><i><sub>U</sub></i><sub>(</sub><i><sub>p</sub></i><sub>) = </sub><i><sub>y</sub></i><sub>0</sub>(prog (<i>x</i><sub>0</sub>) = <i>p</i>).<br>Since <i>x</i><sub>0</sub> was chosen at random from {0, 1}<i><sup>ω</sup></i>, we take the probability of prog (<i>x</i><sub>0</sub>) = <i>p</i> to be 2<sup> − ℓ(</sup><i><sup>p</sup></i><sup>)</sup>, where ℓ(<i>p</i>) is the length of <i>p</i> as a bit string. Hence, the probability of (*) is</p><p><br><i>m</i>(<i>y</i><sub>0</sub>) := ∑<i><sub>p</sub></i><sub> ∈ 𝒫: </sub><i><sub>U</sub></i><sub>(</sub><i><sub>p</sub></i><sub>) = </sub><i><sub>y</sub></i><sub>0</sub>2<sup> − ℓ(</sup><i><sup>p</sup></i><sup>)</sup>.<br>&nbsp;</p><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/kolmogorov-complexity\">Kolmogorov complexity</a></li><li><a href=\"https://www.lesswrong.com/tag/aixi\">AIXI</a></li><li><a href=\"https://www.lesswrong.com/tag/occam-s-razor\">Occam's razor</a></li></ul><h2>References</h2><ul><li><a href=\"http://www.scholarpedia.org/article/Algorithmic_probability\">Algorithmic probability</a> on Scholarpedia</li></ul>",
    "description_length": 4593,
    "viewCount": 578,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "LnEEs8xGooYmQ8iLA",
    "name": "Truth, Semantics, & Meaning",
    "slug": "truth-semantics-and-meaning",
    "postCount": 145,
    "description_html": "<p><strong>Truth, Semantics, and Meaning</strong>: What does it mean to assert that something is true? A very popular answer is <a href=\"https://www.lesswrong.com/tag/map-and-territory\">map-territory correspondence theory</a>. But the details of this theory are not clear, and there are other contenders.</p><h2>Truth as Correspondence</h2><p>Many consider truth as the correspondence between reality and one's beliefs about reality. Within this frame, truth itself is not necessarily limited to one's belief about something. For a statement/ideal/proposed fact to be considered \"true,\" you must take it as its definition. Truth doesn't imply that something has to be proven in order for it to be made true, but that the statement/ideal/proposed fact has to be true all of the time, regardless of one's belief.</p><p>Alfred Tarski defined truth in terms of an infinite family of sentences such as:</p><blockquote><p>The sentence 'snow is white' is <i>true</i> if and only if snow is white.</p></blockquote><p>To understand whether a belief is true, we need (only) to understand what possible states of the world would make it true or false, and then ask directly about the world. Often, people assume that ideals and morals change with culture; as they tend to do. Unfortunately, many people struggle with their belief of \"truth\" based on their religion. Because of their belief, they object the currently accepted \"truth\" about the world, about life (how we all got here), and most importantly, what is considered \"right\" or \"wrong.\"</p><p>\"Truth\" is not, however, a determination. Truth is not simply a belief. Truth is an ideal, concept, or fact that can be observed. Whether an individual has a belief derived from their religion on what is truth or not, unless they have observed it, they cannot prove whether their belief is truth or not. Reiterating from above: the lack of proof or justification, or even rationalization, does not change the status of truth. What's truth is truth, and what is false, is false. Humans simply decide to reject notions and proposed facts as truth if they are not observable, or are not able to show any proof.</p><p>'Truth' is a very simple concept, understood perfectly well by three-year-olds, but often made unnecessarily complicated by adults.</p><h2>Other Theories of Truth</h2><p>&lt;needed&gt;</p><h2>Notable Posts</h2><ul><li><a href=\"https://www.lesswrong.com/lw/eqn/the_useful_idea_of_truth/\">The Useful Idea of Truth</a> - A basic guide to what 'truth' means.</li><li><a href=\"https://www.lesswrong.com/lw/go/why_truth_and/\">Why truth? And...</a> - You have an instrumental motive to care about the truth of your <i>beliefs about</i> anything you care about.</li><li><a href=\"https://www.lesswrong.com/lw/lz/guardians_of_the_truth/\">Guardians of the Truth</a> - Endorsing a concept of truth is not the same as endorsing a particular belief as eternally, absolutely, knowably true.</li><li><a href=\"https://www.lesswrong.com/lw/hp/feeling_rational/\">Feeling Rational</a> - Emotions cannot be true or false, but they can follow from true or false beliefs.</li><li><a href=\"https://www.lesswrong.com/lw/jz/the_meditation_on_curiosity/\">The Meditation on Curiosity</a> - In particular, the <a href=\"https://www.lesswrong.com/tag/litany-of-tarski\">Litany of Tarski</a>.</li><li><a href=\"https://www.lesswrong.com/lw/sf/fake_norms_or_truth_vs_truth/\">Fake Norms, or \"Truth\" vs. Truth</a> - Our society has a moral norm for applauding \"truth\", but actual truths get much less applause (this is a bad thing).</li></ul><h2>External links</h2><ul><li><a href=\"http://yudkowsky.net/rational/the-simple-truth\">The Simple Truth</a></li></ul><h2>See also</h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Epistemic_rationality\">Epistemic rationality</a></li><li><a href=\"https://www.lesswrong.com/tag/litany-of-tarski\">Litany of Tarski</a>, <a href=\"https://www.lesswrong.com/tag/litany-of-gendlin\">Litany of Gendlin</a></li><li><a href=\"https://www.lesswrong.com/tag/self-deception\">Self-deception</a></li><li><a href=\"https://www.lesswrong.com/tag/belief\">Belief</a></li><li><a href=\"https://www.lesswrong.com/tag/highly-advanced-epistemology-101-for-beginners\">Highly Advanced Epistemology 101 for Beginners</a></li></ul>",
    "description_length": 4263,
    "viewCount": 171,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "HAFdXkW4YW4KRe2Gx",
    "name": "Utility Functions",
    "slug": "utility-functions",
    "postCount": 187,
    "description_html": "<p>A <strong>utility function</strong> assigns numerical values (\"utilities\") to outcomes, in such a way that outcomes with higher utilities are absolutely always <u><a href=\"http://lesswrong.com/tag/preference\">preferred</a></u> to outcomes with lower utilities, with no exceptions; the lack of exploitable holes in the preference ordering is necessary for the definition and separates utility from mere reward.</p><p><em>See also: </em><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value</a>, <a href=\"https://www.lesswrong.com/tag/decision-theory\">Decision Theory</a>, <a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory</a>, <a href=\"https://www.lesswrong.com/tag/orthogonality-thesis/\">Orthogonality Thesis</a>, <a href=\"http://lesswrong.com/tag/utilitarianism\">Utilitarianism</a>, <a href=\"https://www.lesswrong.com/tag/preference\">Preference</a>, <a href=\"https://www.lesswrong.com/tag/utility\">Utility</a>, <a href=\"https://www.lesswrong.com/tag/vnm-theorem\">VNM Theorem</a></p><p>Utility Functions do not work very well in practice for individual humans. Human drives are not coherent nor is there any reason to think they would converge to a utility-function-grade level of reliability (<a href=\"https://www.lesswrong.com/lw/l3/thou_art_godshatter/\">Thou Art Godshatter</a>), and even people with a strong interest in the concept have trouble working out what their utility function actually is even slightly (<a href=\"https://www.lesswrong.com/lw/zv/post_your_utility_function/\">Post Your Utility Function</a>). Furthermore, humans appear to calculate reward and loss separately - adding one to the other does not predict their behavior accurately, and thus human reward is not human utility. This makes humans highly exploitable - and in fact, not being exploitable would be a minimum requirement in order to qualify as having a coherent utility function.</p><p><a href=\"https://www.lesswrong.com/users/pjeby\">pjeby</a> posits humans' difficulty in understanding their own utility functions as the root of <a href=\"https://www.lesswrong.com/tag/akrasia\">akrasia</a>.</p><p>However, utility functions can be a useful model for dealing with humans in groups, <em>e.g.</em> in economics.</p><p>The <a href=\"https://www.lesswrong.com/tag/vnm-theorem\">VNM Theorem</a> tag is likely to be a strict subtag of the Utility Functions tag, because the VNM theorem establishes when preferences can be represented by a utility function, but a post discussing utility functions may or may not discuss the VNM theorem/axioms.</p><p>Because utility functions arise from VNM rationality, they may still be of note in understanding intelligent systems even when the system does not explicitly store a utility function anywhere, since reducing exploitable error rate should eventually converge to utility-function-like guarantees.</p>",
    "description_length": 2945,
    "viewCount": 335,
    "parentTagId": "rationality-theory-concepts"
  },
  {
    "core-tag": "Rationality",
    "_id": "HXA9WxPpzZCCEwXHT",
    "name": "Alief",
    "slug": "alief",
    "postCount": 24,
    "description_html": "<p>An <strong>alief</strong> is a belief-like attitude, behavior, or expectation that can coexist with a contradictory belief. For example, the fear felt when a monster jumps out of the darkness in a scary movie is based on the alief that the monster is about to attack you, even though you believe that it cannot.</p><p>Philospoher Tamar Gendler introduced the word in her 2008 paper <i>Alief and Belief</i> as a sort of pun on <a href=\"https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2\">dual process theory</a>; what beliefs (\"B-liefs\") are to system 2, aliefs (\"A-liefs\") are to system 1. Thus, beliefs are explicitly held beliefs which inform slow reasoning, while aliefs are implicit attitudes which guide fast reactions. However, dual process theory is not totally necessary to make sense of the term alief.</p><p>Gendler (2008) also introduced a related pun of \"cesire vs desire\"; a desire (\"D-zire\") is an explicit want which enters into explicit planning, while a cesire (\"C-zire\") is an implicit one which guides reactions.</p><p><i>Related tags: </i><a href=\"https://www.lesswrong.com/tag/belief\">Belief</a>, <a href=\"https://www.lesswrong.com/tag/emotions\">Emotion</a>,&nbsp;</p><h2>Blog posts</h2><ul><li><a href=\"https://www.lesswrong.com/lw/1l/the_mystery_of_the_haunted_rationalist/\">The Mystery of the Haunted Rationalist</a></li><li><a href=\"https://www.lesswrong.com/lw/1xh/living_luminously/\">Living Luminously</a></li></ul><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/hollywood-rationality\">Hollywood rationality</a></li><li><a href=\"https://www.lesswrong.com/tag/corrupted-hardware\">Corrupted hardware</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Separate_magisteria\">Separate magisteria</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Living_Luminously_(sequence)\">Living Luminously (sequence)</a></li></ul>",
    "description_length": 1884,
    "viewCount": 277,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "E8PHMuf7tsr8teXAe",
    "name": "Betting",
    "slug": "betting",
    "postCount": 90,
    "description_html": "<p><strong>Betting</strong> is staking money (or some other form of value) on one's beliefs. It is considered rationally virtuous to bet on one's beliefs, as the real stakes force one to actually consider precisely what they <a href=\"https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences\">anticipate</a> will really happen. LessWrong has a culture of betting.</p><p><i>See also:</i> <a href=\"https://www.lesswrong.com/tag/prediction-markets\">Prediction Markets, </a><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\">Forecasting &amp; Prediction, </a><a href=\"https://www.lesswrong.com/tag/forecasts\">Forecasts (Specific Predictions)</a></p><h2>Why is betting important?</h2><p>The argument in favor of betting is that one should generally either accept a proposed bet, in order to make money in expectation, or update their beliefs so the bet becomes unprofitable. There are exceptions to this rule, some theoretical, such as the example of <a href=\"https://www.lesswrong.com/posts/G7HgP9KTWAMSv6oEJ/bets-and-updating\">Omega and Omicron</a>, and some practical, such as uncertainty about whether the bet will be fulfilled. Offering a bet forces someone to think more carefully and share their beliefs more precisely. Losing a bet, even small, can make it more emotionally visceral in a way that might lead to sharpening belief <a href=\"https://www.lesswrong.com/tag/calibration\">calibration</a> more. Bets can be made about beliefs that can be immediately verified or about beliefs that will only be verifiable in the future.</p><p>In popular culture, this idea is often referred to as \"putting one's money where one's mouth is\".</p><p><a href=\"https://marginalrevolution.com/marginalrevolution/2012/11/a-bet-is-a-tax-on-bullshit.html\">A Bet is a Tax on Bullshit</a> mentions that:</p><blockquote><p>In fact, the NYTimes should require that Silver, and other pundits, bet their beliefs. Furthermore, to remove any possibility of manipulation, the NYTimes should escrow a portion of Silver’s salary in a blind trust bet. In other words, the NYTimes should bet a portion of Silver’s salary, at the odds implied by Silver’s model, randomly choosing which side of the bet to take, only revealing to Silver the bet and its outcome after the election is over. A blind trust bet creates incentives for Silver to be disinterested in the outcome but very interested in the accuracy of the forecast.</p></blockquote><p>In <a href=\"https://www.econlib.org/archives/2009/03/what_does_the_b.html\">What Does the Betting Norm Tax?</a>, Bryan Caplan says that such a norm should also be present among scholars.</p><h2>Operationalization for Bets</h2><p><i>Operationalizing a belief</i> is the practice of transforming a belief into a bet with a clear, unambiguous resolution criteria. Sometimes this can be difficult, but there can be ways around some difficulties as explained in <a href=\"https://www.lesswrong.com/posts/LzyN9wzEdfS3j5SmT/tricky-bets-and-truth-tracking-fields\">Tricky Bets and Truth-Tracking Fields</a>. The same challenges are present for prediction markets.</p><h2>Prediction Markets</h2><p>A<a href=\"https://www.lesswrong.com/tag/prediction-markets\"> prediction market</a> is a way for everyone to participate in betting on a particular question. A positive externality of prediction markets, and to a lesser extent bets, is providing a reliable probability on its questions. It can also act as an insurer. <a href=\"https://www.lesswrong.com/posts/ts4KmAR8aJoGMawLb/link-bets-do-not-necessarily-reveal-beliefs\">3</a><a href=\"https://www.lesswrong.com/posts/JDKfPsHvBwgq4Knn9/buy-insurance-bet-against-yourself\">4</a> <a href=\"http://www.truthcoin.info/\">Truthcoin</a>, an idea for a decentralized prediction market, has the slogan \"Making cheap talk expensive\".</p><p><a href=\"http://longbets.org/\">Long Bets</a> is also a useful platform to make certain bets.</p><blockquote><p>The purpose of Long Bets is to improve long–term thinking. Long Bets is a public arena for enjoyably competitive predictions, of interest to society, with philanthropic money at stake. The Long Now Foundation furnishes the continuity to see even the longest bets through to public resolution. This website provides a forum for discussion about what may be learned from the bets and their eventual outcomes.</p></blockquote><p>However, Long Bets hasn't good incentives to make long term bets as explained by Jeff Kaufman in <a href=\"https://www.jefftk.com/p/long-bets-by-confidence-level\">Long Bets by Confidence Level</a>.</p><p>See also:</p><ul><li><a href=\"https://www.lesswrong.com/tag/bets-registry\">LessWiki Bets Registry</a> (outdated)</li></ul><p>External links:</p><ul><li><a href=\"https://www.lesswrong.com/posts/msf7BHMrWTczbQckh/risk-aversion-does-not-explain-people-s-betting-behaviours\">Risk aversion does not explain people's betting behaviours</a></li><li><a href=\"https://www.lesswrong.com/posts/ABMMQ5gSGHwRgExJk/a-method-for-fair-bargaining-over-odds-in-2-player-bets\">A method for fair bargaining over odds in 2 player bets!</a></li></ul>",
    "description_length": 5105,
    "viewCount": 376,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "5f5c37ee1b5cdee568cfb0d6",
    "name": "Cached Thoughts",
    "slug": "cached-thoughts",
    "postCount": 23,
    "description_html": "<p><strong>Cached Thoughts </strong>are ideas, attitudes, and beliefs that a person has formed on some past occasion, and hasn't re-evaluated since then. The name references the concept of a <a href=\"https://en.wikipedia.org/wiki/Cache_(computing)\">cache</a> in computing: a component storing data that has been calculated or retrieved once, so that it is quickly available without needing to be recalculated or re-retrieved.</p><p><i>See also</i>: <a href=\"https://www.lesswrong.com/tag/groupthink\">Groupthink</a>, <a href=\"https://www.lesswrong.com/tag/information-cascades\">Information Cascades</a>, <a href=\"https://www.lesswrong.com/tag/status-quo-bias\">Status quo bias</a>, <a href=\"https://www.lesswrong.com/tag/semantic-stopsign\">Semantic Stopsign</a>, <a href=\"https://wiki.lesswrong.com/wiki/Separate_magisteria\">Separate Magisteria</a>, <a href=\"https://www.lesswrong.com/tag/rationalist-taboo\">Rationalist Taboo</a></p><p>Cached thoughts can be useful in saving computational resources at the cost of some memory load, and also at the risk of maintaining a belief long past the point when evidence should force an update. In particular, cached thoughts can result in a lack of creative approaches to problem-solving, as cached solutions may interfere with the formation of novel ones. What is generally called <a href=\"https://www.lesswrong.com/tag/common-sense\">common sense</a> is more or less a collection of cached thoughts.</p><blockquote><p>In modern civilization particularly, no one can think fast enough to think their own thoughts. If I’d been abandoned in the woods as an infant, raised by wolves or silent robots, I would scarcely be recognizable as human. No one can think fast enough to recapitulate the wisdom of a hunter-gatherer tribe in one lifetime, starting from scratch. As for the wisdom of a literate civilization, forget it.</p><p>But the flip side of this is that I continually see people who aspire to critical thinking, repeating back cached thoughts which were not invented by critical thinkers. – Eliezer Yudkowsky, <a href=\"https://www.lesswrong.com/posts/2MD3NMLBPCqPfnfre/cached-thoughts\">Cached Thoughts</a></p></blockquote><h2>Main Post</h2><ul><li><a href=\"http://lesswrong.com/lw/k5/cached_thoughts/\">Cached Thoughts</a></li></ul><h2>Notable Posts</h2><ul><li><a href=\"http://lesswrong.com/lw/k8/how_to_seem_and_be_deep/\">How to Seem (and Be) Deep</a> — Just find ways of violating cached expectations.</li><li><a href=\"http://lesswrong.com/lw/ic/the_virtue_of_narrowness/\">The Virtue of Narrowness</a> and <a href=\"http://lesswrong.com/lw/k7/original_seeing/\">Original Seeing</a> — One way to fight cached patterns of thought is to focus on precise concepts.</li><li><a href=\"http://lesswrong.com/lw/d2/cached_procrastination/\">Cached Procrastination</a></li><li><a href=\"http://lesswrong.com/lw/4e/cached_selves/\">Cached Selves</a></li></ul>",
    "description_length": 2891,
    "viewCount": 227,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "8hPTCJbwJnLBmfpCX",
    "name": "Calibration",
    "slug": "calibration",
    "postCount": 69,
    "description_html": "<p>Someone is <strong>well-calibrated</strong> if the things they predict with X% chance of happening in fact occur X% of the time. Importantly, calibration is <i>not the same as accuracy. </i>Calibration is about accurately assessing how good your predictions are, not making good predictions. Person A, whose predictions are marginally better than chance (60% of them come true when choosing from two options) and who is precisely 60% confident in their choices, is perfectly calibrated. In contrast, Person B, who is 99% confident in their predictions, and right 90% of the time, is more <i>accurate</i> than Person A, but less <i>well-calibrated</i>.</p><p><i>See also: </i><a href=\"https://www.lesswrong.com/tag/betting?showPostCount=true&amp;useTagName=true\"><i>Betting</i></a><i>, </i><a href=\"https://www.lesswrong.com/tag/epistemic-modesty?showPostCount=true&amp;useTagName=true\"><i>Epistemic Modesty</i></a><i>, </i><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\"><i>Forecasting &amp; Prediction</i></a></p><p>Being well-calibrated has value for rationalists separately from accuracy. Among other things, being well-calibrated lets you make good <a href=\"https://www.lesswrong.com/tag/betting\">bets </a>/ <a href=\"https://www.lesswrong.com/tag/planning-and-decision-making\">make good decisions</a>, communicate information helpfully to others if they know you to be well-calibrated (See <a href=\"https://www.lesswrong.com/tag/group-rationality\">Group Rationality</a>), and helps prioritize <a href=\"https://www.lesswrong.com/tag/value-of-information\">which information is worth acquiring</a>.</p><p>Note that all expressions of quantified confidence in <a href=\"https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=false&amp;useTagName=false\">beliefs</a> can be well- or poorly- calibrated. For example, calibration applies to whether a person's 95% confidence intervals captures the true outcome 95% of the time.</p><p><strong>List of Calibration Exercises</strong></p><p><i>(From: </i><a href=\"https://www.lesswrong.com/posts/LdFbx9oqtKAAwtKF3/list-of-probability-calibration-exercises)\">https://www.lesswrong.com/posts/LdFbx9oqtKAAwtKF3/list-of-probability-calibration-exercises)</a></p><ul><li><a href=\"https://bayes-up.web.app/\">https://bayes-up.web.app/</a></li><li><a href=\"http://acritch.com/media/mphd/calibration-exercises.pdf\">http://acritch.com/media/mphd/calibration-exercises.pdf</a></li><li><a href=\"http://acritch.com/credence-game/\">http://acritch.com/credence-game/</a></li><li><a href=\"http://confidence.success-equation.com/\">http://confidence.success-equation.com/</a></li><li><a href=\"https://calibration-practice.neocities.org/\">https://calibration-practice.neocities.org/</a></li><li>All different URLs for the same application:<ul><li><a href=\"https://programs.clearerthinking.org/calibrate_your_judgment.html\">https://programs.clearerthinking.org/calibrate_your_judgment.html</a></li><li><a href=\"https://www.openphilanthropy.org/calibration\">https://www.openphilanthropy.org/calibration</a></li><li><a href=\"https://80000hours.org/calibration-training/\">https://80000hours.org/calibration-training/</a></li></ul></li><li><a href=\"https://play.google.com/store/apps/details?id=com.the_calibration_game\">https://play.google.com/store/apps/details?id=com.the_calibration_game</a></li><li><a href=\"https://www.metaculus.com/tutorials/\">https://www.metaculus.com/tutorials/</a><ul><li><a href=\"https://www.metaculus.com/help/prediction-resources/\">https://www.metaculus.com/help/prediction-resources/</a>&nbsp;</li></ul></li><li><a href=\"https://outsidetheasylum.blog/probability-calibration/\">https://outsidetheasylum.blog/probability-calibration/</a></li><li><a href=\"https://peterattiamd.com/confidence/\">https://peterattiamd.com/confidence/</a></li><li><a href=\"http://quantifiedintuitions.org/calibration\">http://quantifiedintuitions.org/calibration</a></li></ul><p>Exercises that are dead/unmaintained:</p><ul><li><a href=\"http://web.archive.org/web/20100529074053/http://www.acceleratingfuture.com/tom/?p=129\">http://web.archive.org/web/20100529074053/http://www.acceleratingfuture.com/tom/?p=129</a></li><li><a href=\"http://credencecalibration.com/\">http://credencecalibration.com/</a> (dead link)</li><li><a href=\"https://calibration.lazdini.lv/\">https://calibration.lazdini.lv/</a> (dead link)</li><li><a href=\"http://web.archive.org/web/20161020032514/http://calibratedprobabilityassessment.org/\">http://web.archive.org/web/20161020032514/http://calibratedprobabilityassessment.org/</a></li><li><a href=\"https://predictionbook.com/credence_games/try\">https://predictionbook.com/credence_games/try</a><ul><li><a href=\"https://github.com/bellroy/predictionbook/issues/262\">Read-only</a>; recommends <a href=\"https://fatebook.io/\">https://fatebook.io/</a>&nbsp;</li></ul></li><li><a href=\"https://calibration-training.netlify.app/\">https://calibration-training.netlify.app/</a> (dead link)</li><li><a href=\"http://www.2pih.com/caltest/\">http://www.2pih.com/caltest/</a> (broken)</li></ul>",
    "description_length": 5062,
    "viewCount": 141,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "XYHzLjwYiqpeqaf4c",
    "name": "Dark Arts",
    "slug": "dark-arts",
    "postCount": 61,
    "description_html": "<p><strong>Dark Arts </strong>is a colloquial term for techniques or methods which involve deception and/or manipulation of others or oneself into believing things for non-truth-seeking reasons. These techniques may prey on human cognitive biases.</p><p>Some use the term to refer more narrowly to techniques that work equally well to compel both true and false beliefs, i.e., they are <a href=\"https://www.lesswrong.com/posts/qajfiXo5qRThZQG7s/guided-by-the-beauty-of-our-weapons\">symmetric weapons</a>. Some focus more on the Dark Arts as applied to oneself (self-deception) vs applied to manipulating others.</p><p>An example from the <a href=\"https://www.lesswrong.com/posts/4DBBQkEQvNEWafkek/dark-arts-of-rationality\">Dark Arts of Rationality</a>:</p><blockquote><p>Today, we're going to talk about Dark rationalist techniques: productivity tools which seem incoherent, mad, and downright irrational. These techniques include:</p><ol><li>Willful Inconsistency</li><li>Intentional Compartmentalization</li><li>Modifying Terminal Goals</li></ol></blockquote><h2>Art vs. Technology</h2><p>Sometimes these arts are further augmented by the use of <strong>persuasion technology</strong>, such as broadcast advertising or PowerPoint slides. Persuasion technology may prevent the person who is being targeted from carefully deliberating on the intended message, or thinking up an effective response to it in real time.</p><p>Such effects can be caused by something as benign as the use of a specialist vocabulary which the target is unfamiliar with, or an institutional vocabulary with high-status connotations: this is one reason why many specialist professions employ ethical codes to regulate their unbalanced power relationship with customers.</p><p>The use of such techniques as whiteboards or PowerPoint slides brings additional concerns, since these tend to connote a single party as the one \"in charge\" of the presentation: this makes it even more difficult for the intended audience to raise any effective objection, and encourages them to focus their attention on the content of the whiteboard or slides. Said content is often presented as a list of abrupt \"bullet points\", further connoting it as factual, objective and neutral. One outspoken critic of PowerPoint, management professor David R. Beatty, states: \"It is like a disease. It's the AIDS of management.\" Beatty further states that Powerpoint \"removes subtlety and thinking\".</p><p>Many futurists expect that a technological singularity of even a very mild character will lead to an explosion in the use of radically effective persuasive technology, or \"cognotechnology\"--a term coined by American military researchers at the Lawrence Livermore Laboratories. The collection and distribution of information about people may spiral out beyond any feasible control, perhaps even comprising their inner thought processes; cognitive monitoring may range from non-intrusive body monitoring as seen in a <a href=\"https://wiki.lesswrong.com/wiki/polygraph\">polygraph</a> to outright <a href=\"https://wiki.lesswrong.com/wiki/brain_emulation\">brain emulation</a>. In this scenario, persuasion technology may easily blend over into outright mind control. This is clearly a rather paranoiac and dystopian scenario; nevertheless, the fact that it is being seriously discussed has persuasive potential in itself, such as for directing funding for research into guaranteed <a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a>, as opposed to naïvely pursuing expanded funding for neuroscience or artificial intelligence.</p><h2>Notable Posts</h2><ul><li><a href=\"http://robinhanson.typepad.com/overcomingbias/2009/02/against-propaganda-.html\">Against Propaganda</a> by <a href=\"https://lessestwrong.com/tag/robin-hanson\">Robin Hanson</a></li><li><a href=\"http://www.overcomingbias.com/2009/03/deceptive-writing-styles.html\">Lying With Style</a> by <a href=\"https://lessestwrong.com/tag/robin-hanson\">Robin Hanson</a></li><li><a href=\"https://lessestwrong.com/lw/62/defense_against_the_dark_arts_case_study_1/\">Defense Against The Dark Arts: Case Study #1</a> by <a href=\"https://wiki.lesswrong.com/wiki/Yvain\">Yvain</a></li><li><a href=\"https://lessestwrong.com/lw/yg/informers_and_persuaders/\">Informers and Persuaders</a> by <a href=\"https://lessestwrong.com/tag/eliezer-yudkowsky\">Eliezer_Yudkowsky</a></li><li><a href=\"https://lessestwrong.com/lw/2v2/the_dark_arts_preamble\">The Dark Arts - Preamble</a> by <a href=\"http://www.staresattheworld.com/\">Aurini</a></li><li><a href=\"https://lessestwrong.com/lw/9iw/the_dark_arts_a_beginners_guide/\">The Dark Arts: A Beginner's Guide</a> by <a href=\"http://lesswrong.com/user/faul_sname/overview/\">faul_sname</a></li></ul><h2>Other Links</h2><ul><li><a href=\"https://lessestwrong.com/lw/b1/persuasiveness_vs_soundness/789\">Meta-commentary on this terminology</a></li></ul><h2>See Also</h2><ul><li><a href=\"https://lessestwrong.com/tag/anti-epistemology\">Anti-epistemology</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Curiosity_stopper\">Curiosity stopper</a></li><li><a href=\"https://lessestwrong.com/tag/mind-killer\">Mind-killer</a></li><li><a href=\"https://lessestwrong.com/tag/not-technically-a-lie\">Not technically a lie</a></li><li><a href=\"https://lessestwrong.com/tag/inferential-distance\">Inferential distance</a></li></ul>",
    "description_length": 5355,
    "viewCount": 1223,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "32DdRimdM7sB5wmKu",
    "name": "Empiricism",
    "slug": "empiricism",
    "postCount": 43,
    "description_html": "<blockquote><p>\"The sixth virtue is <strong>empiricism</strong>. The roots of knowledge are in observation and its fruit is prediction. What tree grows without roots? What tree nourishes us without fruit? If a tree falls in a forest and no one hears it, does it make a sound? One says, “Yes it does, for it makes vibrations in the air.” Another says, “No it does not, for there is no auditory processing in any brain.” Though they argue, one saying “Yes”, and one saying “No”, the two do not anticipate any different experience of the forest. Do not ask which beliefs to profess, but which experiences to anticipate. Always know which difference of experience you argue about. Do not let the argument wander and become about something else, such as someone’s virtue as a rationalist. Jerry Cleaver said: “What does you in is not failure to apply some high-level, intricate, complicated technique. It’s overlooking the basics. Not keeping your eye on the ball.” Do not be blinded by words. When words are subtracted, anticipation remains.\" - <a href=\"https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality\">(Twelve Virtues of Rationality)</a></p></blockquote>",
    "description_length": 1183,
    "viewCount": 77,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "8daMDi9NEShyLqxth",
    "name": "Forecasting & Prediction",
    "slug": "forecasting-and-prediction",
    "postCount": 427,
    "description_html": "<p><strong>Forecasting&nbsp;</strong>or&nbsp;<strong>Predicting</strong> is the act of making statements about what will happen in the future (and in some cases, the past) and then scoring the predictions. Posts marked with this tag are for discussion of the practice, skill, and methodology of forecasting. Posts exclusively containing object-level lists of forecasts and predictions are in&nbsp;<a href=\"https://www.lesswrong.com/tag/forecasts\"><u>Forecasts</u></a>.</p><blockquote><p><i>Above all, don’t ask what to believe—ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the center of the inquiry.</i></p><p>—<a href=\"https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences\"><u>Making Beliefs Pay Rent</u></a></p></blockquote><p>Forecasting allows individuals and institutions to test their internal models of reality. &nbsp;A forecaster with a good track record can have more confidence in future predictions and hence actions in the same area as they have a good track record in. Organisations with decision-makers with good track records can likewise be more confident in their choices.</p><p>Crucially, forecasting is a tool to test decision making, rather than a tool for good decision making. If your decision makers are found to be poor forecasters, that is a bad sign, but if your decision making process doesn't involve forecasting, it's not a bad sign. It's not clear that it should.</p><h1>Where to start</h1><p>Some common recommendations for getting into forecasting are as follows:</p><ul><li>Track your personal forecasts. Get a notebook, spreadsheet or <a href=\"https://fatebook.io\">fatebook.io</a> and write what you think will happen and a % or odds chance. Follow up on it later.</li><li>Bet fake money on <a href=\"https://manifold.markets/\">manifold.markets</a>. It's still pretty addictive, so if you have a gambling problem, please avoid.</li><li>Take part in the monthly <a href=\"https://www.quantifiedintuitions.org/estimation-game\">estimation game</a>. Test your ability to estimate quantities. This is correlated with your ability to navigate the world well</li><li>Forecast on <a href=\"https://metaculus.com\">metaculus.com</a>. Questions are often pretty focused on geopolitics</li><li>Read <a href=\"https://www.amazon.co.uk/Superforecasting-Science-Prediction-Philip-Tetlock/dp/1847947158\">Superforecasting</a> by Philip Tetlock. This is if books are a way you learn well.&nbsp;</li></ul><h1>Forecasting Techniques</h1><p>Forecasting is hard but many top forecasters use common techniques. This suggests that forecasting is a skill that can be learnt and practised.</p><h2>Base rates</h2><p><a href=\"https://en.wikipedia.org/wiki/Reference_class_forecasting\"><i><u>Reference Class Forecasting on Wikipedia</u></i></a></p><p>Suppose we are trying to find the probability that an event will occur within the next 5 years. One good place to start is by asking \"of all similar time periods, what fraction of the time does this event occur?\". This is the base rate.</p><p>If we want to know the probability that Joe Biden is President of the United States on Nov. 1st, 2024, we could ask</p><ul><li>What fraction of presidential terms are fully completed (last all 4 years)? The answer to this is 49 out of the 58 total terms, or around <strong>84%</strong>.</li><li>On the other hand, we know that Biden has already made it through 288 days of his term. If we remove the 5 presidents who left office before that, there are 49 out of 53 or around <strong>92%</strong>.</li><li>But alternately, Joe Biden is pretty old (78 to be exact). If we look up <a href=\"https://www.ssa.gov/oact/STATS/table4c6.html\">death rate per year in actuarial tables</a>, it's around 5.1% per year, so this leaves him with a ~15% chance of death or a <strong>85%</strong> chance of surviving his term.</li></ul><p>These are all examples of using base rates. [These examples are taken from&nbsp;<a href=\"https://www.lesswrong.com/posts/ahWnHGZCWqzTnXs4i/base-rates-and-reference-classes\"><u>Base Rates and Reference Classes</u></a> by jsteinhardt.]</p><p>Base rates represent the outside view for a given question. They are a good place to start but can often be improved on by updating the probability according to an inside view.</p><p>Note that there are often several reference classes we could use, each implying a different base rate. The problem of deciding which class to use is known as the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Reference_class_problem\"><u>reference class problem</u></a>.</p><h2>Calibration training</h2><p>A forecaster is said to be calibrated if the events they say have a X% chance of happening, happen X% of the time.</p><p>Most people are overconfident. When they say an event has a 99% chance of happening, often the events happen much less frequently than that.</p><p>This natural overconfidence can be corrected with calibration training. In calibration training, you are asked to answer a set of factual questions, assigning a probability to each of your answers.</p><p>A calibration exercise can be found here: <a href=\"https://www.quantifiedintuitions.org/calibration\">https://www.quantifiedintuitions.org/calibration</a>&nbsp;</p><h2>Question decomposition</h2><p>Much like Fermi estimation, questions about future events can often be decomposed into many different questions, these questions can be answered, and the answers to these questions can be used to reconstruct an answer to the original question.</p><p>Suppose you are interested in whether AI will cause a catastrophe by 2100. For AI to cause such an event, several things need to be true: (1) it needs to be possible to build advanced AI with agentic planning and strategic awareness by 2100, (2) there need to be strong incentives to apply such a system, (3) it needs to be difficult to align such a system should it be deployed, (4) a deployed and unaligned AI would act in unintended and high-impact power seeking ways causing trillions of dollars in damage, (5) of these consequences will result in the permanent disempowerment of all humanity and (6) this disempowerment will constitute an existential catastrophe. Taking the probabilities that Eli Lifland assigned to each question gives a 80%, 85%, 75%, 90%, 80% and 95% chance of events 1 through 6 respectively. Since each event is conditional on the ones before it, we can find the probability of the original question by multiplying all the probabilities together. This gives Eli Lifland a probability of existential risk from misaligned AI before 2100 to be approximately 35%. For more detail see Eli's original post <a href=\"https://www.foxy-scout.com/wwotf-review/\">here</a>.</p><p>Decomposing questions into their constituent parts, assigning probabilities to these sub-questions, and combining these probabilities to answer the original questions is believed to improve forecasts. This is because, while each forecast is noisy, combining the estimates from many questions cancels the noise and leaves us with the signal.</p><p>Question decomposition is also good at increasing epistemic legibility. It helps forecasters to communicate to others why they've made the forecast that they did and it allows them to identify their specific points of disagreement.</p><h2>Premortems</h2><p><a href=\"https://en.wikipedia.org/wiki/Pre-mortem\"><u>Premortems on Wikipedia</u></a></p><p>A premortem is a strategy used once you've assigned a probability to an event. You ask yourself to imagine that the forecast was wrong and you then work backwards to determine what could potentially have caused this.</p><p>It is simply a way to reframe the question \"in what ways might I be wrong?\" but in a way that reduces motivated reasoning caused by attachment to the bottom line.&nbsp;</p><h2>Practice</h2><p><a href=\"https://forecasting.wiki/wiki/How_to_Start_Forecasting\"><i><u>Getting Started on the Forecasting Wiki</u></i></a></p><p>While the above techniques are useful, they are no substitute for actually making predictions. Get out there and make predictions! Use the above techniques. Keep track of your predictions. Periodically evaluate questions that have been resolved and review your performance. Assess the degree to which you are calibrated. Look out for systematic mistakes that you might be making. Make more predictions! Over time, like with any skill, your ability can and should improve.</p><h2>Other Resources</h2><p>Other resources include:</p><ul><li>Superforcasting by Philip Tetlock and Dan Gardener</li><li>Intro to Forecasting by Alex Lawson</li><li><a href=\"https://forecasting.substack.com/\"><u>Forecasting Newsletter</u></a> by Nuño Sempere</li></ul><h2>Forecasting Research</h2><p>Forecasting beyond 3 years is not good. Anything above .25 is worse than random. Many questions are too specific and too far away for forecasting to be useful to them (<a href=\"https://forum.effectivealtruism.org/posts/hqkyaHLQhzuREcXSX/data-on-forecasting-accuracy-across-different-time-horizons \">Dillon 2020</a>).<img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/lpmcuvclu9akc6ytdjvb\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/nkitkelpgq1otpiure0i 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/kc2luqtf0ugy8h6li17k 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/qxguotvyvchc3wrjx87f 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/ogxcnvsd9jtza50txxtz 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/uejg0ctlm5vpyq0kgxvx 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/hmze1uwmirvsoyxgq3fo 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/bznkoxebe3nzzzmrtglz 980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/ijpmqbdjflcx7nl9ztek 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/ifjc9bcavcswykrhhfjv 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/pmbpmgv6i12aedfmnzbd 1314w\"></p><h3>Difficulties in Applying Forecasting</h3><p>Decision makers largely don't trust forecasts. Even if you had the perfect set of 1000 forecast that gave policy recommendations (which is usually not the case), decision makers would need to want to act on them. That they don't is a significant bottleneck to successfully use forecasting.</p><p>It is difficult to forecast things policy makers actually care about. Forecasting sites forecast things like \"will Putin leave power\" rather than \"If Putin leaves power between July18th and the end of Aug how will that affect the likelihood of a rogue nuclear warhead\". &nbsp;This question probably still isn't specific enough to be useful - it doesn't forecast specific policy outcomes. And if it <i>were, </i>decision makers would have to trust the results, which they currently largely don't. This is related to the problem of specifying good forecasting questions, especially for nebulous domains.</p><h1>State of the Art</h1><p>For many years there have been calls to apply forecasting techniques to non-academic domains including journalism, policy, investing and business strategy. Several organisations now exist within these niche.</p><h2>Metaculus</h2><p><a href=\"https://www.metaculus.com/\"><u>Metaculus</u></a> is a popular and established web platform for forecasting. Their questions mainly focus on geopolitics, the coronavirus pandemic and topics of interest to Effective Altruism.</p><p>They host prediction competitions with real money prizes and collect and track public predictions made by various figures.</p><h2>Cultivate Labs</h2><p><a href=\"https://www.cultivatelabs.com/\"><u>Cultivate Labs</u></a> build tools that companies can use to crowdsource information from among their employees. This helps leadership to understand the consensus of people working on the ground and use this to improve the decisions they make.</p><h2>Kalshi</h2><p><a href=\"https://kalshi.com/\"><u>Kalshi</u></a> provide real money prediction markets on geopolitical events. The financial options they provide are intended to be used as hedges for political risk.</p><h2>Manifold.Markets</h2><p><a href=\"https://manifold.markets/\"><u>Manifold.Markets</u></a> is a prediction market platform that uses play money. It is noteworthy for its ease of use, great UI and the fact that the market creator decides how the market resolves.</p><h2>QURI</h2><p><a href=\"https://quantifieduncertainty.org/\"><u>QURI</u></a> is a research organisation that builds tools that make it easier to make good forecasts. Their most notable tool is Squiggle - a programming language designed to be used to make legible forecasts in a wide range of contexts.</p><h1>Forecasters on twitter</h1><ul><li><strong>Top forecasters on </strong><a href=\"https://www.lesswrong.com/users/metaculus?mention=user\"><strong>@Metaculus</strong></a><strong>:</strong><ul><li><a href=\"https://twitter.com/SmoLurks\">@SmoLurks</a> (ranked #1 in 2021)</li><li><a href=\"https://twitter.com/Jotto999\">@Jotto999</a> (#11 in 2019)&nbsp;</li><li><a href=\"https://twitter.com/ryanbeck111\">@ryanbeck111</a> (#12 in 2021)</li><li><a href=\"https://twitter.com/SchoeneggerPhil\">@SchoeneggerPhil</a> (#15 in 2023)</li><li><a href=\"https://twitter.com/lxrjl\">@lxrjl</a> (#15 in 2021)</li><li><a href=\"https://twitter.com/JgaltTweets\">@JgaltTweets</a> (#16 in 2023)</li><li><a href=\"https://twitter.com/peterwildeford\">@peterwildeford</a> (#19 in 2023)</li><li><a href=\"https://twitter.com/nextbigfuture\">@nextbigfuture</a> (#25 in 2023)</li><li><a href=\"https://twitter.com/draaglom\">@draaglom</a> (#26 in 2023)</li><li><a href=\"https://twitter.com/EgeErdil2\">@EgeErdil2</a> (#27 in 2021)&nbsp;</li><li><a href=\"https://twitter.com/LinchZhang\">@LinchZhang</a> (#29 in 2020)&nbsp;</li><li><a href=\"https://twitter.com/beala\">@beala</a> (#57 in 2021)&nbsp;</li><li><a href=\"https://twitter.com/NathanpmYoung\">@NathanpmYoung</a> (#72 in 2023)</li><li><a href=\"https://twitter.com/MatthewJBar\">@MatthewJBar</a> (#79 in 2021)&nbsp;</li></ul></li><li>&nbsp;<strong>Top forecasters on </strong><a href=\"https://twitter.com/INFERpub\">@INFERpub</a><strong>:</strong><ul><li><a href=\"https://twitter.com/celloMolly\">@celloMolly</a> (#6 in 2022)</li><li><a href=\"https://twitter.com/NunoSempere\">@NunoSempere</a> (#7 in 2022)&nbsp;</li><li><a href=\"https://twitter.com/cmeinel\">@cmeinel</a> (#12 in 2023) &nbsp;</li></ul></li><li><strong>Top forecasters on </strong><a href=\"https://twitter.com/GJ_Open\">@GJ_Open:</a> &nbsp;<ul><li><a href=\"https://twitter.com/leonardbarrett\">@leonardbarrett</a> (#1 in News 2022 tournament)</li><li><a href=\"https://twitter.com/juan_cambeiro\">@juan_cambeiro</a> (#3 in Coronavirus 2022)</li></ul></li><li><strong>Top bettors on</strong> <a href=\"https://twitter.com/Polymarket\">@Polymarket</a><strong>:</strong><ul><li><a href=\"https://twitter.com/Domahhhh\">@Domahhhh</a></li></ul></li><li><strong>Top bettors on</strong> <a href=\"https://twitter.com/ManifoldMarkets\">@ManifoldMarkets</a><strong>:&nbsp;</strong><ul><li><a href=\"https://twitter.com/firstuserhere\">@firstuserhere</a> ($1m play money lifetime profit)</li><li><a href=\"https://twitter.com/ReplyHobbes\">@ReplyHobbes</a> ($600k)</li><li><a href=\"https://twitter.com/Chrisbilbo\">@Chrisbilbo</a> ($450k) <strong>Top forecasting teams</strong></li></ul></li><li><a href=\"https://twitter.com/swift_centre\">@swift_centre</a> <strong>&amp; </strong><a href=\"https://twitter.com/SamotsvetyF\">@SamotsvetyF</a><strong>:</strong><ul><li><a href=\"https://twitter.com/MWStory\">@MWStory</a></li><li><a href=\"https://twitter.com/TolgaBilge_\">@TolgaBilge_</a></li></ul></li><li><strong>Official</strong> <a href=\"https://twitter.com/superforecaster\">@superforecaster</a><strong>:&nbsp;</strong><ul><li><a href=\"https://www.lesswrong.com/users/sam_atis?mention=user\">@sam_atis</a>&nbsp;</li><li><a href=\"https://twitter.com/davidmanheim\">@davidmanheim</a></li><li><a href=\"https://twitter.com/BalkanDevlen\">@BalkanDevlen</a></li><li><a href=\"https://twitter.com/KitsonJ1\">@KitsonJ1</a></li><li><a href=\"https://twitter.com/thatMikeBishop\">@thatMikeBishop</a></li><li><a href=\"https://twitter.com/EmilDimanchev\">@EmilDimanchev</a></li><li><a href=\"https://twitter.com/foxyforecaster\">@foxyforecaster</a></li><li><a href=\"https://twitter.com/rdeneufville\">@rdeneufville</a></li><li><a href=\"https://twitter.com/Allmyalibis\">@Allmyalibis</a></li><li><a href=\"https://twitter.com/brettabroad\">@brettabroad</a></li><li><a href=\"https://twitter.com/JoeGillis23\">@JoeGillis23</a></li><li><a href=\"https://twitter.com/HartmanMath\">@HartmanMath</a></li><li><a href=\"https://twitter.com/scholarandcat\">@scholarandcat</a></li><li><a href=\"https://twitter.com/dkcoutant\">@dkcoutant</a></li><li><a href=\"https://twitter.com/malcmur\">@malcmur</a></li></ul></li></ul><h1>See also</h1><ul><li><a href=\"https://www.lesswrong.com/tag/antiprediction\"><u>Antiprediction</u></a></li><li><a href=\"https://www.lesswrong.com/tag/making-beliefs-pay-rent\"><u>Making beliefs pay rent</u></a></li><li><a href=\"https://www.lesswrong.com/tag/prediction-markets\"><u>Prediction market</u></a></li><li><a href=\"https://www.lesswrong.com/tag/calibration\"><u>Calibration</u></a></li><li><a href=\"https://www.lesswrong.com/tag/black-swans\"><u>Black swan</u></a></li><li><a href=\"https://www.lesswrong.com/tag/betting\"><u>Betting</u></a></li></ul><p><br>&nbsp;</p>",
    "description_length": 17971,
    "viewCount": 391,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "zv7v2ziqexSn5iS9v",
    "name": "Group Rationality",
    "slug": "group-rationality",
    "postCount": 99,
    "description_html": "<p>In almost anything, individuals are inferior to groups. Several articles address this concern regarding rationality, i.e., the topic of <strong>Group Rationality</strong>.</p><h2>External links</h2><ul><li><a href=\"https://medium.com/@ThingMaker/open-problems-in-group-rationality-5636440a2cd1\">Open Problems in Group Rationality</a> is one of the main articles about the subject.</li><li><a href=\"http://ratio.huji.ac.il/dp/dp154.pdf\">Individual and Group Behavior in the Ultimatum Game</a></li><li><a href=\"http://www.andrew.cmu.edu/user/kzollman/research/Presentations/LRR%20-%20IndividualVsSocial.pdf\">Individual vs. Group Rationality in Inquiry</a></li></ul><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/rationality-as-martial-art\">Rationality as martial art</a></li><li><a href=\"https://www.lesswrong.com/tag/problem-of-verifying-rationality\">Problem of verifying rationality</a></li><li><a href=\"/community\">Less Wrong meetup groups</a></li><li><a href=\"https://www.lesswrong.com/tag/the-craft-and-the-community\">The Craft and the Community</a></li></ul>",
    "description_length": 1082,
    "viewCount": 103,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "x6evH6MyPK3nxsoff",
    "name": "Identity",
    "slug": "identity",
    "postCount": 80,
    "description_html": "<p><strong>Identity </strong>is an individual's conception of themselves (1). We might conceive of this as the set of <i>I am ___ </i>statements an individual would make about themselves. It seems correct that <i>identity </i>can be dangerous for epistemics since the desire to maintain one's identity can interfere with updating correctly or changing actions [<a href=\"https://www.lesswrong.com/posts/BXQsZmubkovJ76Ldo/the-actionable-version-of-keep-your-identity-small\">1</a>], but at the same time there are potentially useful and safe ways to maintain an identity which even enhances one's rationality [<a href=\"https://www.lesswrong.com/posts/uR8c2NPp4bWHQ5u45/strategic-choice-of-identity\">1</a>, <a href=\"https://www.lesswrong.com/posts/Zupr296Zy74wpihXT/use-your-identity-carefully\">2</a>].<br><br><br><i>(1) We might consider cases where an external party imposes an identity on someone, but that case has not been the topic of most discussion of Identity on LessWrong.</i></p><p><strong>External resources:</strong> <a href=\"http://www.paulgraham.com/identity.html#f2n\">Keep Your Identity Small</a> by Paul Graham</p><p><strong>Related Pages: </strong><a href=\"https://www.lesswrong.com/tag/personal-identity\">Personal Identity</a>, <a href=\"https://www.lesswrong.com/tag/self-improvement\">Self Improvement</a></p>",
    "description_length": 1324,
    "viewCount": 178,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "rWzGNdjuep56W5u2d",
    "name": "Inside/Outside View",
    "slug": "inside-outside-view",
    "postCount": 58,
    "description_html": "<p>An <strong>Inside View </strong>on a topic involves making predictions based on your understanding of the details of the process. An <strong>Outside View </strong>involves ignoring these details and using an estimate based on a class of roughly similar previous cases (alternatively, this is called <a href=\"http://en.wikipedia.org/wiki/Reference_class_forecasting\">reference class forecasting</a>), though it has been <a href=\"https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view\">pointed out</a> that the possible meaning has expanded beyond that.</p><p>For example, someone working on a project may estimate that they can reasonably get 20% of it done per day, so they will get it done in five days (inside view). Or they might consider that all of their previous projects were completed just before the deadline, so since the deadline for this project is in 30 days, that's when it will get done (outside view).</p><p>The terms were originally developed by Daniel Kahneman and Amos Tversky. An early use is in <a href=\"http://doi.org/10.1287/mnsc.39.1.17\">Timid Choices and Bold Forecasts: A Cognitive Perspective on Risk Taking (Kahneman &amp; Lovallo, 1993)</a> and the terms were popularised in <i>Thinking, Fast and Slow</i> (Kahneman, 2011; <a href=\"https://www.mckinsey.com/business-functions/strategy-and-corporate-finance/our-insights/daniel-kahneman-beware-the-inside-view\">relevant excerpt</a>). The planning example is discussed in <a href=\"https://www.lesswrong.com/posts/CPm5LTwHrvBJCa9h5/planning-fallacy\">The Planning Fallacy</a>.&nbsp;</p><h3>Examples of outside view</h3><p><strong>1.</strong> From <a href=\"https://www.overcomingbias.com/2007/07/beware-the-insi.html\">Beware the Inside View</a>, by Robin Hanson:</p><blockquote><p>I did 1500 piece jigsaw puzzle of fireworks, my first jigsaw in at least ten years.&nbsp; Several times I had the strong impression that I had carefully eliminated every possible place a piece could go, or every possible piece that could go in a place.&nbsp; I was very tempted to conclude that many pieces were missing, or that the box had extra pieces from another puzzle.&nbsp; This wasn’t impossible – the puzzle was an open box a relative had done before.&nbsp; And the alternative seemed humiliating.&nbsp;</p></blockquote><blockquote><p>But I allowed a very different part of my mind, using different considerations, to overrule this judgment; so many extra or missing pieces seemed unlikely.&nbsp; And in the end there was only one missing and no extra pieces.&nbsp; I recall a similar experience when I was learning to program. I would carefully check my program and find no errors, and then when my program wouldn’t run I was tempted to suspect compiler or hardware errors.&nbsp; Of course the problem was almost always my fault.&nbsp; &nbsp;</p></blockquote><p><strong>2.</strong> Japanese students expected to finish their essays an average of 10 days before deadline. The average completion time was actually 1 day before deadline. When asked when they'd completed similar, previous tasks, the average reply was 1 day before deadline[1].</p><p><strong>3.</strong> Students instructed to visualize how, where, and when they would perform their Christmas shopping, expected to finish shopping more than a week before Christmas. A control group asked when they expected their Christmas shopping to be finished, expected it to be done 4 days before Christmas. Both groups finished 3 days before Christmas[2].</p><h3>Problems with the outside view</h3><p>It is controversial how far the lesson of these experiments can be extended. Robin Hanson argues that this implies that, in futurism, forecasts should be made by trying to find a reference class of similar cases, rather than by trying to visualize outcomes. Eliezer Yudkowsky responds that this leads to \"reference class tennis\" wherein people feel that the same event 'obviously' belongs to two different reference classes, and that the above experiments were performed in cases where the new example was highly similar to past examples. I.e., this year's Christmas shopping optimism and last year's Christmas shopping optimism are much more similar to one another, than the invention of the Internet is to the invention of agriculture. If someone else then feels that the invention of the Internet is more like the category 'recent communications innovations' and should be forecast by reference to television instead of agriculture, both sides pleading the outside view has no resolution except \"I'm taking my reference class and going home!\"</p><p>More possible limitations and problems with using the outside view are discussed in <a href=\"https://www.lesswrong.com/posts/pqoxE3AGMbse68dvb/the-outside-view-s-domain\">The Outside View's Domain</a> and <a href=\"https://www.lesswrong.com/posts/FsfnDfADftGDYeG4c/outside-view-as-conversation-halter\">\"Outside View\" as Conversation-Halter</a>. <a href=\"https://www.lesswrong.com/posts/iyRpsScBa6y4rduEt/model-combination-and-adjustment\">Model Combination and Adjustment</a> discusses the implications of there usually existing multiple <i>different</i> outside views. <a href=\"https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view\">Taboo \"Outside View\"</a> argues that the meaning of \"Outside View\" have expanded too much, and that it should be <a href=\"https://www.lesswrong.com/tag/rationalist-taboo\">tabooed</a> and replaced with more precise terminology. An alternative to \"inside/outside view\" has been proposed in <a href=\"https://www.lesswrong.com/s/uLEjM2ij5y3CXXW6c/p/vKbAWFZRDBhyD6K6A\">Gears Level &amp; Policy Level</a>.</p><h2>External Posts</h2><ul><li><a href=\"http://www.overcomingbias.com/2007/07/beware-the-insi.html\">Beware the Inside View</a> by <a href=\"https://lessestwrong.com/tag/robin-hanson\">Robin Hanson</a></li></ul><h2>See Also</h2><ul><li><a href=\"https://lessestwrong.com/tag/planning-fallacy\">Planning fallacy</a></li><li><a href=\"https://www.lesswrong.com/tag/modest-epistemology\">Modest Epistemology</a></li><li><a href=\"https://lessestwrong.com/tag/near-far-thinking\">Near/far thinking</a></li><li><a href=\"https://lessestwrong.com/tag/connotation\">Connotation</a>, <a href=\"https://lessestwrong.com/tag/absurdity-heuristic\">Absurdity heuristic</a></li><li><a href=\"https://lessestwrong.com/tag/arguing-by-analogy\">Arguing by analogy</a></li><li><a href=\"https://lessestwrong.com/tag/intelligence-explosion\">Intelligence explosion</a>, <a href=\"https://lessestwrong.com/tag/the-hanson-yudkowsky-ai-foom-debate\">The Hanson-Yudkowsky AI-Foom Debate</a></li></ul><p>[1] Buehler, R., Griffin, D., &amp; Ross, M. 2002. Inside the planning fallacy: The causes and consequences of optimistic time predictions. Heuristics and biases: The psychology of intuitive judgment, 250-270. Cambridge, UK: Cambridge University Press.</p><p>[2] Buehler, R., Griffin, D. and Ross, M. 1995. It's about time: Optimistic predictions in work and love. European Review of Social Psychology, Volume 6, eds. W. Stroebe and M. Hewstone. Chichester: John Wiley &amp; Sons.</p>",
    "description_length": 7082,
    "viewCount": 793,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "Zwv9eHi7KGg5KA9oM",
    "name": "Introspection",
    "slug": "introspection",
    "postCount": 73,
    "description_html": null,
    "description_length": null,
    "viewCount": 107,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "z95PGFXtPpwakqkTA",
    "name": "Intuition",
    "slug": "intuition",
    "postCount": 46,
    "description_html": "<p><strong>Intuition</strong> is the ability to acquire knowledge without recourse to conscious reasoning. Also relevant are <strong>Intuition Pumps</strong>, a thought experiment, a model, or anything else, that's structured to allow the thinker to use their intuition to develop an answer to a problem. (From Wikipedia <a href=\"https://en.wikipedia.org/wiki/Intuition\">1</a>, <a href=\"https://en.wikipedia.org/wiki/Intuition_pump\">2</a>) Sometimes intuitions are useful, but sometimes they may be misleading; this itself is a facet of rationality.</p><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/inside-outside-view\">Inside/Outside View</a></li><li><a href=\"https://www.lesswrong.com/tag/fermi-estimation\">Fermi Estimation</a></li><li><a href=\"https://www.lesswrong.com/tag/predictive-processing\">Predictive Processing</a></li></ul>",
    "description_length": 853,
    "viewCount": 157,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "ZpG9rheyAkgCoEQea",
    "name": "Practice & Philosophy of Science",
    "slug": "practice-and-philosophy-of-science",
    "postCount": 242,
    "description_html": "<p><strong>Practice and Philosophy of Science</strong> is for posts that discuss how science is done or should be done; examples include <a href=\"https://www.lesswrong.com/posts/tSemJckYr29Gnxod2/building-intuitions-on-non-empirical-arguments-in-science\">Building Intuitions on Non-Empirical Arguments in Science</a> and the <a href=\"https://www.lesswrong.com/s/fxynfGCSHpY4FmBZy\">Science and Rationality sequence</a>. (It is not for posts that simply report on a new scientific result.)</p>",
    "description_length": 491,
    "viewCount": 212,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "fF9GEdWXKJ3z73TmB",
    "name": "Scholarship & Learning",
    "slug": "scholarship-and-learning",
    "postCount": 334,
    "description_html": "<p><strong>Scholarship &amp; Learning. </strong>Here be posts on how to study, research, and learn.</p><p>Topics include, but are not limited to: how to research, how to understand material deeply, note-taking, and useful scholarship resources.</p><blockquote><p><i>The eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains. – </i><a href=\"https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality\"><i>Twelve Virtues of Rationality</i></a></p></blockquote><h2>See Also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/spaced-repetition\">Spaced Repetition</a> is a technique for long-term retention of learned material.</li><li><a href=\"https://www.lesswrong.com/tag/fact-posts?showPostCount=true&amp;useTagName=true\">Fact Posts</a> are pieces of writing that attempt to build an understanding of the world, starting bottom up with empirical facts rather than \"opinions\".</li><li>The other <a href=\"https://www.lesswrong.com/tag/virtues?showPostCount=true&amp;useTagName=true\">Virtues</a> of Rationality.</li></ul><h2>Top Resources</h2><ul><li><a href=\"https://www.lesswrong.com/posts/37sHjeisS9uJufi4u/scholarship-how-to-do-it-efficiently\">Scholarship: How to Do It Efficiently</a> is a guide to quickly researching topics and understanding what is known within a field.</li><li><a href=\"https://www.lesswrong.com/posts/RKz7pc6snBttndxXz/literature-review-for-academic-outsiders-what-how-and-why-1\">Literature Review For Academic Outsiders: What, How, and Why</a> similar to the first resource, contains many links to further resources.</li><li><a href=\"https://www.lesswrong.com/posts/gxbGKa2AnQsrn3Gni/how-do-you-assess-the-quality-reliability-of-a-scientific\">[Question] How do you assess the quality / reliability of a scientific study?</a> A question post with many highly excellent lengthy responses, several which received bounty payouts.</li><li><a href=\"https://www.lesswrong.com/posts/w5F4w8tNZc6LcBKRP/on-learning-difficult-things\">On learning difficult things</a> covers techniques and methods for studying difficult topics.</li><li><a href=\"https://www.lesswrong.com/posts/TPjbTXntR54XSZ3F2/paper-reading-for-gears\">Paper-Reading for Gears</a> is a guide studying to actually build up a mechanistic, gears-level understanding of a topic.</li><li><a href=\"https://www.lesswrong.com/posts/oPEWyxJjRo4oKHzMu/the-3-books-technique-for-learning-a-new-skilll\">The 3 Books Technique for Learning a New Skilll</a> is a short post suggests finding a What, How, and Why book for any skill or topic you wish to learn.</li><li><a href=\"https://www.lesswrong.com/posts/xg3hXCYQPJkwHyik2/the-best-textbooks-on-every-subject\">The Best Textbooks on Every Subject</a> crowd-sourced list where every recommendation requires that the recommender have read three books on the topic and can explain why one textbook is better than others.</li><li><a href=\"https://www.lesswrong.com/posts/rBkZvbGDQZhEymReM/forum-participation-as-a-research-strategy\">Forum participation as a research strategy</a> argues that participation on discussion forums on a research topic is actually a great way for researchers to make progress.</li><li><a href=\"https://www.lesswrong.com/posts/Sdx6A6yLByRRs8iLY/fact-posts-how-and-why\">Fact Posts: How and Why</a> is guide on exploring empirical question by starting with raw facts rather than expert opinion and prior analysis. Compared more typical research, the Fact Post method helps you ground your understanding in facts and see the topic freshly.</li><li><a href=\"https://www.lesswrong.com/posts/tRQek3Xb9cKZ2o6iA/how-to-not-do-a-literature-review\">How to (not) do a literature review</a> which contains a very concrete list of steps for literature reviews, including mistakes to avoid.</li></ul><p><strong>External Resources</strong></p><ul><li><a href=\"https://www.gwern.net/Search\">Internet Search Tips</a> by Gwern Branwen is a long, extremely detailed practical guide on how to conduct an online search for references, papers, and books that are difficult to find, including 13 case studies.</li></ul>",
    "description_length": 4311,
    "viewCount": 581,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "qoTbWwaJtTSKosRCA",
    "name": "Taking Ideas Seriously",
    "slug": "taking-ideas-seriously",
    "postCount": 25,
    "description_html": null,
    "description_length": null,
    "viewCount": 148,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "DbMQGrxbhLxtNkmca",
    "name": "Value of Information",
    "slug": "value-of-information",
    "postCount": 28,
    "description_html": "<p><strong>Value of Information</strong> (VoI) is a concept from <a href=\"https://www.lesswrong.com/lw/8xr/decision_analysis_sequence/\">decision analysis</a>: how much answering a question allows a decision-maker to improve its decision.</p><p><em>See also: <a href=\"https://www.lesswrong.com/tag/bayes-theorem-bayesianism\">Bayes theorem</a>, <a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory</a></em></p>",
    "description_length": 466,
    "viewCount": 88,
    "parentTagId": "rationality-applied-topics"
  },
  {
    "core-tag": "Rationality",
    "_id": "Kj9q8FXoauL7mQDWt",
    "name": "Affect Heuristic",
    "slug": "affect-heuristic",
    "postCount": 16,
    "description_html": null,
    "description_length": null,
    "viewCount": 130,
    "parentTagId": "rationality-failure-modes"
  },
  {
    "core-tag": "Rationality",
    "_id": "xcBbcAJrvTEkxikW9",
    "name": "Bucket Errors",
    "slug": "bucket-errors",
    "postCount": 15,
    "description_html": "<p>A <strong>Bucket Error </strong>is when multiple different concepts or variables are incorrectly lumped together in one's mind as a single concept/variable, potentially leading to distortions of one's thinking. Bucket Errors are related to <a href=\"https://www.lesswrong.com/posts/y5MxoeacRKKM3KQth/fallacies-of-compression\">Fallacies of Compression</a>.<br><br>The term<i>,</i> <i>Bucket Error</i>, was introduced in&nbsp;<a href=\"https://www.lesswrong.com/posts/EEv9JeuY5xfuDDSgF/flinching-away-from-truth-is-often-about-protecting-the\">\"Flinching away from truth” is often about *protecting* the epistemology</a> where an example is given of a child who refuses to believe that they made a spelling mistake. The child is unwilling to believe this fact because they believe having made a spelling mistake is incompatible with becoming a writer which is their dream. \"Becoming a writer\" and \"never making spelling mistakes\" are lumped in the same bucket despite in fact being separate variables.</p><p>Bucket Errors are similar to the concepts of <a href=\"https://en.wikipedia.org/wiki/Equivocation\">equivocation</a>, identification in Buddhism, or fusion/defusion in modern psychotherapy. See: <a href=\"https://www.lesswrong.com/posts/RQrWd5jPZQtpH8f4v/fusion-and-equivocation-in-korzybski-s-general-semantics\">Fusion and Equivocation in Korzybski's General Semantics</a>.</p><p><strong>Related Pages:</strong> <a href=\"https://www.lesswrong.com/tag/compartmentalization\">Compartmentalization</a>, <a href=\"https://www.lesswrong.com/tag/distinctions\">Distinctions</a></p>",
    "description_length": 1576,
    "viewCount": 116,
    "parentTagId": "rationality-failure-modes"
  },
  {
    "core-tag": "Rationality",
    "_id": "KWFhr6A2dHEb6wmWJ",
    "name": "Compartmentalization",
    "slug": "compartmentalization",
    "postCount": 18,
    "description_html": "<p><strong>Compartmentalization</strong> is keeping information and processes within your mind segregated, especially in ways that keep knowledge possessed by some of your reasoning processes being accessed by other processes.</p><p>From an alternative angle, one can think of compartmentalizing one's different activities or domains from each other. when one couple the skills or habits from one to another, e.g., the religious scientist who does not apply scientific thinking outside the lab.&nbsp;</p><p>One might even have excellent epistemological performance in one domain and terrible performance in others.</p><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/semantic-stopsign\">Semantic stopsign</a>, <a href=\"https://www.lesswrong.com/tag/anti-epistemology\">Anti-epistemology</a></li><li><a href=\"https://www.lesswrong.com/tag/cached-thought\">Cached thought</a></li><li><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\">Shut up and multiply</a>, <a href=\"https://www.lesswrong.com/tag/bite-the-bullet\">Bite the bullet</a>, <a href=\"https://www.lesswrong.com/tag/absurdity-heuristic\">Absurdity heuristic</a></li><li><a href=\"https://www.lesswrong.com/tag/dangerous-knowledge\">Dangerous knowledge</a></li><li><a href=\"https://www.lesswrong.com/tag/general-knowledge\">General knowledge</a>, <a href=\"https://www.lesswrong.com/tag/understanding\">Understanding</a></li><li><a href=\"https://www.lesswrong.com/tag/alief\">Alief</a></li><li><a href=\"https://www.lesswrong.com/tag/aversion-ugh-fields\">Ugh field</a></li><li><a href=\"https://www.lesswrong.com/tag/distinctions\">Distinctions</a></li></ul>",
    "description_length": 1628,
    "viewCount": 96,
    "parentTagId": "rationality-failure-modes"
  },
  {
    "core-tag": "Rationality",
    "_id": "5hpGj9nDLgokfghvR",
    "name": "Confirmation Bias",
    "slug": "confirmation-bias",
    "postCount": 38,
    "description_html": "<p><strong>Confirmation bias</strong> (also known as positive bias) is the tendency to search for, interpret, favor, and recall information in a way that confirms or strengthens one's prior personal beliefs or hypotheses [<a href=\"https://en.wikipedia.org/wiki/Confirmation_bias\">1</a>]. &nbsp;For example, one might test hypotheses with positive rather than negative examples, thus missing obvious disconfirming tests.</p><blockquote>“I had, also, during many years followed a golden rule, namely, that whenever a published fact, a new observation or thought came across me, which was opposed to my general results, to make a memorandum of it without fail and at once; for I had found by experience that such facts and thoughts were far more apt to escape from the memory than favourable ones. Owing to this habit, very few objections were raised against my views which I had not at least noticed and attempted to answer.” - Charles Darwin (autobiography)</blockquote><p><em>See also: </em><a href=\"https://www.lesswrong.com/tag/motivated-skepticism\">Motivated skepticism</a>, <a href=\"https://www.lesswrong.com/tag/privileging-the-hypothesis\">Privileging the hypothesis</a>, <a href=\"https://www.lesswrong.com/tag/falsifiability\">Falsifiability</a>, <a href=\"https://www.lesswrong.com/tag/heuristics-and-biases\">Heuristics and Biases</a>, <a href=\"https://lesswrong.com/tag/availability-heuristic\">Availability heuristic</a>, <a href=\"https://www.lesswrong.com/tag/surprise\">Surprise</a>, <a href=\"https://www.lesswrong.com/tag/narrative-fallacy\">Narrative fallacy</a></p><h2>External Links</h2><ul><li><a href=\"https://www.edge.org/conversation/kevin_kelly-speculations-on-the-future-of-science\">Speculations on the Future of Science </a>by Kevin Kelly</li><li><a href=\"http://psy2.ucsd.edu/~mckenzie/Wason1960QJEP.pdf\">On the Failure to Eliminate Hypotheses in a Conceptual Task</a> by P.C. Wason</li><li><a href=\"https://www.overcomingbias.com/2009/02/write-your-hypothetical-apostasy.html\">Write Your Hypothetical Apostasy</a> by <a href=\"https://www.lesswrong.com/tag/nick-bostrom\">Nick Bostrom</a></li><li><a href=\"https://en.wikipedia.org/wiki/Confirmation_bias\">Confirmation Bias</a>, Wikipedia</li></ul>",
    "description_length": 2214,
    "viewCount": 88,
    "parentTagId": "rationality-failure-modes"
  },
  {
    "core-tag": "Rationality",
    "_id": "PvridmTCj2qsugQCH",
    "name": "Goodhart's Law",
    "slug": "goodhart-s-law",
    "postCount": 115,
    "description_html": "<p><strong>Goodhart's Law</strong> states that when a proxy for some value becomes the target of optimization pressure, the proxy will cease to be a good proxy. One form of Goodhart is demonstrated by the Soviet story of a factory graded on how many shoes they produced (a good proxy for productivity) – they soon began producing a higher number of tiny shoes. Useless, but the numbers look good.</p>\n<p>Goodhart's Law is of particular relevance to <a href=\"https://www.lesswrong.com/tag/ai\">AI Alignment</a>. Suppose you have something which is generally a good proxy for \"the stuff that humans care about\", it would be dangerous to have a powerful AI optimize for the proxy, in accordance with Goodhart's law, the proxy will breakdown.</p>\n<h2>Goodhart Taxonomy</h2>\n<p>In <a href=\"https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy\">Goodhart Taxonomy</a>, Scott Garrabrant identifies four kinds of Goodharting:</p>\n<ul>\n<li>Regressional Goodhart - When selecting for a proxy measure, you select not only for the true goal, but also for the difference between the proxy and the goal.</li>\n<li>Causal Goodhart - When there is a non-causal correlation between the proxy and the goal, intervening on the proxy may fail to intervene on the goal.</li>\n<li>Extremal Goodhart - Worlds in which the proxy takes an extreme value may be very different from the ordinary worlds in which the correlation between the proxy and the goal was observed.</li>\n<li>Adversarial Goodhart - When you optimize for a proxy, you provide an incentive for adversaries to correlate their goal with your proxy, thus destroying the correlation with your goal.</li>\n</ul>\n<h2>See Also</h2>\n<ul>\n<li><a href=\"https://lesswrong.com/tag/groupthink\">Groupthink</a>, <a href=\"https://lesswrong.com/tag/information-cascades\">Information cascade</a>, <a href=\"https://lesswrong.com/tag/affective-death-spiral\">Affective death spiral</a></li>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Adaptation_executers\">Adaptation executers</a>, <a href=\"https://lesswrong.com/tag/superstimuli\">Superstimulus</a></li>\n<li><a href=\"https://lesswrong.com/tag/signaling\">Signaling</a>, <a href=\"https://lesswrong.com/tag/filtered-evidence\">Filtered evidence</a></li>\n<li><a href=\"https://lesswrong.com/tag/cached-thought\">Cached thought</a></li>\n<li><a href=\"https://lesswrong.com/tag/modesty-argument\">Modesty argument</a>, <a href=\"https://lesswrong.com/tag/egalitarianism\">Egalitarianism</a></li>\n<li><a href=\"https://lesswrong.com/tag/rationalization\">Rationalization</a>, <a href=\"https://lesswrong.com/tag/dark-arts\">Dark arts</a></li>\n<li><a href=\"https://lesswrong.com/tag/epistemic-hygiene\">Epistemic hygiene</a></li>\n<li><a href=\"https://lesswrong.com/tag/scoring-rule\">Scoring rule</a></li>\n</ul>\n",
    "description_length": 2775,
    "viewCount": 480,
    "parentTagId": "rationality-failure-modes"
  },
  {
    "core-tag": "Rationality",
    "_id": "ALwRRZqvhaop8gxkT",
    "name": "Groupthink",
    "slug": "groupthink",
    "postCount": 35,
    "description_html": null,
    "description_length": null,
    "viewCount": 86,
    "parentTagId": "rationality-failure-modes"
  },
  {
    "core-tag": "Rationality",
    "_id": "4R8JYu4QF2FqzJxE5",
    "name": "Heuristics & Biases",
    "slug": "heuristics-and-biases",
    "postCount": 256,
    "description_html": "<p><strong>Heuristics</strong> and <strong>Biases</strong> are the ways human reasoning differs from a theoretical ideal agent, due to reasoning shortcuts that don't always work (heuristics) and systematic errors (biases).</p>\n<p><em>See also</em>: <a href=\"https://www.lesswrong.com/tag/affect-heuristic?showPostCount=true&amp;useTagName=true\">Affect Heuristic</a>, <a href=\"https://www.lesswrong.com/tag/confirmation-bias\">Confirmation Bias</a>, <a href=\"https://www.lesswrong.com/tag/fallacies\">Fallacies</a>, <a href=\"https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM\">Predictably Wrong</a>, <a href=\"https://www.lesswrong.com/tag/rationality?showPostCount=true\">Rationality</a>, <a href=\"https://www.lesswrong.com/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic\">Your Intuitions Are Not Magic</a>, <a href=\"https://lesswrong.com/tag/bias\">Bias</a>, <a href=\"https://lesswrong.com/tag/heuristic\">Heuristic</a></p>\n<h1>Basics</h1>\n<p><a href=\"https://www.lesswrong.com/posts/jnZbHi873v9vcpGpZ/what-s-a-bias-again\">“Cognitive biases”</a> are those obstacles to truth which are produced, not by the cost of information, nor by limited computing power, but by <em>the shape of our own mental machinery</em>. For example, our mental processes might be evolutionarily adapted to specifically believe some things that arent true, so that we could win political arguments in a tribal context. Or the mental machinery might be adapted not to particularly care whether something is true, such as when we feel the urge to believe what others believe to get along socially. Or the bias may be a side-effect of a useful reasoning heuristic. The availability heuristic is not itself a bias, but it gives rise to them; the machinery uses an algorithm (give things more evidential weight if they come to mind more readily) that does some good cognitive work but also produces systematic errors.</p>\n<p>Our brains are doing something wrong, and after a lot of experimentation and/or heavy thinking, someone identifies the problem verbally and concretely; then we call it a “(cognitive) bias.” Not to be confused with the colloquial “that person is biased,” which just means “that person has a skewed or prejudiced attitude toward something.”</p>\n<p>A bias is an obstacle to our goal of obtaining truth, and thus <em>in our way</em>.</p>\n<p>We are here to pursue the great human quest for truth: for we have desperate need of the knowledge, and besides, we're curious. To this end let us strive to overcome whatever obstacles lie in our way, whether we call them “biases” or not.</p>\n<p><a href=\"https://www.lesswrong.com/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic\">It's also useful to know the kinds of faults human brains are prone to, in the same way it's useful to know that your car's brakes are a little gummy (so you don't sail through a red light and into an 18-wheeler).</a></p>\n<p>The Sequence, <a href=\"https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM\">Predictably Wrong</a>, offers an excellent introduction to the topic for those who are not familiar.</p>\n<h1>Wait a minute... fallacies, biases, heuristics... what's the difference??</h1>\n<p>While a <strong>bias</strong> is always wrong, a <strong>heuristic</strong> is just a shortcut which may or may not give you an accurate answer. Just because you know complex mathematical methods for precisely calculating the flight of objects through space doesn't mean you should be using them to play volleyball. Which is to say, heuristics are necessary for actually getting anything done. But because they are just approximations they frequently <em>produce</em> biases, which is where the problem lies. \"Fallacy\" is often used to mean a very similar thing as bias on LessWrong. [Needs better clarification]</p>\n<p>A good example of a heuristic is the <a href=\"https://www.lesswrong.com/tag/affect-heuristic?showPostCount=true&amp;useTagName=true\">affect heuristic</a>-- people tend to guess unknown traits about people or things <em>based on</em> the perceived goodness of badness of known traits. In some circumstances this is a useful shortcut-- you may like to assume, for instance, that people who are good singers are more likely to be good dancers, too. However, it also frequently produces (unconscious) biases-- a bias towards believing that people who are tall and good looking have better moral character, for instance.</p>\n<h1>So if I learn all the biases, I can conquer the world with my superior intellect?</h1>\n<p>Well, no. If it were that easy we wouldn't need a community initially dedicated to overcoming bias (the name of <a href=\"https://www.overcomingbias.com/\">the blog which this website grew out of</a>). Unfortunately, <a href=\"https://www.iejme.com/article/university-students-knowledge-and-biases-in-conditional-probability-reasoning\">learning about a bias alone doesn't seem to improve your ability to avoid it in real life</a>. There's also the (major) issue that <a href=\"https://www.lesswrong.com/posts/AdYdLP2sRqPMoe8fb/knowing-about-biases-can-hurt-people\">knowing about biases can hurt people</a>. Instead of being purely focused on removing negative habits, there is now a major focus at LessWrong to implementing <a href=\"https://www.lesswrong.com/tag/techniques?showPostCount=false&amp;useTagName=false\">positive habits</a>. These are skills such as how to update (change your mind) the correct amount in response to evidence, how to resolve disagreements with others, how to introspect, and many more.</p>\n",
    "description_length": 5500,
    "viewCount": 180,
    "parentTagId": "rationality-failure-modes"
  },
  {
    "core-tag": "Rationality",
    "_id": "PJKgSRkXkCqXmCk3M",
    "name": "Mind Projection Fallacy",
    "slug": "mind-projection-fallacy",
    "postCount": 25,
    "description_html": "<p>The <strong>Mind Projection Fallacy</strong> is the error of projecting the properties of your own mind onto the external world. For example, one might erroneously think that because they enjoy the taste of chocolate, the chocolate has the inherent property of tastiness, and therefore everyone else must like its taste too.</p>\n<p>Overcoming the mind projection fallacy requires realizing that our minds are not transparent windows unto veridical reality; when you look at a rock, you experience not the rock itself, but your mind's <em>representation</em> of the rock, reconstructed from photons bouncing off its surface. Sugar in and of itself is not <em>inherently</em> sweet; the sugar itself only has the chemical properties that it does, which your brain <em>interprets</em> as sweet.</p>\n<h2>History</h2>\n<p>Physicist and <a href=\"https://lesswrong.com/tag/bayesianism\">Bayesian</a> philosopher <a href=\"https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes\">E.T. Jaynes</a> coined the term <em>mind projection fallacy</em> to refer to this kind of failure to distinguish between epistemological claims (statements about belief, about your map, about what we can <em>say</em> about reality) and ontological claims (statements about reality, about the territory, about how things <em>are</em>). In particular, the concept was applied in the critique of <a href=\"https://en.wikipedia.org/wiki/Frequentist_inference\">frequentist</a> interpretation of the notion of <a href=\"https://wiki.lesswrong.com/wiki/probability\">probability</a> as a property of physical systems rather than an epistemic device concerned with levels of certainty, <a href=\"https://lesswrong.com/tag/bayesian-probability\">Bayesian probability</a>.</p>\n<h2>Notable Posts</h2>\n<ul>\n<li><a href=\"https://lesswrong.com/lw/oi/mind_projection_fallacy/\">Mind Projection Fallacy</a></li>\n<li><a href=\"https://lesswrong.com/lw/oj/probability_is_in_the_mind/\">Probability is in the Mind</a></li>\n<li><a href=\"https://lesswrong.com/lw/8tv/examples_of_the_mind_projection_fallacy/\">Examples of the Mind Projection Fallacy?</a></li>\n</ul>\n<h2>See Also</h2>\n<ul>\n<li><a href=\"https://lesswrong.com/tag/bayesian-probability\">Bayesian probability</a></li>\n<li><a href=\"https://lesswrong.com/tag/magic\">Magic</a></li>\n<li><a href=\"https://lesswrong.com/tag/the-map-is-not-the-territory\">The map is not the territory</a></li>\n<li><a href=\"https://lesswrong.com/tag/2-place-and-1-place-words\">2-place and 1-place words</a></li>\n</ul>\n",
    "description_length": 2493,
    "viewCount": 301,
    "parentTagId": "rationality-failure-modes"
  },
  {
    "core-tag": "Rationality",
    "_id": "LDTSbmXtokYAsEq8e",
    "name": "Motivated Reasoning",
    "slug": "motivated-reasoning",
    "postCount": 69,
    "description_html": "<p><strong>Motivated Reasoning</strong> is a label for various mental processes that lead to desired conclusions regardless of the veracity of those conclusions.</p><p><i>Related</i>: <a href=\"https://www.lesswrong.com/tag/confirmation-bias\">Confirmation Bias</a>, <a href=\"https://www.lesswrong.com/tag/rationalization\">Rationalization</a>, <a href=\"https://www.lesswrong.com/tag/self-deception\">Self-deception</a>&nbsp;</p><h2>Notable Posts</h2><ul><li><a href=\"https://lessestwrong.com/lw/it/semantic_stopsigns/\">Semantic Stopsigns</a></li><li><a href=\"https://lessestwrong.com/lw/j2/explainworshipignore/\">Explain/Worship/Ignore?</a></li><li><a href=\"https://lessestwrong.com/lw/jy/avoiding_your_beliefs_real_weak_points/\">Avoiding Your Belief's Real Weak Points</a></li><li><a href=\"https://lessestwrong.com/lw/km/motivated_stopping_and_motivated_continuation/\">Motivated Stopping and Motivated Continuation</a></li><li><a href=\"https://lesswrong.com/lw/js/the_bottom_line/\">The Bottom Line</a></li></ul><h2>See also</h2><ul><li><a href=\"https://lessestwrong.com/tag/rationalization\">Rationalization</a></li><li><a href=\"https://lessestwrong.com/tag/motivated-skepticism\">Motivated skepticism</a></li><li><a href=\"https://lessestwrong.com/tag/filtered-evidence\">Filtered evidence</a></li><li><a href=\"https://lessestwrong.com/tag/confirmation-bias\">Positive bias</a></li><li><a href=\"https://lessestwrong.com/tag/aversion-ugh-fields\">Ugh field</a></li></ul>",
    "description_length": 1462,
    "viewCount": 118,
    "parentTagId": "rationality-failure-modes"
  },
  {
    "core-tag": "Rationality",
    "_id": "5BvRW4FxdD8DFhiew",
    "name": "Pica",
    "slug": "pica",
    "postCount": 6,
    "description_html": "<p><strong>Pica</strong> is the name of an eating disorder where people eat non-nutritional items. An example is people with iron deficiency eating ice cubes; there is no iron in ice cubes, but iron and crunchiness sort of go together. By analogy, <strong>pica</strong> is also when people are missing something, and respond by doing something which doesn't provide it, or doesn't provide much of it, like watching sitcoms because they're lonely or playing Minecraft because they feel unproductive.</p><p><a href=\"https://www.lesswrong.com/posts/L6Ktf952cwdMJnzWm/motive-ambiguity?commentId=QLS75v2wdDHpo9CX3\">AllAmericanBreakfast suggests</a> that pica is like <a href=\"https://www.lesswrong.com/tag/goodhart-s-law\">Goodhart's Law</a>, with the added <a href=\"https://www.lesswrong.com/tag/goodhart-s-law\">failure</a> that the metric being maximized isn't even clearly related to the problem you're trying to solve, giving two examples:</p><ul><li>Evaluate your startup by the sheer effort you're putting in? That's Goodhart's Law. Evaluate it by <a href=\"https://www.youtube.com/watch?v=zbKaPN-0NcM&amp;ab_channel=LeslieKnopeRocks\">how cool the office looks</a>? That's pica.</li><li>Evaluate your relationship by the sheer amount of physical affection? That's Goodhart's Law. Evaluate it by how much misery you put each other through \"for love?\" That's pica</li></ul><p><strong>Related Pages:</strong> <a href=\"https://www.lesswrong.com/tag/akrasia\">Akrasia</a>, <a href=\"https://www.lesswrong.com/tag/goodhart-s-law\">Goodhart's Law</a></p>",
    "description_length": 1543,
    "viewCount": 137,
    "parentTagId": "rationality-failure-modes"
  },
  {
    "core-tag": "Rationality",
    "_id": "9YFoDPFwMoWthzgkY",
    "name": "Pitfalls of Rationality",
    "slug": "pitfalls-of-rationality",
    "postCount": 76,
    "description_html": "<p><strong>Pitfalls of Rationality</strong> are frequent <a href=\"https://www.lesswrong.com/tag/failure-mode\">error modes</a>, obstacles or problems that arise when people try to practice rationality, or engage with rationality-related materials. Related concepts include the \"valley of bad rationality\".<br><br>There are two threads touched in posts under this tag:</p><ol><li>Things that go wrong when people try to be more rational and they unintentionally end up making things worse.</li><li>Arguably, why haven't rationalists visible succeeded at their bold and ambitious goals yet?</li></ol><p>Regarding the first point, from <a href=\"https://www.lesswrong.com/posts/oZNXmHcdhb4m7vwsv/incremental-progress-and-the-valley\">Incremental Progress and the Valley</a>:</p><blockquote><p>Ah.&nbsp; Well, here's the the thing:&nbsp; An <i>incremental</i> step in the direction of rationality, if the result is still irrational in other ways, does not have to yield <i>incrementally </i>more winning.</p><p>The optimality theorems that we have for probability theory and decision theory, are for <i>perfect</i> probability theory and decision theory.&nbsp; There is no companion theorem which says that, starting from some flawed initial form, every <i>incremental</i> modification of the algorithm that takes the structure closer to the ideal, must yield an <i>incremental</i> improvement in performance.&nbsp; This has not yet been proven, because it is not, in fact, true.</p></blockquote><p>See also: <a href=\"https://www.lesswrong.com/tag/criticisms-of-the-rationalist-movement\">Criticisms of the Rationalist Movement</a>, <a href=\"https://www.lesswrong.com/tag/value-of-rationality\">Value of Rationality</a></p>",
    "description_length": 1714,
    "viewCount": 112,
    "parentTagId": "rationality-failure-modes"
  },
  {
    "core-tag": "Rationality",
    "_id": "ZzxvopS4BwLuQy42n",
    "name": "Rationalization",
    "slug": "rationalization",
    "postCount": 77,
    "description_html": "<p><strong>Rationalization </strong>is the act of finding reasons to believe what one has already decided they want to believe. It is a decidedly terrible way to arrive at true beliefs.</p><blockquote><p><i>“Rationalization.” What a curious term. I would call it a wrong word. You cannot “rationalize” what is not already rational. It is as if “lying” were called “truthization.” – </i><a href=\"https://www.lessestwrong.com/posts/SFZoEBpLo9frSJGkc/rationalization\">Rationalization</a></p></blockquote><p>Rationality starts from evidence, and then crunches forward through belief updates, in order to output a probable conclusion. \"Rationalization\" starts from a conclusion, and then works backward to arrive at arguments apparently favoring that conclusion. Rationalization argues for a side already selected; rationality tries to choose between sides.</p><p>Rationalization can be conscious or unconscious. It can take on a blatant, conscious form, in which you are aware that you want a particular side to be correct and you deliberately compose arguments for only that side, looking over the evidence and consciously filtering which facts will be presented. Or it can occur at perceptual speeds, without conscious intent or conscious awareness.</p><p>Defeating rationalization - or even <i>discovering</i> rationalizations - is a lifelong battle for the aspiring rationalist.</p><h2>See Also</h2><ul><li><a href=\"https://lessestwrong.com/tag/motivated-skepticism\">Motivated skepticism</a>, <a href=\"https://lessestwrong.com/tag/motivated-reasoning\">motivated cognition</a></li><li><a href=\"https://lessestwrong.com/tag/filtered-evidence\">Filtered evidence</a></li><li><a href=\"https://lessestwrong.com/tag/fake-simplicity\">Fake simplicity</a></li><li><a href=\"https://lessestwrong.com/tag/self-deception\">Self-deception</a></li><li><a href=\"https://lessestwrong.com/tag/litany-of-gendlin\">Litany of Gendlin</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Occam's_Imaginary_Razor\">Occam's Imaginary Razor</a></li><li><a href=\"https://lessestwrong.com/tag/hope\">Hope</a>, <a href=\"https://lessestwrong.com/tag/oops\">oops</a></li></ul>",
    "description_length": 2140,
    "viewCount": 101,
    "parentTagId": "rationality-failure-modes"
  },
  {
    "core-tag": "Rationality",
    "_id": "YTCrHWYHAsAD74EHo",
    "name": "Self-Deception",
    "slug": "self-deception",
    "postCount": 83,
    "description_html": "<p><strong>Self-deception</strong> is a state of preserving a wrong <a href=\"https://lesswrong.com/tag/belief\">belief</a>, often facilitated by denying or <a href=\"https://lesswrong.com/tag/rationalization\">rationalizing away</a> the relevance, significance, or importance of opposing <a href=\"https://lesswrong.com/tag/evidence\">evidence</a> and logical arguments. Beliefs supported by self-deception are often chosen for reasons other than how closely those beliefs approximate <a href=\"https://lesswrong.com/tag/truth-semantics-and-meaning\">truth</a>.</p><p><i>Related:</i> <a href=\"https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&amp;useTagName=true\">Anticipated Experiences</a>, <a href=\"https://www.lesswrong.com/tag/motivated-reasoning?showPostCount=true&amp;useTagName=true\">Motivated Reasoning</a>, <a href=\"https://www.lesswrong.com/tag/rationalization?showPostCount=true&amp;useTagName=true\">Rationalization</a></p><p>On LessWrong, a common distinction is between <a href=\"https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=false&amp;useTagName=false\">beliefs as expectation-controllers</a> and <a href=\"https://www.lesswrong.com/posts/dLbkrPu5STNCBLRjr/applause-lights\">other</a> <a href=\"https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password\">things</a> <a href=\"https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/CqyJzDZWvGhhFJ7dY\">people</a> <a href=\"https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/RmCjazjupRGcHSm5N\">commonly</a> <a href=\"https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/nYkMLFpx77Rz3uo9c\">label</a> as beliefs. When these different things conflict, a person is said to have engaged in self-deception.</p><p>Deceiving yourself is <a href=\"https://lesswrong.com/lw/s/belief_in_selfdeception/\">harder than it seems</a>. What looks like a successively adopted false belief may actually be just a <a href=\"https://lesswrong.com/tag/belief-in-belief\">belief in false belief</a>.</p><p>An example from <a href=\"https://www.lesswrong.com/posts/rZX4WuufAPbN6wQTv/no-really-i-ve-deceived-myself\">No, Really, I've Deceived Myself</a>:</p><blockquote><p><i>When this woman was in high school, she thought she was an atheist.&nbsp; But she decided, at that time, that she should act as if she believed in God.&nbsp; And then—she told me earnestly—over time, she came to really believe in God.</i></p></blockquote><blockquote><p><i>So far as I can tell, she is completely wrong about that.&nbsp; Always throughout our conversation, she said, over and over, \"I believe in God\", never once, \"There is a God.\"&nbsp; When I asked her why she was religious, she never once talked about the consequences of God existing, only about the consequences of believing in God.&nbsp; Never, \"God will help me\", always, \"my belief in God helps me\".&nbsp; When I put to her, \"Someone who just wanted the truth and looked at our universe would not even invent God as a hypothesis,\" she agreed outright.</i></p></blockquote><blockquote><p><i>She hasn't actually deceived herself into believing that God exists or that the Jewish religion is true.&nbsp; Not even close, so far as I can tell.</i></p></blockquote><blockquote><p><i>On the other hand, I think she really does believe she has deceived herself.</i></p></blockquote><h2>Blog posts</h2><ul><li><a href=\"http://lesswrong.com/lw/h7/selfdeception_hypocrisy_or_akrasia/\"><u>Self-deception: Hypocrisy or Akrasia?</u></a></li></ul><h2>Sequence by <a href=\"https://wiki.lesswrong.com/wiki/Eliezer_Yudkowsky\"><u>Eliezer Yudkowsky</u></a></h2><p><i>Part of </i><a href=\"https://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\"><i><u>How To Actually Change Your Mind</u></i></a><i> sequence</i></p><ul><li><a href=\"http://lesswrong.com/lw/r/no_really_ive_deceived_myself/\"><u>No, Really, I've Deceived Myself</u></a></li><li><a href=\"http://lesswrong.com/lw/s/belief_in_selfdeception\"><u>Belief in Self Deception</u></a></li><li><a href=\"http://lesswrong.com/lw/1f/moores_paradox\"><u>Moore's Paradox</u></a></li><li><a href=\"http://lesswrong.com/lw/1o/dont_believe_youll_selfdeceive\"><u>Don't Believe You'll Self-Deceive</u></a></li><li><a href=\"http://lesswrong.com/lw/1r/striving_to_accept\"><u>Striving to Accept</u></a></li></ul><h2>Other resources</h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Robin_Hanson\"><u>Robin Hanson</u></a> (2009). \"Enhancing Our Truth Orientation\". in Larissa Behrendt, Nick Bostrom. <i>Human Enhancement</i>. Oxford University Press. (<a href=\"http://hanson.gmu.edu/moretrue.pdf\"><u>PDF</u></a>)</li></ul><h2>See also</h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Anti-epistemology\"><u>Anti-epistemology</u></a>, <a href=\"https://wiki.lesswrong.com/wiki/Belief_in_belief\"><u>Belief in belief</u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Semantic_stopsign\"><u>Semantic stopsign</u></a>, <a href=\"https://wiki.lesswrong.com/wiki/Compartmentalization\"><u>Compartmentalization</u></a>, <a href=\"https://wiki.lesswrong.com/wiki/Motivated_skepticism\"><u>Motivated skepticism</u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Improper_belief\"><u>Improper belief</u></a>, <a href=\"https://wiki.lesswrong.com/wiki/Truth\"><u>Truth</u></a></li></ul>",
    "description_length": 5228,
    "viewCount": 188,
    "parentTagId": "rationality-failure-modes"
  },
  {
    "core-tag": "Rationality",
    "_id": "stnsBEmuGpnSfQ5vj",
    "name": "Sunk-Cost Fallacy",
    "slug": "sunk-cost-fallacy",
    "postCount": 12,
    "description_html": "<p>The <strong>Sunk Cost Fallacy</strong> is the tendency to consider costs that have already been paid and cannot be reclaimed when deciding whether to continue a project. Thinking you have to keep going because you've already put in so much. In reality, it is usually much more important to consider whether the benefits of finishing the project are worth more than the <i>remaining</i> costs.</p><h2>External Links</h2><ul><li><a href=\"http://www.gwern.net/Sunk%20cost\">Are sunk costs fallacies?</a></li></ul><h2>See Also</h2><ul><li><a href=\"https://lessestwrong.com/tag/status-quo-bias\">Status quo bias</a></li><li><a href=\"https://lessestwrong.com/tag/loss-aversion\">Loss aversion</a>, <a href=\"https://lessestwrong.com/tag/prospect-theory\">Prospect theory</a></li><li><a href=\"https://lessestwrong.com/tag/cached-thought\">Cached thought</a></li><li><a href=\"https://lessestwrong.com/tag/narrative-fallacy\">Narrative fallacy</a></li></ul>",
    "description_length": 944,
    "viewCount": 105,
    "parentTagId": "rationality-failure-modes"
  },
  {
    "core-tag": "Rationality",
    "_id": "kCuRQE5Tkv9zeKyzK",
    "name": "Common Knowledge",
    "slug": "common-knowledge",
    "postCount": 30,
    "description_html": "<p><strong>Common knowledge</strong> is information that everyone knows and, importantly, that everyone knows that everyone knows, and so on, ad infinitum. If information is <i>common knowledge</i> in a group of people, that information that can be relied and acted upon with the trust that everyone else is also coordinating around that information. This stands, in contrast, to merely publicly known information where one person cannot be sure that another person knows the information, or that another person knows that they know the information. Establishing true common knowledge is, in fact, rather hard.</p><p><strong>Related Pages:</strong> <a href=\"https://www.lesswrong.com/tag/public-discourse\">Public discourse</a>, <a href=\"https://www.lesswrong.com/tag/consensus\">Consensus</a>, <a href=\"https://www.lesswrong.com/tag/inferential-distance\">Inferential Distance</a></p><p><strong>External posts:</strong>&nbsp;<br><a href=\"https://www.scottaaronson.com/blog/?p=3376\">The Kolmogorov option</a> by Scott Aaronson<br><a href=\"https://slatestarcodex.com/2017/10/23/kolmogorov-complicity-and-the-parable-of-lightning/\">kolmogorov complicity and-the parable of lightning</a> by Scott Alexander</p>",
    "description_length": 1204,
    "viewCount": 287,
    "parentTagId": "rationality-communication"
  },
  {
    "core-tag": "Rationality",
    "_id": "ZXFpyQWPB5ideFbEG",
    "name": "Conversation (topic)",
    "slug": "conversation-topic",
    "postCount": 135,
    "description_html": "<p>A <strong>conversation </strong>is when two people talk or correspond. Most content here is about <i>how to have good conversations.</i>&nbsp;(<i>This wikitag needs work.)</i><br><br>For records of conversations, see Interviews, Debates,...</p><p>See also:</p><ul><li>Communication</li><li>Communication Cultures</li><li>Relationshops</li><li>Community</li></ul><h2><i>Conversation Halter</i></h2><p>This term was introduced on LessWrong by Eliezer in the <a href=\"https://www.lesswrong.com/posts/wqmmv6NraYv4Xoeyj/conversation-halters\">eponymous post</a>:</p><blockquote><p><i>While working on my book, I found in passing that I'd developed a list of what I started out calling \"stonewalls\", but have since decided to refer to as \"conversation halters\".&nbsp; These tactics of argument are distinguished by their being attempts to cut off the flow of debate - which is rarely the wisest way to think, and should certainly rate an alarm bell.</i></p></blockquote>",
    "description_length": 966,
    "viewCount": 251,
    "parentTagId": "rationality-communication"
  },
  {
    "core-tag": "Rationality",
    "_id": "GpcY5Q226TTy4Cv8N",
    "name": "Decoupling vs Contextualizing",
    "slug": "decoupling-vs-contextualizing",
    "postCount": 9,
    "description_html": null,
    "description_length": null,
    "viewCount": 89,
    "parentTagId": "rationality-communication"
  },
  {
    "core-tag": "Rationality",
    "_id": "wzgcQCrwKfETcBpR9",
    "name": "Disagreement",
    "slug": "disagreement",
    "postCount": 128,
    "description_html": "<p><strong>Disagreement</strong> is when two people have different beliefs.</p><h2>Aumann's Agreement Theorem</h2><p>Considered of particular relevance to disagreement between people trying to be rational, <a href=\"https://lesswrong.com/tag/aumann-s-agreement-theorem\">Aumann's agreement theorem</a> can be <a href=\"https://lesswrong.com/tag/aumann-agreement\">informally interpreted</a> as suggesting that if two people are honest seekers of truth, and both <i>believe</i> each other to be honest, then they should update on each other's opinions and quickly reach agreement. The very fact that a person believes something is <a href=\"https://lesswrong.com/tag/rational-evidence\">Rational evidence</a> that that something is true, and so this fact <a href=\"http://www.overcomingbias.com/2007/01/extraordinary_c.html\">should be taken into account</a> when forming your belief.</p><p>Outside of well-functioning <a href=\"https://lesswrong.com/tag/prediction-markets\">prediction markets</a>, Aumann agreement can probably only be approximated by careful deliberative discourse. Interest in Aumann agreement has waned in recent years within the Rationalist community, perhaps out of a sense Aumann agreement cannot be practically achieved by humans – there is too much background information to be exchanged. Instead, people now focus on things more like Double-Crux</p><h2>External Posts</h2><ul><li><a href=\"http://www.overcomingbias.com/2006/12/reasonable_disa.html\">Reasonable Disagreement</a> by Nicholas Shackel</li><li><a href=\"http://www.overcomingbias.com/2006/12/agreeing_to_agr.html\">Agreeing to Agree</a> by <a href=\"https://en.wikipedia.org/wiki/Hal_Finney_(cypherpunk)\">Hal Finney</a></li><li><a href=\"http://www.overcomingbias.com/2006/12/you_are_never_e.html\">You Are Never Entitled to Your Opinion</a> by <a href=\"https://lesswrong.com/tag/robin-hanson\">Robin Hanson</a></li><li><a href=\"http://www.overcomingbias.com/2006/12/normative_bayes.html\">Normative Bayesianism and Disagreement</a> by Nicholas Shackel</li><li><a href=\"http://www.overcomingbias.com/2009/01/disagreement-is-nearfar-bias.html\">Disagreement is Near/Far Bias</a> by <a href=\"https://lesswrong.com/tag/robin-hanson\">Robin Hanson</a></li><li><a href=\"http://www.spencergreenberg.com/2011/12/the-seven-causes-of-disagreement/\">The Seven Causes of Disagreement</a> by Spencer Greenberg</li><li><a href=\"http://www.paulgraham.com/disagree.html\">How to Disagree</a> by Paul Graham</li></ul><h2>See also</h2><ul><li><a href=\"https://lesswrong.com/tag/aumann-s-agreement-theorem\">Aumann's agreement theorem</a></li><li><a href=\"https://lesswrong.com/tag/modesty-argument\">Modesty argument</a></li><li><a href=\"https://lesswrong.com/tag/disagreements-on-less-wrong\">Disagreements on Less Wrong</a></li><li><a href=\"https://lesswrong.com/tag/arguments-as-soldiers\">Arguments as soldiers</a></li><li><a href=\"https://www.lesswrong.com/tag/double-crux\">Double-Crux</a></li><li><a href=\"https://www.lesswrong.com/tag/conversation-topic\">Conversation</a></li></ul><h2>References</h2><p>(<a href=\"http://hanson.gmu.edu/deceive.pdf\">PDF</a>, <a href=\"http://www.newmedia.ufm.edu/gsm/index.php?title=Are_Disagreements_Honest%3F\">Talk video</a>)</p><ul><li><a href=\"http://cowles.econ.yale.edu/P/cp/p05b/p0552.pdf\">We Can't Disagree Forever</a> by John Geanakoplos and Heraklis Polemarchakis</li><li><a href=\"http://www.kellogg.northwestern.edu/research/math/papers/377.pdf\">Information, Trade, and Common Knowledge</a> by Paul Milgrom and Nancy Stokey</li></ul>",
    "description_length": 3529,
    "viewCount": 126,
    "parentTagId": "rationality-communication"
  },
  {
    "core-tag": "Rationality",
    "_id": "EdDGrAxYcrXnKkDca",
    "name": "Distillation & Pedagogy",
    "slug": "distillation-and-pedagogy",
    "postCount": 172,
    "description_html": "<p><strong>Distillation</strong> is the process of taking a complex subject, and making it easier to understand. <strong>Pedagogy </strong>is the method and practice of teaching. A good intellectual pipeline requires not just discovering new ideas, but making it easier for newcomers to learn them, stand on the shoulders of giants, and discover even more ideas.</p><p>Chris Olah, founder of <a href=\"https://distill.pub/\">distill.pub</a>, writes in his essay <a href=\"https://distill.pub/2017/research-debt/\">Research Debt</a>:</p><blockquote><p>Programmers talk about technical debt: there are ways to write software that are faster in the short run but problematic in the long run. Managers talk about institutional debt: institutions can grow quickly at the cost of bad practices creeping in. Both are easy to accumulate but hard to get rid of.</p><p>Research can also have debt. It comes in several forms:</p><ul><li><strong>Poor Exposition</strong> – Often, there is no good explanation of important ideas and one has to struggle to understand them. This problem is so pervasive that we take it for granted and don’t appreciate how much better things could be.</li><li><strong>Undigested Ideas</strong> – Most ideas start off rough and hard to understand. They become radically easier as we polish them, developing the right analogies, language, and ways of thinking.</li><li><strong>Bad abstractions and notation</strong> – Abstractions and notation are the user interface of research, shaping how we think and communicate. Unfortunately, we often get stuck with the first formalisms to develop even when they’re bad. For example, an object with extra electrons is negative, and pi is wrong.</li><li><strong>Noise</strong> – Being a researcher is like standing in the middle of a construction site. Countless papers scream for your attention and there’s no easy way to filter or summarize them.Because most work is explained poorly, it takes a lot of energy to understand each piece of work. For many papers, one wants a simple one sentence explanation of it, but needs to fight with it to get that sentence. Because the simplest way to get the attention of interested parties is to get everyone’s attention, we get flooded with work. Because we incentivize people being “prolific,” we get flooded with a lot of work… We think noise is the main way experts experience research debt.</li></ul><p>The insidious thing about research debt is that it’s normal. Everyone takes it for granted, and doesn’t realize that things could be different. For example, it’s normal to give very mediocre explanations of research, and people perceive that to be the ceiling of explanation quality. On the rare occasions that truly excellent explanations come along, people see them as one-off miracles rather than a sign that we could systematically be doing better.</p></blockquote><p>See also <a href=\"lesswrong.com/tag/scholarship-and-learning\">Scholarship and Learning</a>, and <a href=\"https://www.lesswrong.com/tag/good-explanations-advice\">Good Explanations</a>.</p>",
    "description_length": 3061,
    "viewCount": 111,
    "parentTagId": "rationality-communication"
  },
  {
    "core-tag": "Rationality",
    "_id": "KQP7fNjin8Zqg4N2x",
    "name": "Double-Crux",
    "slug": "double-crux",
    "postCount": 34,
    "description_html": "<p><strong>Double-Crux</strong> is a technique for addressing complex disagreements by systematically uncovering the <i>cruxes</i> upon which the disagreement hinges. A crux for an individual is any fact that if they believed differently about it, they would change their conclusion in the overall disagreement. A <i>double-crux </i>is a crux for both parties. Perhaps we disagree on whether swimming in a lake is safe. A crux for each of us is the presence of crocodiles in water: I believe there aren't, you believe there are. Either of us would change our mind about the safety if we were persuaded about this crux.</p><p>Double-Crux differs from typical debates which are usually adversarial (your opinion vs mine), and instead attempt to be a collaborative attempt to uncover the true structure of the disagreement and what would change the disputants minds.</p><p>Related: <a href=\"https://www.lesswrong.com/tag/disagreement\">Disagreement</a>, <a href=\"https://www.lesswrong.com/tag/conversation-topic\">Conversation</a></p><p>A version of the technique is described in <a href=\"https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-resolving-disagreement\">Double Crux – A Strategy for Resolving Disagreement</a> written by (then) CFAR instructor, Duncan_Sabien. The Center for Applied Rationality (CFAR) originated the technique. Eli Tyre, another CFAR instructor who has spent a lot of time developing the technique, more recently shared <a href=\"https://www.lesswrong.com/posts/hNztRARB52Riy36Kz/the-basic-double-crux-pattern\">The Basic Double Crux pattern</a>.</p><h2>See Also</h2><ul><li><a href=\"https://srconstantin.wordpress.com/2017/08/30/gleanings-from-double-crux-on-the-craft-is-not-the-community/\">Gleanings from Double Crux on “The Craft is Not The Community”</a> - a writeup of Double-Crux being used in practice.</li></ul>",
    "description_length": 1866,
    "viewCount": 835,
    "parentTagId": "rationality-communication"
  },
  {
    "core-tag": "Rationality",
    "_id": "WPkEd3et8f488w8LT",
    "name": "Good Explanations (Advice)",
    "slug": "good-explanations-advice",
    "postCount": 18,
    "description_html": "<p>This <strong>Good Explanations (Advice) </strong>tag<strong> </strong>is for advice on how to write good explanations.&nbsp;</p>",
    "description_length": 131,
    "viewCount": 133,
    "parentTagId": "rationality-communication"
  },
  {
    "core-tag": "Rationality",
    "_id": "A4kr45wS7fBW5PBpf",
    "name": "Ideological Turing Tests",
    "slug": "ideological-turing-tests",
    "postCount": 12,
    "description_html": null,
    "description_length": null,
    "viewCount": 588,
    "parentTagId": "rationality-communication"
  },
  {
    "core-tag": "Rationality",
    "_id": "YQW2DxpZFTrqrxHBJ",
    "name": "Inferential Distance",
    "slug": "inferential-distance",
    "postCount": 53,
    "description_html": "<p><strong>Inferential Distance</strong> between two people with respect to an item of knowledge is the amount of steps or concepts a person needs to share before they can successfully communicate the <a href=\"https://www.lesswrong.com/tag/object-level-and-meta-level\">object level</a> point. This can be thought of as the missing foundation or building block concepts needed to think clearly about a specific thing.</p><p>In <a href=\"https://www.lessestwrong.com/posts/HLqWn5LASfhhArZ7w/expecting-short-inferential-distances\">Expecting Short Inferential Distances</a>, Eliezer Yudkowsky posits that humans systematically underestimate inferential distances.</p><blockquote><p><i>And if you think you can explain the concept of “systematically underestimated inferential distances” briefly, in just a few words, I’ve got some sad news for you&nbsp;.&nbsp;.&nbsp;. – </i><a href=\"https://www.lessestwrong.com/posts/HLqWn5LASfhhArZ7w/expecting-short-inferential-distances\">Expecting Short Inferential Distances</a></p></blockquote><h2>Example: Evidence for Evolution</h2><p>Explaining the <a href=\"https://lessestwrong.com/tag/evidence\">evidence</a> for the theory of <a href=\"https://lessestwrong.com/tag/evolution\">evolution</a> to a physicist would be easy; even if the physicist didn't already know about evolution, they would understand the concepts of evidence, <a href=\"https://lessestwrong.com/tag/occam-s-razor\">Occam's razor</a>, naturalistic explanations, and the general orderly nature of the universe. Explaining the evidence for the theory of evolution to someone without a science background would be much harder. Before even mentioning the specific evidence for evolution, you would have to explain the concept of evidence, why some kinds of evidence are more valuable than others, what does and doesn't count as evidence, and so on. This would be unlikely to work during a short conversation.</p><p>There is a short inferential distance between you and the physicist; there is a very long inferential distance between you and the person without any science background. Many members of Less Wrong believe that expecting short inferential distances is a classic error. It is also a very difficult problem to solve, since most people will feel <a href=\"https://wiki.lesswrong.com/wiki/offence\">offended</a> if you explicitly say that there is too great an inferential distance between you to explain a theory properly. Some people have attempted to explain this through <a href=\"https://lessestwrong.com/tag/evolutionary-psychology\">evolutionary psychology</a>: in the ancestral environment, there was minimal difference in knowledge between people, and therefore no need to worry about inferential distances.</p><h2>External Links</h2><ul><li><a href=\"http://everydayutilitarian.com/essays/why-its-hard-to-explain-things-inferential-distance/\">Why It's Hard to Explain Things: Inferential Distance</a> by Peter Hurford</li><li><a href=\"https://jkorpela.fi/wiio.html\">How all human communication fails, except by accident, or a commentary of Wiio's laws</a></li></ul><h2>See Also</h2><ul><li><a href=\"https://lessestwrong.com/tag/general-knowledge\">General knowledge</a></li><li><a href=\"https://lessestwrong.com/tag/modesty-argument\">Modesty argument</a></li><li><a href=\"https://lessestwrong.com/tag/illusion-of-transparency\">Illusion of transparency</a></li><li><a href=\"https://lessestwrong.com/tag/absurdity-heuristic\">Absurdity heuristic</a></li><li><a href=\"https://www.lesswrong.com/tag/common-knowledge\">Common Knowledge</a></li></ul>",
    "description_length": 3555,
    "viewCount": 388,
    "parentTagId": "rationality-communication"
  },
  {
    "core-tag": "Rationality",
    "_id": "sHbKQDqrSinRPcnBv",
    "name": "Information Cascades",
    "slug": "information-cascades",
    "postCount": 18,
    "description_html": "<p>An <strong>information cascade</strong> occurs when people update on other people's beliefs, which may individually be a <a href=\"https://www.lesswrong.com/tag/aumann-s-agreement-theorem\">rational decision</a> but may still result in a self-reinforcing community opinion that does not necessarily reflect reality.</p><h2>See Also</h2><ul><li><a href=\"/tag/groupthink\">Groupthink</a></li><li><a href=\"https://lessestwrong.com/tag/egalitarianism\">Egalitarianism</a>, <a href=\"https://lessestwrong.com/tag/modesty-argument\">Modesty argument</a></li><li><a href=\"https://lessestwrong.com/tag/epistemic-luck\">Epistemic luck</a>, <a href=\"https://lessestwrong.com/tag/privileging-the-hypothesis\">Privileging the hypothesis</a></li><li><a href=\"https://lessestwrong.com/tag/free-floating-belief\">Free-floating belief</a></li><li><a href=\"https://lessestwrong.com/tag/groupthink\">Groupthink</a>, <a href=\"https://lessestwrong.com/tag/affective-death-spiral\">Affective death spiral</a></li><li><a href=\"https://lessestwrong.com/tag/goodhart-s-law\">Goodhart's law</a></li><li><a href=\"https://lessestwrong.com/tag/religion\">Religion</a></li></ul>",
    "description_length": 1139,
    "viewCount": 193,
    "parentTagId": "rationality-communication"
  },
  {
    "core-tag": "Rationality",
    "_id": "LNsEBXoFdAy8yzvbw",
    "name": "Memetic Immune System",
    "slug": "memetic-immune-system",
    "postCount": 28,
    "description_html": "<p><strong>Memetic Immune System</strong></p><blockquote>Intelligent people sometimes do things more stupid than stupid people are capable of. There are a variety of reasons for this; but one has to do with the fact that all cultures have dangerous memes circulating in them, and cultural antibodies to those memes. The trouble is that these antibodies are not logical. On the contrary; these antibodies are often highly <em>illogical</em>. They are the blind spots that let us live with a dangerous meme without being impelled to action by it.</blockquote><p>-Phil Goetz, <a href=\"https://www.lesswrong.com/posts/aHaqgTNnFzD7NGLMx/reason-as-memetic-immune-disorder\">Reason as memetic immune disorder</a></p>",
    "description_length": 708,
    "viewCount": 128,
    "parentTagId": "rationality-communication"
  },
  {
    "core-tag": "Rationality",
    "_id": "FtT2T9bRbECCGYxrL",
    "name": "Philosophy of Language",
    "slug": "philosophy-of-language",
    "postCount": 198,
    "description_html": null,
    "description_length": null,
    "viewCount": 137,
    "parentTagId": "rationality-communication"
  },
  {
    "core-tag": "Rationality",
    "_id": "RE6h98Ziwcfh4EP9T",
    "name": "Steelmanning",
    "slug": "steelmanning",
    "postCount": 41,
    "description_html": "<p><strong>Steelmanning </strong>is the act of taking a view, or opinion, or argument and constructing the strongest possible version of it. It is the opposite of strawmanning.</p><p><strong>External Posts:</strong><br><a href=\"https://thingofthings.wordpress.com/2016/08/09/against-steelmanning/\">Against Steelmanning</a> by Thing of Things</p><p>See also: <a href=\"http://lesswrong.com/tag/disagreement\">Disagreement</a>, <a href=\"https://www.lesswrong.com/tag/ideological-turing-tests\">Ideological Turing Tests</a>, <a href=\"https://lessestwrong.com/tag/least-convenient-possible-world\">Least convenient possible world</a></p>",
    "description_length": 629,
    "viewCount": 1315,
    "parentTagId": "rationality-communication"
  },
  {
    "core-tag": "Rationality",
    "_id": "KQP7fNjin8Zqg4N2x",
    "name": "Double-Crux",
    "slug": "double-crux",
    "postCount": 34,
    "description_html": "<p><strong>Double-Crux</strong> is a technique for addressing complex disagreements by systematically uncovering the <i>cruxes</i> upon which the disagreement hinges. A crux for an individual is any fact that if they believed differently about it, they would change their conclusion in the overall disagreement. A <i>double-crux </i>is a crux for both parties. Perhaps we disagree on whether swimming in a lake is safe. A crux for each of us is the presence of crocodiles in water: I believe there aren't, you believe there are. Either of us would change our mind about the safety if we were persuaded about this crux.</p><p>Double-Crux differs from typical debates which are usually adversarial (your opinion vs mine), and instead attempt to be a collaborative attempt to uncover the true structure of the disagreement and what would change the disputants minds.</p><p>Related: <a href=\"https://www.lesswrong.com/tag/disagreement\">Disagreement</a>, <a href=\"https://www.lesswrong.com/tag/conversation-topic\">Conversation</a></p><p>A version of the technique is described in <a href=\"https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-resolving-disagreement\">Double Crux – A Strategy for Resolving Disagreement</a> written by (then) CFAR instructor, Duncan_Sabien. The Center for Applied Rationality (CFAR) originated the technique. Eli Tyre, another CFAR instructor who has spent a lot of time developing the technique, more recently shared <a href=\"https://www.lesswrong.com/posts/hNztRARB52Riy36Kz/the-basic-double-crux-pattern\">The Basic Double Crux pattern</a>.</p><h2>See Also</h2><ul><li><a href=\"https://srconstantin.wordpress.com/2017/08/30/gleanings-from-double-crux-on-the-craft-is-not-the-community/\">Gleanings from Double Crux on “The Craft is Not The Community”</a> - a writeup of Double-Crux being used in practice.</li></ul>",
    "description_length": 1866,
    "viewCount": 835,
    "parentTagId": "rationality-techniques"
  },
  {
    "core-tag": "Rationality",
    "_id": "AeqCtS3BaY3cwzKAs",
    "name": "Fermi Estimation",
    "slug": "fermi-estimation",
    "postCount": 37,
    "description_html": "<p>A <strong>Fermi Estimation</strong> is a rough calculation which aims to be right within ~an order of magnitude, prioritizing getting a good enough to be useful answer without putting large amounts of thought and research in rather than being extremely accurate.</p><p><strong>Related Pages:</strong> <a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\">Forecasting &amp; Prediction</a></p>",
    "description_length": 407,
    "viewCount": 194,
    "parentTagId": "rationality-techniques"
  },
  {
    "core-tag": "Rationality",
    "_id": "JHzjkFnQgsrRrucqQ",
    "name": "Focusing",
    "slug": "focusing",
    "postCount": 27,
    "description_html": "<p><strong>Focusing</strong> refers to a family of introspective techniques taught by <a href=\"https://www.lesswrong.com/tag/center-for-applied-rationality-cfar\">CFAR</a> whose aim is to access one's \"gut\" or \"<a href=\"https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2?useTagName=false\">System 1</a>\" feelings. Archetypically, sensations within the body are approached with a spirit of gentle curiosity, and possible verbal labels are checked against felt senses. Where successful, this can improve internal understanding and allow split off trauma or conflict between <a href=\"https://www.lesswrong.com/tag/subagents\">subagents</a> to be processed for improved <a href=\"https://www.lesswrong.com/tag/internal-alignment-human\">internal alignment</a>.</p>\n<p>Focusing draws in name from the <a href=\"https://www.amazon.com/Focusing-Eugene-T-Gendlin/dp/0553278339\">book and technique</a> of the same name by psychologist Eugene Gendlin (Who's also known on LessWrong for the <a href=\"https://www.lesswrong.com/tag/litany-of-gendlin\">Litany of Gendlin</a>), and since his introduction, some have developed their own variations [<a href=\"https://www.lesswrong.com/posts/PXqQhYEdbdAYCp88m/focusing-for-skeptics\">1</a>].</p>\n<p>Related techniques: <a href=\"https://www.lesswrong.com/tag/internal-double-crux\">Internal Double Crux</a>, <a href=\"https://www.lesswrong.com/tag/inner-simulator-suprise-o-meter\">Inner Simulator</a>.</p>\n",
    "description_length": 1448,
    "viewCount": 369,
    "parentTagId": "rationality-techniques"
  },
  {
    "core-tag": "Rationality",
    "_id": "eamWQNQ2dPYWEwhqr",
    "name": "Goal Factoring",
    "slug": "goal-factoring",
    "postCount": 17,
    "description_html": "<html><head></head><body><p><strong>Goal Factoring </strong>is a rationality technique for planning which proceeds by first identifying the underlying goals motivating one or more behaviors and then searching for alternative sets of behaviors that better accomplish the goals.&nbsp;</p><p>For example, someone might refactor the two behaviors of <i>going to the gym </i>and <i>browsing Facebook </i>into the single behavior of <i>play tennis with my friend </i>which more efficiently and effectively accomplishes the underlying goals of <i>have social interaction </i>and <i>get exercise</i>.</p></body></html>",
    "description_length": 610,
    "viewCount": 410,
    "parentTagId": "rationality-techniques"
  },
  {
    "core-tag": "Rationality",
    "_id": "69L5E2XPqdMF2B3gw",
    "name": "Internal Double Crux",
    "slug": "internal-double-crux",
    "postCount": 13,
    "description_html": "<p><strong>Internal Double Crux (IDC) </strong>is a tool for resolving conflict in one's mind. It's a script for focusing on two conflicting inner voices and holding space for them to debate and compromise. A sort of internal couples therapy, if you will. It's built on the notion of <a href=\"https://www.lesswrong.com/tag/double-crux\">double crux</a>, which is a tool for resolving disagreements between two people, but in this situation applied to the inside of one's own mind as though it is made up of many smaller people. IDC was developed by staff at the Center for Applied Rationality.</p><p>For those who find formal instructions helpful, these are the four instructions that CFAR staff give to attendees.</p><ol><li>Find an internal disagreement</li><li>Operationalize the disagreement</li><li>Seek double cruxes</li><li>Resonate</li></ol><p>The current best writeups are in <a href=\"https://www.lesswrong.com/posts/Z9cbwuevS9cqaR96h/cfar-participant-handbook-now-available-to-all\">the CFAR Handbook</a> and in Alkjash's <a href=\"https://www.lesswrong.com/posts/mQmx4kQQtHeBip9ZC/internal-double-crux\">Hammertime sequence</a>.</p><p>For a similar technique that IDC is arguably a special case of, see <a href=\"https://www.lesswrong.com/tag/internal-family-systems\">Internal Family Systems</a>.</p>",
    "description_length": 1306,
    "viewCount": 320,
    "parentTagId": "rationality-techniques"
  },
  {
    "core-tag": "Rationality",
    "_id": "BhrpjXqGuke5GnF6g",
    "name": "Hamming Questions",
    "slug": "hamming-questions",
    "postCount": 27,
    "description_html": null,
    "description_length": null,
    "viewCount": 453,
    "parentTagId": "rationality-techniques"
  },
  {
    "core-tag": "Rationality",
    "_id": "pJthrJDMw54JFGise",
    "name": "Murphyjitsu",
    "slug": "murphyjitsu",
    "postCount": 13,
    "description_html": "<blockquote> In the course of making plans, Murphyjitsu is the practice of strengthening plans by repeatedly envisioning and defending against failure modes until you would be <em>shocked</em> to see it fail. Here’s the basic setup of Murphyjitsu:</blockquote><blockquote>1. Make a plan.</blockquote><blockquote>2. Imagine that you’ve passed the deadline and find out that the plan failed.</blockquote><blockquote>3. If you’re <em>shocked</em> in this scenario, you’re done.</blockquote><blockquote>4. Otherwise, simulate the most likely failure mode, defend against it, and repeat.</blockquote><p>-alkjash, <a href=\"https://www.lesswrong.com/posts/N47M3JiHveHfwdbFg/hammertime-day-10-murphyjitsu\">Hammertime Day 10: Murphyjitsu</a></p><h1>See also</h1><ul><li><a href=\"https://www.lesswrong.com/tag/inner-simulator-suprise-o-meter\">Inner Simulator / Surprise-o-meter</a></li></ul>",
    "description_length": 881,
    "viewCount": 412,
    "parentTagId": "rationality-techniques"
  },
  {
    "core-tag": "Rationality",
    "_id": "5gcpKG2XEAZGj5DEf",
    "name": "Noticing",
    "slug": "noticing",
    "postCount": 34,
    "description_html": "<html><head></head><body><p>See also: <a href=\"https://www.lesswrong.com/tag/introspection\">Introspection</a></p></body></html>",
    "description_length": 127,
    "viewCount": 115,
    "parentTagId": "rationality-techniques"
  },
  {
    "core-tag": "Rationality",
    "_id": "2wjPMY34by2gXEXA2",
    "name": "Techniques",
    "slug": "techniques",
    "postCount": 123,
    "description_html": "<html><head></head><body><p>A <strong>technique </strong>or <strong>rationality technique </strong>is a set of actions (including \"mental actions\") for improving one's thinking so as to form accurate beliefs and/or make better decisions. Ideally, techniques are refined to the point that they can be taught and trained.</p></body></html>",
    "description_length": 337,
    "viewCount": 152,
    "parentTagId": "rationality-techniques"
  },
  {
    "core-tag": "Rationality",
    "_id": "CD6DGZJD4ningyzWF",
    "name": "Trigger-Action Planning",
    "slug": "trigger-action-planning",
    "postCount": 33,
    "description_html": "<p><strong>Trigger-Action Planning (</strong>TAP), sometimes <strong>Trigger-Action Patterns</strong>, and formerly <strong>Implementation Intentions </strong>are techniques for getting oneself to successfully enact desired actions (or inactions) by training something like a \"stimulus-response\" pair. The technique was spread by CFAR which initially drew upon the psychology literature of Implementation Intentions.&nbsp;</p><p>After it was clear that TAPs should be heavily applied to cognitive motions/thought patterns, some decided that the 'P' should stand for 'Pattern' rather than 'Plan'.</p><h2><strong>Resources</strong></h2><ul><li>The CFAR Participant Handbook [1] contains a chapter on TAPs.</li><li>Brienne Yudkowsky's writings on the topic of Noticing, found mostly at her blog [<a href=\"https://agentyduck.blogspot.com/search?q=noticing\">1</a>], are particularly good material related to training TAPs.</li></ul><h2><strong>Related Tags</strong></h2><ul><li><a href=\"https://www.lesswrong.com/tag/deliberate-practice\">Deliberate Practice</a></li></ul>",
    "description_length": 1066,
    "viewCount": 190,
    "parentTagId": "rationality-techniques"
  },
  {
    "core-tag": "AI",
    "_id": "TiEFKWDvD3jsKumDx",
    "name": "AIXI",
    "slug": "aixi",
    "postCount": 44,
    "description_html": "<p><strong>AIXI</strong> is a mathematical formalism for a hypothetical <a href=\"https://www.lesswrong.com/tag/superintelligence\">(super)intelligence</a>, developed by Marcus Hutter (2005, 2007). AIXI is not computable, and so does not serve as a design for a real-world AI, but is considered a valuable theoretical illustration with both positive and negative aspects (things AIXI would be able to do and things it arguably couldn't do).</p><p><i>See also: </i><a href=\"https://www.lesswrong.com/tag/solomonoff-induction\">Solomonoff induction</a>, <a href=\"https://www.lesswrong.com/tag/decision-theory\">Decision theory</a>, <a href=\"https://www.lesswrong.com/ai\">AI</a></p><p>The AIXI formalism says roughly to consider all possible computable models of the environment, Bayes-update them on past experiences, and use the resulting updated predictions to model the expected sensory reward of all possible strategies. This is an application of <a href=\"https://www.lesswrong.com/tag/solomonoff-induction?useTagName=true\">Solomonoff Induction</a>.</p><p>AIXI can be viewed as the border between AI problems that would be 'simple' to solve using unlimited computing power and problems which are structurally 'complicated'.</p><h2><strong>How AIXI works</strong></h2><p>Hutter (<a href=\"http://www.hutter1.net/ai/aixigentle.htm\">2007</a>) describes AIXI as a combination of decision theory and algorithmic information theory: \"Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff’s theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameterless theory of universal Artificial Intelligence.\"</p><p>AIXI operates within the following agent model: There is an <i>agent</i>, and an <i>environment</i>, which is a computable function unknown to the agent. Thus the agent will need to have a probability distribution on the range of possible environments.</p><p>On each clock tick, the agent receives an <i>observation</i> (a bitstring/number) from the environment, as well as a reward (another number).</p><p>The agent then outputs an <i>action</i> (another number).</p><p>To do this, AIXI guesses at a probability distribution for its environment, using <a href=\"https://www.lesswrong.com/tag/solomonoff-induction\">Solomonoff induction</a>, a formalization of <a href=\"https://www.lesswrong.com/tag/occam-s-razor\">Occam's razor</a>: Simpler computations are more likely <i>a priori</i> to describe the environment than more complex ones. This probability distribution is then Bayes-updated by how well each model fits the evidence (or more precisely, by throwing out all computations which have not exactly fit the environmental data so far, but for technical reasons this is roughly equivalent as a model). AIXI then calculates the expected reward of each action it might choose--weighting the likelihood of possible environments as mentioned. It chooses the best action by extrapolating its actions into its future time horizon recursively, using the assumption that at each step into the future it will again choose the best possible action using the same procedure.</p><p>Then, on each iteration, the environment provides an observation and reward as a function of the full history of the interaction; the agent likewise is choosing its action as a function of the full history.</p><p>The agent's intelligence is defined by its expected reward across all environments, weighting their likelihood by their complexity.</p><p>AIXI is not a feasible AI, because <a href=\"https://www.lesswrong.com/tag/solomonoff-induction\">Solomonoff induction</a> is not computable, and because some environments may not interact over finite time horizons (AIXI only works over some finite time horizon, though any finite horizon can be chosen). A somewhat more computable variant is the time-space-bounded AIXItl. Real AI algorithms explicitly inspired by AIXItl, e.g. the Monte Carlo approximation by Veness et al. (2011) have shown interesting results in simple general-intelligence test problems.</p><p>For a short (half-page) technical introduction to AIXI, see <a href=\"https://web.archive.org/web/20160425092747/http://www.jair.org/media/3125/live-3125-5397-jair.pdf\">Veness et al. 2011</a>, page 1-2. For a full exposition of AIXI, see <a href=\"http://www.hutter1.net/ai/aixigentle.htm\">Hutter 2007</a>.</p><h2><strong>Relevance to Friendly AI</strong></h2><p>Because it abstracts optimization power away from human mental features, AIXI is valuable in considering the possibilities for future artificial general intelligence - a compact and non-anthropomorphic specification that is technically complete and closed; either some feature of AIXI follows from the equations or it does not. In particular, it acts as a constructive demonstration of an AGI which does not have human-like <a href=\"https://www.lesswrong.com/tag/terminal-value\">terminal values</a> and will act solely to maximize its reward function. (Yampolskiy &amp; Fox 2012).</p><p>AIXI has limitations as a model for future AGI, for example, the <a href=\"https://www.lesswrong.com/tag/anvil-problem\">Anvil problem</a>: AIXI lacks a self-model. It extrapolates its own actions into the future indefinitely, on the assumption that it will keep working in the same way in the future. Though AIXI is an abstraction, any real AI would have a physical embodiment that could be damaged, and an implementation which could change its behavior due to bugs; and the AIXI formalism completely ignores these possibilities.</p><h2><strong>References</strong></h2><ul><li><a href=\"https://intelligence.org/files/AGI-HMM.pdf\">R.V. Yampolskiy, J. Fox (2012) Artificial General Intelligence and the Human Mental Model. In Amnon H. Eden, Johnny Søraker, James H. Moor, Eric Steinhart (Eds.), The Singularity Hypothesis.The Frontiers Collection. London: Springer.</a></li><li><a href=\"http://www.hutter1.net/ai/aixigentle.htm\">M. Hutter (2007) Universal Algorithmic Intelligence: A mathematical top-&gt;down approach</a>. In Goertzel &amp; Pennachin (eds.), Artificial General Intelligence, 227-287. Berlin: Springer.</li><li>M. Hutter, (2005) Universal Artificial Intelligence: Sequential decisions based on algorithmic probability. Berlin: Springer.</li><li><a href=\"http://www.jair.org/media/3125/live-3125-5397-jair.pdf\">J. Veness, K.S. Ng, M. Hutter, W. Uther and D. Silver (2011) A Monte-Carlo AIXI Approximation</a>, <i>Journal of Artiﬁcial Intelligence Research</i> 40, 95-142]</li></ul><h2><strong>Blog posts</strong></h2><ul><li><a href=\"https://www.lesswrong.com/lw/8qy/aixi_and_existential_despair/\">AIXI and Existential Despair</a> by <a href=\"https://www.lesswrong.com/users/paulfchristiano\">paulfchristiano</a></li><li><a href=\"https://www.lesswrong.com/r/discussion/lw/az7/video_paul_christianos_impromptu_tutorial_on_aixi/\">[video] Paul Christiano's impromptu tutorial on AIXI and TDT</a></li></ul><h2><strong>See also</strong></h2><ul><li><a href=\"https://www.lesswrong.com/tag/solomonoff-induction\">Solomonoff induction</a></li><li><a href=\"https://www.lesswrong.com/tag/decision-theory\">Decision theory</a></li></ul>",
    "description_length": 7272,
    "viewCount": 630,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "W6QZYSNt5FgWgvbdT",
    "name": "Coherent Extrapolated Volition",
    "slug": "coherent-extrapolated-volition",
    "postCount": 63,
    "description_html": "<p><strong>Coherent Extrapolated Volition</strong> was a term developed by <a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky</a> while discussing <a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a> development. It’s meant as an argument that it would not be sufficient to explicitly program what we think our desires and motivations are into an AI, instead, we should find a way to program it in a way that it would act in our best interests – what we <i>want</i> it to do and not what we <i>tell</i> it to.</p><p><i>Related</i>:<a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"> Friendly AI</a>, <a href=\"https://www.lesswrong.com/tag/metaethics-sequence\">Metaethics Sequence</a>, <a href=\"https://www.lesswrong.com/tag/complexity-of-value\">Complexity of Value</a></p><blockquote><p><i>In calculating CEV, an AI would predict what an idealized version of us would want, \"if we knew more, thought faster, were more the people we wished we were, had grown up farther together\". It would recursively iterate this prediction for humanity as a whole, and determine the desires which converge. This initial dynamic would be used to generate the AI's utility function.</i>&nbsp;</p></blockquote><p>Often CEV is used generally to refer to what the idealized version of a person would want, separate from the context of building aligned AI's.</p><h2>What is volition?</h2><p>As an example of the classical concept of volition, the author develops a simple thought experiment: imagine you’re facing two boxes, A and B. One of these boxes, and only one, has a diamond in it – box B. You are now asked to make a guess, whether to choose box A or B, and you chose to open box A. It was your <i>decision</i> to take box A, but your <i>volition</i> was to choose box B, since you wanted the diamond in the first place.</p><p>Now imagine someone else – Fred – is faced with the same task and you want to help him in his decision by giving the box he chose, box A. Since you know where the diamond is, simply handing him the box isn’t helping. As such, you mentally extrapolate a volition for Fred, based on a version of him that knows where the diamond is, and imagine he actually wants box B.</p><h2>Coherent Extrapolated Volition</h2><blockquote><p>\"The \"Coherent\" in \"Coherent Extrapolated Volition\" does not indicate the idea that an extrapolated volition is necessarily coherent. The \"Coherent\" part indicates the idea that if you build an FAI and run it on an extrapolated human, the FAI should only <i>act on the coherent parts</i>. Where there are multiple attractors, the FAI should hold satisficing avenues open, not try to decide itself.\" - <a href=\"https://www.lesswrong.com/posts/rm8tv9qZ9nwQxhshx/you-provably-can-t-trust-yourself?commentId=8C59BDhfe5cxhcqpG\">Eliezer Yudkowsky</a></p></blockquote><p>In developing friendly AI, one acting for our best interests, we would have to take care that it would have implemented, from the beginning, a <i>coherent extrapolated volition of humankind</i>. In calculating CEV, an AI would predict what an idealized version of us would want, \"if we knew more, thought faster, were more the people we wished we were, had grown up farther together\". It would recursively iterate this prediction for humanity as a whole, and determine the desires which converge. This initial dynamic would be used to generate the AI's utility function.</p><p>The main problems with CEV include, firstly, the great difficulty of implementing such a program - “If one attempted to write an ordinary computer program using ordinary computer programming skills, the task would be a thousand lightyears beyond hopeless.” Secondly, the possibility that human values may not converge. Yudkowsky considered CEV obsolete almost immediately after its publication in 2004. He states that there's a \"principled distinction between discussing CEV as an initial dynamic of Friendliness, and discussing CEV as a Nice Place to Live\" and his essay was essentially conflating the two definitions.</p><h2>Further Reading &amp; References</h2><ul><li><a href=\"http://intelligence.org/files/CEV.pdf\">Coherent Extrapolated Volition</a> by Eliezer Yudkowsky (2004)</li><li><a href=\"http://intelligence.org/files/CEV-MachineEthics.pdf\">Coherent Extrapolated Volition: A Meta-Level Approach to Machine Ethics</a> by Nick Tarleton (2010)</li><li><a href=\"https://web.archive.org/web/20131231151554/http://www.acceleratingfuture.com/michael/blog/2009/12/a-short-introduction-to-coherent-extrapolated-volition-cev/\">A Short Introduction to Coherent Extrapolated Volition</a> by Michael Anissimov</li><li><a href=\"https://www.lesswrong.com/lw/2b7/hacking_the_cev_for_fun_and_profit/\">Hacking the CEV for Fun and Profit</a> by Wei Dai</li><li><a href=\"https://www.lesswrong.com/lw/3fn/two_questions_about_cev_that_worry_me/\">Two questions about CEV that worry me</a> by Vladimir Slepnev</li><li><a href=\"https://www.lesswrong.com/lw/5l0/beginning_resources_for_cev_research/\">Beginning resources for CEV research</a> by Luke Muehlhauser</li><li><a href=\"https://www.lesswrong.com/lw/7sb/cognitive_neuroscience_arrows_impossibility/\">Cognitive Neuroscience, Arrow's Impossibility Theorem, and Coherent Extrapolated Volition</a> by Luke Muehlhauser</li><li><a href=\"https://www.lesswrong.com/lw/8iy/objections_to_coherent_extrapolated_volition/\">Objections to Coherent Extrapolated Volition</a> by Alexander Kruel</li></ul><h2>See also</h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a></li><li><a href=\"https://www.lesswrong.com/tag/metaethics-sequence\">Metaethics sequence</a></li><li><a href=\"https://www.lesswrong.com/tag/complexity-of-value\">Complexity of value</a></li><li><a href=\"https://www.lesswrong.com/tag/coherent-aggregated-volition\">Coherent Aggregated Volition</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Roko's_basilisk\">Roko's basilisk</a></li></ul>",
    "description_length": 5941,
    "viewCount": 1675,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "R6uagTfhhBeejGrrf",
    "name": "Complexity of Value",
    "slug": "complexity-of-value",
    "postCount": 91,
    "description_html": "<p><strong>Complexity of value</strong> is the thesis that human values have high <a href=\"https://wiki.lesswrong.com/wiki/Kolmogorov_complexity\">Kolmogorov complexity</a>; that our <a href=\"https://wiki.lesswrong.com/wiki/preferences\">preferences</a>, the things we care about, cannot be summed by a few simple rules, or compressed. <strong><a href=\"https://www.lesswrong.com/lw/y3/value_is_fragile/\">Fragility of value</a></strong> is the thesis that losing even a small part of the rules that make up our values could lead to results that most of us would now consider as unacceptable (just like dialing nine out of ten phone digits correctly does not connect you to a person 90% similar to your friend). For example, all of our values <em>except</em> novelty might yield a future full of individuals replaying only one optimal experience through all eternity.</p><p>Related: <a href=\"https://www.lesswrong.com/tag/metaethics\">Ethics &amp; Metaethics</a>, <a href=\"https://www.lesswrong.com/tag/fun-theory\">Fun Theory</a>, <a href=\"https://www.lesswrong.com/tag/preference\">Preference</a>, <a href=\"https://www.lesswrong.com/tag/wireheading\">Wireheading</a></p><p>Many human choices can be compressed, by representing them by simple rules - the desire to survive produces innumerable actions and subgoals as we fulfill that desire. But people don't <em>just</em> want to survive - although you can compress many human activities to that desire, you cannot compress all of human existence into it. The human equivalents of a utility function, our terminal values, contain many different elements that are not strictly reducible to one another. William Frankena offered <a href=\"http://plato.stanford.edu/entries/value-intrinsic-extrinsic/#WhaHasIntVal\">this list</a> of things which many cultures and people seem to value (for their own sake rather than strictly for their external consequences):</p><blockquote>Life, consciousness, and activity; health and strength; pleasures and satisfactions of all or certain kinds; happiness, beatitude, contentment, etc.; truth; knowledge and true opinions of various kinds, understanding, wisdom; beauty, harmony, proportion in objects contemplated; aesthetic experience; morally good dispositions or virtues; mutual affection, love, friendship, cooperation; just distribution of goods and evils; harmony and proportion in one's own life; power and experiences of achievement; self-expression; freedom; peace, security; adventure and novelty; and good reputation, honor, esteem, etc.</blockquote><p>The \"etc.\" at the end is the tricky part, because there may be a great many values not included on this list.</p><p>One hypothesis is that natural selection reifies selection pressures as <a href=\"https://www.lesswrong.com/tag/adaptation-executors\">psychological drives, which then continue to execute</a> <a href=\"https://www.lesswrong.com/lw/yi/the_evolutionarycognitive_boundary/\">independently of any consequentialist reasoning in the organism</a>. This may also continue without that organism explicitly representing, let alone caring about, the original evolutionary context. Under this view, we have no reason to expect these terminal values to be reducible to any one thing, or each other.</p><p>Taken in conjunction with another LessWrong claim, that all values are morally relevant, this would suggest that those philosophers who seek to do so are mistaken in trying to find cognitively tractable overarching principles of ethics. However, it is coherent to suppose that not all values are morally relevant, and that the morally relevant ones form a tractable subset.</p><p>Complexity of value also runs into underappreciation in the presence of bad <a href=\"https://www.lesswrong.com/tag/metaethics\">metaethics</a>. The local flavor of metaethics could be characterized as cognitivist, without implying \"thick\" notions of instrumental rationality; in other words, moral discourse can be about a coherent subject matter, without all possible minds and agents necessarily finding truths about that subject matter to be psychologically compelling. An <a href=\"https://www.lesswrong.com/tag/paperclip-maximizer\">expected paperclip maximizer</a> doesn't disagree with you about morality any more than you disagree with it about \"which action leads to the greatest number of expected paperclips\", it is just constructed to find the latter subject matter psychologically compelling but not the former. Failure to appreciate that \"But it's just paperclips! What a dumb goal! No sufficiently intelligent agent would pick such a dumb goal!\" is a judgment carried out on a local brain that evaluates paperclips as inherently low-in-the-preference-ordering means that someone will expect all moral judgments to be automatically reproduced in a sufficiently intelligent agent, since, after all, they would not lack the intelligence to see that paperclips are so obviously inherently-low-in-the-preference-ordering. This is a particularly subtle species of <a href=\"https://www.lesswrong.com/tag/anthropomorphism\">anthropomorphism</a> and <a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy\">mind projection fallacy</a>.</p><p>Because the human brain very often fails to grasp all these difficulties involving our values, we tend to think building an awesome future is much less problematic than it really is. Fragility of value is relevant for building <a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a>, because an <a href=\"https://wiki.lesswrong.com/wiki/AGI\">AGI</a> which does not respect human values is likely to create a world that we would consider devoid of value - not necessarily full of explicit attempts to be evil, but perhaps just a dull, boring loss.</p><p>As values are orthogonal with intelligence, they can freely vary no matter how intelligent and efficient an AGI is [<a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">1</a>]. Since human / humane values have high Kolmogorov complexity, a random AGI is highly unlikely to maximize human / humane values. The fragility of value thesis implies that a poorly constructed AGI might e.g. turn us into blobs of perpetual orgasm. Because of this relevance the complexity and fragility of value is a major theme of <a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky</a>'s writings.</p><p>Wrongly designing the future because we wrongly encoded human values is a serious and difficult to assess type of <a href=\"https://www.lesswrong.com/tag/existential-risk\">Existential risk</a>. \"Touch too hard in the wrong dimension, and the physical representation of those values will shatter - <em>and not come back, for there will be nothing left to want to bring it back</em>. And the referent of those values - a worthwhile universe - would no longer have any physical reason to come into being. Let go of the steering wheel, and the Future crashes.\" [<a href=\"https://www.lesswrong.com/lw/y3/value_is_fragile/\">2</a>]</p><h2>Complexity of Value and AI</h2><p>Complexity of value poses a problem for <a href=\"http://lesswrong.com/tag/ai\">AI alignment</a>. If you can't easily compress what humans want into a simple function that can be fed into a computer, it isn't easy to make a powerful AI that does things humans want and doesn't do things humans don't want. <a href=\"https://www.lesswrong.com/tag/value-learning\">Value Learning</a> attempts to address this problem.</p><h2>Major posts</h2><ul><li><a href=\"https://www.lesswrong.com/lw/xy/the_fun_theory_sequence/\">The Fun Theory Sequence</a> describes some of the many complex considerations that determine <em>what sort of happiness</em> we most prefer to have - given that many of us would decline to just have an electrode planted in our pleasure centers.</li><li><a href=\"https://www.lesswrong.com/lw/l3/thou_art_godshatter/\">Thou Art Godshatter</a> describes the <a href=\"https://www.lesswrong.com/tag/evolutionary-psychology\">evolutionary psychology</a> behind the complexity of human values - how they got to be complex, and why, given that origin, there is no reason in hindsight to expect them to be simple. We certainly are not built to <a href=\"https://wiki.lesswrong.com/wiki/adaptation_executers\">maximize genetic fitness</a>.</li><li><a href=\"https://www.lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/\">Not for the Sake of Happiness (Alone)</a> tackles the <a href=\"https://www.lesswrong.com/tag/hollywood-rationality\">Hollywood Rationality</a> trope that \"rational\" preferences must reduce to selfish hedonism - caring strictly about personally experienced pleasure. An ideal Bayesian agent - implementing strict Bayesian decision theory - can have a utility function that <a href=\"https://www.lesswrong.com/lw/l4/terminal_values_and_instrumental_values/\">ranges over anything, not just internal subjective experiences</a>.</li><li><a href=\"https://www.lesswrong.com/lw/lq/fake_utility_functions/\">Fake Utility Functions</a> describes the seeming fascination that many have with trying to compress morality down to a single principle. The <a href=\"https://www.lesswrong.com/lw/lp/fake_fake_utility_functions/\">sequence leading up</a> to this post tries to explain the cognitive twists whereby people <a href=\"https://www.lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/\">smuggle</a> all of their complicated <em>other</em> preferences into their choice of <em>exactly</em> which acts they try to <em><a href=\"https://www.lesswrong.com/lw/kq/fake_justification/\">justify using</a></em> their single principle; but if they were <em>really</em> following <em>only</em> that single principle, they would <a href=\"https://www.lesswrong.com/lw/kz/fake_optimization_criteria/\">choose other acts to justify</a>.</li></ul><h2>Other posts</h2><ul><li><a href=\"https://www.lesswrong.com/lw/y3/value_is_fragile/\">Value is Fragile</a></li><li><a href=\"https://www.lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/\">The Hidden Complexity of Wishes</a></li><li><a href=\"https://www.lesswrong.com/lw/ky/fake_morality/\">Fake Morality</a></li><li><a href=\"https://www.lesswrong.com/lw/1o9/welcome_to_heaven/\">Welcome to Heaven</a></li><li><a href=\"https://www.lesswrong.com/lw/1oj/complexity_of_value_complexity_of_outcome/\">Complexity of Value ≠ Complexity of Outcome</a></li><li><a href=\"https://www.lesswrong.com/lw/65w/not_for_the_sake_of_pleasure_alone/\">Not for the Sake of Pleasure Alone</a></li><li><a href=\"https://casparoesterheld.com/2017/02/10/a-non-comprehensive-list-of-human-values/\">A Non-Comprehensive List of Human Values</a></li></ul><h2>See also</h2><ul><li><a href=\"http://intelligence.org/files/ComplexValues.pdf\">Complex Value Systems are Required to Realize Valuable Futures</a></li><li><a href=\"https://www.lesswrong.com/tag/human-universal\">Human universal</a></li><li><a href=\"https://www.lesswrong.com/tag/fake-simplicity\">Fake simplicity</a></li><li><a href=\"https://www.lesswrong.com/tag/metaethics-sequence\">Metaethics sequence</a></li><li><a href=\"https://www.lesswrong.com/tag/fun-theory\">Fun theory</a></li><li><a href=\"https://www.lesswrong.com/tag/magical-categories\">Magical categories</a></li><li><a href=\"https://www.lesswrong.com/tag/friendly-artificial-intelligence\">Friendly Artificial Intelligence</a></li><li><a href=\"https://www.lesswrong.com/tag/preference\">Preference</a></li><li><a href=\"https://www.lesswrong.com/tag/wireheading\">Wireheading</a></li><li><a href=\"https://www.lesswrong.com/tag/the-utility-function-is-not-up-for-grabs\">The utility function is not up for grabs</a></li></ul>",
    "description_length": 11571,
    "viewCount": 347,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "b8nerwC3Dp2Q9MvbG",
    "name": "Corrigibility",
    "slug": "corrigibility",
    "postCount": 152,
    "description_html": "<p>A&nbsp;<a href=\"https://arbital.greaterwrong.com/p/corrigibility/\"><u>'corrigible'&nbsp;</u></a>agent is one that<a href=\"https://arbital.greaterwrong.com/p/nonadversarial/\"><u> doesn't interfere</u></a> with what<a href=\"https://arbital.greaterwrong.com/p/value_alignment_programmer/\"><u> we</u></a> would intuitively see as attempts to 'correct' the agent, or 'correct' our mistakes in building it; and permits these 'corrections' despite the apparent<a href=\"https://arbital.greaterwrong.com/p/instrumental_convergence/\"><u> instrumentally convergent reasoning</u></a> saying otherwise.</p><ul><li>If we try to suspend the AI to disk, or shut it down entirely, a corrigible AI will let us do so. This is not something that an AI is automatically incentivized to let us do, since if it is shut down,<a href=\"https://arbital.greaterwrong.com/p/no_coffee_if_dead/\"><u> it will be unable to fulfill what would usually be its goals</u></a>.</li><li>If we try to reprogram the AI, a corrigible AI will not resist this change and will allow this modification to go through. If this is not specifically incentivized, an AI might attempt to fool us into believing the utility function was modified successfully, while actually keeping its original utility function as<a href=\"https://arbital.greaterwrong.com/p/cognitive_steganography/\"><u> obscured</u></a> functionality. By default, this deception could be a<a href=\"https://arbital.greaterwrong.com/p/preference_stability/\"><u> preferred outcome according to the AI's current preferences</u></a>.</li></ul><p>Corrigibility is also used in a broader sense, something like a helpful agent. <a href=\"https://ai-alignment.com/corrigibility-3039e668638\">Paul Christiano has defined corrigibility</a> as an agent that will help me:</p><blockquote><ul><li>Figure out whether I built the right AI and correct any mistakes I made</li><li>Remain informed about the AI’s behavior and avoid unpleasant surprises</li><li>Make better decisions and clarify my preferences</li><li>Acquire resources and remain in effective control of them</li><li>Ensure that my AI systems continue to do all of these nice things</li><li>…and so on</li></ul></blockquote><p>See also:</p><ul><li><a href=\"https://www.youtube.com/watch?v=3TYT1QfdfsM\"><u>AI \"Stop Button\" Problem - Computerphile</u></a></li></ul>",
    "description_length": 2327,
    "viewCount": 224,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "KEAWfxwjitNJFrC68",
    "name": "Deceptive Alignment",
    "slug": "deceptive-alignment",
    "postCount": 151,
    "description_html": "<p><strong>Deceptive Alignment</strong> is when an <a href=\"https://lesswrong.com/tag/ai\">AI</a> which is not actually aligned temporarily acts aligned in order to <a href=\"https://lesswrong.com/tag/deception\">deceive</a> its creators or its training process. It presumably does this to <a href=\"https://lesswrong.com/tag/instrumental-convergence\">avoid being shut down</a> or retrained and to gain access to the power that the creators would give an aligned AI. (The term <i>scheming</i> is sometimes used for this phenomenon.)</p><p>See also: <a href=\"https://lesswrong.com/tag/mesa-optimization\">Mesa-optimization</a>, <a href=\"https://lesswrong.com/tag/treacherous-turn\">Treacherous Turn</a>, <a href=\"https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk\">Eliciting Latent Knowledge</a>, <a href=\"https://www.lesswrong.com/tag/deception\">Deception</a></p>",
    "description_length": 867,
    "viewCount": 310,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "dPPATLhRmhdJtJM2t",
    "name": "Decision Theory",
    "slug": "decision-theory",
    "postCount": 456,
    "description_html": "<p><strong>Decision theory</strong> is the study of principles and algorithms for making correct decisions—that is, decisions that allow an agent to achieve better outcomes with respect to its goals. Every action at least implicitly represents a decision under uncertainty: in a state of partial knowledge, something has to be done, even if that something turns out to be nothing (call it \"the null action\"). Even if you don't know how you make decisions, decisions do get made, and so there has to be some underlying mechanism. What is it? And how can it be done better? Decision theory has the answers.</p>\n<p><em>Note: this page needs to be updated with content regarding Functional Decision Theory, the latest theory from MIRI.</em></p>\n<p><em>Related:</em> <a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory</a>, <a href=\"https://www.lesswrong.com/tag/robust-agents?showPostCount=true&amp;useTagName=true\">Robust Agents</a>, <a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions</a></p>\n<p>A core idea in decision theory is that of <a href=\"https://lesswrong.com/tag/expected-utility\"><em>expected utility</em></a> <em>maximization</em>, usually intractable to directly calculate in practice, but an invaluable theoretical concept. An agent assigns utility to every possible outcome: a real number representing the goodness or desirability of that outcome. The mapping of outcomes to utilities is called the agent's <em>utility function</em>. (The utility function is said to be invariant under affine transformations: that is, the utilities can be scaled or translated by a constant while resulting in all the same decisions.) For every action that the agent could take, sum over the utilities of the various possible outcomes weighted by their probability: this is the <a href=\"https://lesswrong.com/tag/expected-value\">expected</a> utility of the action, and the action with the highest expected utility is to be chosen.</p>\n<h2>Thought experiments</h2>\n<p>The limitations and pathologies of decision theories can be analyzed by considering the decisions they suggest in the certain idealized situations that stretch the limits of decision theory's applicability. Some of the thought experiments more frequently discussed on <a href=\"https://wiki.lesswrong.com/wiki/LW\">LW</a> include:</p>\n<ul>\n<li><a href=\"https://lesswrong.com/tag/newcomb-s-problem\">Newcomb's problem</a></li>\n<li><a href=\"https://lesswrong.com/tag/counterfactual-mugging\">Counterfactual mugging</a></li>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker\">Parfit's hitchhiker</a></li>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Smoker's_lesion\">Smoker's lesion</a></li>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Absentminded_driver\">Absentminded driver</a></li>\n<li><a href=\"https://lesswrong.com/tag/sleeping-beauty-paradox\">Sleeping Beauty problem</a></li>\n<li><a href=\"https://lesswrong.com/tag/prisoner-s-dilemma\">Prisoner's dilemma</a></li>\n<li><a href=\"https://lesswrong.com/tag/pascal-s-mugging\">Pascal's mugging</a></li>\n</ul>\n<h2>Commonly discussed decision theories</h2>\n<p>Standard theories well-known in academia:</p>\n<ul>\n<li>CDT, <a href=\"http://en.wikipedia.org/wiki/Causal_decision_theory\">Causal Decision Theory</a></li>\n<li>EDT, <a href=\"http://en.wikipedia.org/wiki/Evidential_decision_theory\">Evidential Decision Theory</a></li>\n</ul>\n<p>Theories invented by researchers associated with <a href=\"https://wiki.lesswrong.com/wiki/MIRI\">MIRI</a> and LW:</p>\n<ul>\n<li>FDT: <a href=\"https://intelligence.org/2017/10/22/fdt/\">Functional Decision Theory</a></li>\n<li>TDT, <a href=\"https://lesswrong.com/tag/timeless-decision-theory\">Timeless Decision Theory</a></li>\n<li>UDT, <a href=\"https://lesswrong.com/tag/updateless-decision-theory\">Updateless Decision Theory</a></li>\n<li>ADT: <a href=\"https://lesswrong.com/tag/ambient-decision-theory\">Ambient Decision Theory</a> (a variant of UDT)</li>\n<li>FDT: <a href=\"https://intelligence.org/files/DeathInDamascus.pdf\">Cheating Death in Damascus</a></li>\n</ul>\n<p>Other decision theories are listed in <a href=\"https://casparoesterheld.com/a-comprehensive-list-of-decision-theories/\">A comprehensive list of decision theories</a>.</p>\n<h2>Blog posts</h2>\n<ul>\n<li><a href=\"https://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/\">Terminal Values and Instrumental Values</a></li>\n<li><a href=\"https://lesswrong.com/lw/aq9/decision_theories_a_less_wrong_primer/\">Decision Theories: A Less Wrong Primer</a> by orthonormal</li>\n<li><a href=\"https://lesswrong.com/lw/gu1/decision_theory_faq/\">Decision Theory FAQ</a> by lukeprog and crazy88</li>\n</ul>\n<h2>Sequence by <a href=\"https://wiki.lesswrong.com/wiki/AnnaSalamon\">AnnaSalamon</a></h2>\n<ul>\n<li><a href=\"https://lesswrong.com/lw/16f/decision_theory_an_outline_of_some_upcoming_posts/\">Decision theory: An outline of some upcoming posts</a></li>\n<li><a href=\"https://lesswrong.com/lw/16i/confusion_about_newcomb_is_confusion_about/\">Confusion about Newcomb is confusion about counterfactuals</a></li>\n<li><a href=\"https://lesswrong.com/lw/174/decision_theory_why_we_need_to_reduce_could_would/\">Why we need to reduce “could”, “would”, “should”</a></li>\n<li><a href=\"https://lesswrong.com/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/\">Why Pearl helps reduce “could” and “would”, but still leaves us with at least three alternatives</a></li>\n</ul>\n<h2>Sequence by <a href=\"http://lesswrong.com/user/orthonormal/\">orthonormal</a> (Decision Theories: A Semi-Formal Analysis)</h2>\n<ul>\n<li><a href=\"https://lesswrong.com/lw/aq9/decision_theories_a_less_wrong_primer/\">Part 0: Decision Theories: A Less Wrong Primer</a></li>\n<li><a href=\"https://lesswrong.com/lw/axl/decision_theories_a_semiformal_analysis_part_i/\">Part I: The Problem with Naive Decision Theory</a></li>\n<li><a href=\"https://lesswrong.com/lw/az6/decision_theories_a_semiformal_analysis_part_ii/\">Part II: Causal Decision Theory and Substitution</a></li>\n<li><a href=\"https://lesswrong.com/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/\">Part III: Formalizing Timeless Decision Theory</a></li>\n</ul>\n<h2>See also</h2>\n<ul>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Instrumental_rationality\">Instrumental rationality</a></li>\n<li><a href=\"https://lesswrong.com/tag/causality\">Causality</a></li>\n<li><a href=\"https://lesswrong.com/tag/expected-utility\">Expected utility</a></li>\n<li><a href=\"https://lesswrong.com/tag/evidential-decision-theory\">Evidential Decision Theory</a></li>\n<li><a href=\"https://lesswrong.com/tag/timeless-decision-theory\">Timeless decision theory</a>, <a href=\"https://lesswrong.com/tag/updateless-decision-theory\">Updateless decision theory</a></li>\n<li><a href=\"https://lesswrong.com/tag/aixi\">AIXI</a></li>\n</ul>\n",
    "description_length": 6860,
    "viewCount": 965,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "MP8NqPNATMqPrij4n",
    "name": "Embedded Agency",
    "slug": "embedded-agency",
    "postCount": 98,
    "description_html": "<p><strong>Embedded Agency</strong> is the problem that an understanding of the theory of rational agents must account for the fact that the agents we create (and we ourselves) are inside the world or universe we are trying to affect, and not separated from it. This is in contrast with much current basic theory of AI or Rationality (such as Solomonoff induction or Bayesianism) which implicitly supposes a separation between the agent and the-things-the-agent-has-beliefs about. In other words, agents in this universe do not have Cartesian or dualistic boundaries like much of philosophy assumes, and are instead reductionist, that is agents are made up of non-agent parts like bits and atoms.</p>\n<p>Embedded Agency is not a fully formalized research agenda, but Scott Garrabrant and Abram Demski have written the canonical explanation of the idea in their sequence <a href=\"https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh\"><em>Embedded Agency</em></a>. This points to many of the core confusions we have about rational agency and attempts to tie them into a single picture.</p>\n",
    "description_length": 1081,
    "viewCount": 297,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "HJZZxyYXWzB74M4FT",
    "name": "Fixed Point Theorems",
    "slug": "fixed-point-theorems",
    "postCount": 11,
    "description_html": "<p><strong>Fixed Point Theorems</strong> are very general theorems in mathematics that show for a given function&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>&nbsp;and input&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"x\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span></span></span></span></span></span>&nbsp;that&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f(x) = x\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span></span></span></span></span></span>. We say that the input&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"x\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x</span></span></span></span></span></span></span>&nbsp;is a fixed point for the function&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f</span></span></span></span></span></span></span>.</p><p>These come up most commonly on LessWrong in work around <a href=\"https://www.lesswrong.com/tag/embedded-agency\">Embedded Agency</a> research, as well as in discussion of game theory.</p>",
    "description_length": 20001,
    "viewCount": 52,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "PvridmTCj2qsugQCH",
    "name": "Goodhart's Law",
    "slug": "goodhart-s-law",
    "postCount": 115,
    "description_html": "<p><strong>Goodhart's Law</strong> states that when a proxy for some value becomes the target of optimization pressure, the proxy will cease to be a good proxy. One form of Goodhart is demonstrated by the Soviet story of a factory graded on how many shoes they produced (a good proxy for productivity) – they soon began producing a higher number of tiny shoes. Useless, but the numbers look good.</p>\n<p>Goodhart's Law is of particular relevance to <a href=\"https://www.lesswrong.com/tag/ai\">AI Alignment</a>. Suppose you have something which is generally a good proxy for \"the stuff that humans care about\", it would be dangerous to have a powerful AI optimize for the proxy, in accordance with Goodhart's law, the proxy will breakdown.</p>\n<h2>Goodhart Taxonomy</h2>\n<p>In <a href=\"https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy\">Goodhart Taxonomy</a>, Scott Garrabrant identifies four kinds of Goodharting:</p>\n<ul>\n<li>Regressional Goodhart - When selecting for a proxy measure, you select not only for the true goal, but also for the difference between the proxy and the goal.</li>\n<li>Causal Goodhart - When there is a non-causal correlation between the proxy and the goal, intervening on the proxy may fail to intervene on the goal.</li>\n<li>Extremal Goodhart - Worlds in which the proxy takes an extreme value may be very different from the ordinary worlds in which the correlation between the proxy and the goal was observed.</li>\n<li>Adversarial Goodhart - When you optimize for a proxy, you provide an incentive for adversaries to correlate their goal with your proxy, thus destroying the correlation with your goal.</li>\n</ul>\n<h2>See Also</h2>\n<ul>\n<li><a href=\"https://lesswrong.com/tag/groupthink\">Groupthink</a>, <a href=\"https://lesswrong.com/tag/information-cascades\">Information cascade</a>, <a href=\"https://lesswrong.com/tag/affective-death-spiral\">Affective death spiral</a></li>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Adaptation_executers\">Adaptation executers</a>, <a href=\"https://lesswrong.com/tag/superstimuli\">Superstimulus</a></li>\n<li><a href=\"https://lesswrong.com/tag/signaling\">Signaling</a>, <a href=\"https://lesswrong.com/tag/filtered-evidence\">Filtered evidence</a></li>\n<li><a href=\"https://lesswrong.com/tag/cached-thought\">Cached thought</a></li>\n<li><a href=\"https://lesswrong.com/tag/modesty-argument\">Modesty argument</a>, <a href=\"https://lesswrong.com/tag/egalitarianism\">Egalitarianism</a></li>\n<li><a href=\"https://lesswrong.com/tag/rationalization\">Rationalization</a>, <a href=\"https://lesswrong.com/tag/dark-arts\">Dark arts</a></li>\n<li><a href=\"https://lesswrong.com/tag/epistemic-hygiene\">Epistemic hygiene</a></li>\n<li><a href=\"https://lesswrong.com/tag/scoring-rule\">Scoring rule</a></li>\n</ul>\n",
    "description_length": 2775,
    "viewCount": 480,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "b6tJM7Lza74rTfCBF",
    "name": "Goal-Directedness",
    "slug": "goal-directedness",
    "postCount": 90,
    "description_html": "<p><strong>Goal-directedness</strong> is the property of some system to be aiming at some goal. It is in need of formalization, but might prove important in deciding which kind of AI to try to align.</p><p>A goal may be defined as a world-state that an agent tries to achieve. Goal-directed agents may generate internal representations of desired end states, compare them against their internal representation of the current state of the world, and formulate plans for navigating from the latter to the former.</p><p>The goal-generating function may be derived from a pre-programmed lookup table (for simple worlds), from directly inverting the agent's utility function (for simple utility functions), or it may be learned through experience mapping states to rewards and predicting which states will produce the largest rewards. The plan-generating algorithm could range from shortest-path algorithms like A* or Dijkstra's algorithm (for fully-representable world graphs), to policy functions that learn through RL which actions bring the current state closer to the goal state (for simple AI), to some combination or extrapolation (for more advanced AI).</p><p>Implicit goal-directedness may come about in agents that do not have explicit internal representations of goals but that nevertheless learn or enact policies that cause the environment to converge on a certain state or set of states. Such implicit goal-directedness may arise, for instance, in simple reinforcement learning agents, which learn a policy function&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\pi:S\\rightarrow A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;\">π</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\">:</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\">S</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">→</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>&nbsp;that maps states directly to actions.</p>",
    "description_length": 19917,
    "viewCount": 107,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "bbdbpGWWPMfKBzk7z",
    "name": "Gradient Hacking",
    "slug": "gradient-hacking",
    "postCount": 31,
    "description_html": "<p><strong>Gradient Hacking</strong> describes a scenario where a <a href=\"https://lesswrong.com/tag/mesa-optimization\">mesa-optimizer </a>in an <a href=\"https://lesswrong.com/tag/ai\">AI</a> system acts in a way that intentionally manipulates the way that gradient descent updates it, likely to preserve its own mesa-objective in future iterations of the AI.</p><p>See also: <a href=\"https://lesswrong.com/tag/inner-alignment\">Inner Alignment</a></p>",
    "description_length": 450,
    "viewCount": 145,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "wdLqQnzdgiYpDXEWH",
    "name": "Infra-Bayesianism",
    "slug": "infra-bayesianism",
    "postCount": 56,
    "description_html": "<p><strong>Infra-Bayesianism</strong> is a new approach to <a href=\"https://www.lesswrong.com/tag/epistemology\">epistemology</a> / <a href=\"https://www.lesswrong.com/tag/decision-theory\">decision theory</a> / reinforcement learning theory, which builds on \"imprecise probability\" to solve the problem of prior misspecification / grain-of-truth / nonrealizability which plagues <a href=\"https://www.lesswrong.com/tag/bayesianism\">Bayesianism</a> and Bayesian reinforcement learning.</p><p>Infra-Bayesianism also naturally leads to an implementation of <a href=\"https://www.lesswrong.com/tag/updateless-decision-theory\">UDT</a>, and (more speculatively at this stage) has applications to multi-agent theory, <a href=\"https://www.lesswrong.com/tag/embedded-agency\">embedded agency </a>and reflection.</p><p>See the <a href=\"https://www.lesswrong.com/posts/zB4f7QqKhBHa5b37a/introduction-to-the-infra-bayesianism-sequence\">Infra-Bayesianism Sequence</a>.</p>",
    "description_length": 954,
    "viewCount": 318,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "Dw5Z6wtTgk4Fikz9f",
    "name": "Inner Alignment",
    "slug": "inner-alignment",
    "postCount": 286,
    "description_html": "<p>Inner alignment asks the question: How can we robustly aim our AI optimizers at any objective function at all?</p><p>More specifically, Inner Alignment<strong> </strong>is the problem of ensuring <a href=\"https://www.lesswrong.com/tag/mesa-optimization\">mesa-optimizers</a> (i.e. when a trained ML system is itself an optimizer) are aligned with the objective function of the training process.</p><p>As an example, evolution is an optimization force that itself 'designed' optimizers (humans) to achieve its goals. However, humans do not primarily maximize reproductive success, they instead use birth control while still attaining the pleasure that evolution meant as a reward for attempts at reproduction. This is a failure of inner alignment.&nbsp;</p><p>The term was first given a definition in the Hubinger et al paper <i>Risk from Learned Optimization</i>:</p><blockquote><p>We refer to this problem of aligning mesa-optimizers with the base objective as the inner alignment problem. This is distinct from the outer alignment problem, which is the traditional problem of ensuring that the base objective captures the intended goal of the programmers.</p></blockquote><p>Goal misgeneralization due to distribution shift is another example of an inner alignment failure. It is when the mesa-objective appears to pursue the base objective during training but does not pursue it during deployment. We mistakenly think that good performance on the training distribution means that the mesa-optimizer is pursuing the base objective. However, this might have occurred only because there were some correlations in the training distribution resulting in good performance on both the base and mesa objectives. When we had a distribution shift from training to deployment it caused the correlation to be broken and the mesa-objective failed to generalize. This is especially problematic when the capabilities successfully generalize to the deployment distribution while the objectives/goals don't. Since now we have a capable system that is optimizing for a misaligned goal.</p><p>To solve the inner alignment problem, some sub-problems that we would have to make progress on include things such as&nbsp;<a href=\"https://www.lesswrong.com/tag/deceptive-alignment\">deceptive alignment</a>,&nbsp;<a href=\"https://www.alignmentforum.org/tag/distributional-shifts\"><u>distribution shifts</u></a>, and&nbsp;<a href=\"https://www.lesswrong.com/tag/gradient-hacking\"><u>gradient hacking</u></a>.</p><h1>Inner Alignment Vs. Outer Alignment</h1><p>Inner alignment is often talked about as being separate from&nbsp;<a href=\"https://www.lesswrong.com/tag/outer-alignment\"><u>outer alignment</u></a>. The former deals with working on guaranteeing that we are robustly aiming at something, and the latter deals with the problem of what exactly are we aiming at. For more information see the&nbsp;<a href=\"https://www.lesswrong.com/tag/outer-alignment\">corresponding tag</a>.</p><p>It should be kept in mind that you can have both inner and outer alignment failures together. It is not a dichotomy and&nbsp;<a href=\"https://www.alignmentforum.org/posts/JKwrDwsaRiSxTv9ur/categorizing-failures-as-outer-or-inner-misalignment-is\"><u>often even experienced alignment researchers are unable to tell them apart</u></a>. This indicates that the classifications of failures according to these terms are fuzzy. Ideally, we don't think of a binary dichotomy of inner and outer alignment that can be tackled individually but of a more holistic alignment picture that includes the interplay between both inner and outer alignment approaches.</p><h1>Related Pages:&nbsp;</h1><p><a href=\"https://www.lesswrong.com/tag/mesa-optimization\">Mesa-Optimization</a>, <a href=\"https://www.lesswrong.com/tag/treacherous-turn\">Treacherous Turn</a>, <a href=\"https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk\">Eliciting Latent Knowledge</a>, <a href=\"https://www.lesswrong.com/tag/deceptive-alignment\">Deceptive Alignment</a>, <a href=\"https://www.lesswrong.com/tag/deception\">Deception</a></p><h1>External Links:</h1><ul><li><a href=\"https://www.youtube.com/watch?v=bJLcIBixGj8\">Video by Robert Miles</a></li></ul>",
    "description_length": 4185,
    "viewCount": 451,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "xHTXnyp65X8YX6ahT",
    "name": "Instrumental Convergence",
    "slug": "instrumental-convergence",
    "postCount": 112,
    "description_html": "<p><strong>Instrumental convergence</strong> or <strong>convergent instrumental values </strong>is the theorized tendency for most sufficiently intelligent agents to pursue potentially unbounded instrumental goals such as self-preservation and resource acquisition [<a href=\"https://en.wikipedia.org/wiki/Instrumental_convergence\">1</a>]. This concept has also been discussed under the term <i>basic drives.</i></p><p>The idea was first explored by <a href=\"https://en.wikipedia.org/wiki/Steve_Omohundro\">Steve Omohundro</a>. He argued that sufficiently advanced AI systems would all naturally discover similar instrumental subgoals. The view that there are important basic AI drives was subsequently defended by <a href=\"https://lesswrong.com/tag/nick-bostrom\">Nick Bostrom</a> as the<i> instrumental convergence thesis</i>, or the <i>convergent instrumental goals thesis</i>. On this view, a few goals are <a href=\"https://lesswrong.com/tag/instrumental-value\">instrumental</a> to almost all possible <a href=\"https://lesswrong.com/tag/terminal-value\">final</a> goals. Therefore, all advanced AIs will pursue these instrumental goals. Omohundro uses microeconomic theory by von Neumann to support this idea.</p><h2>Omohundro’s Drives</h2><p>Omohundro presents two sets of values, one for self-improving artificial intelligences [<a href=\"http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf\">2</a>] and another he says will emerge in any sufficiently advanced AGI system [<a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">3</a>]. The former set is composed of four main drives:</p><ul><li><strong>Self-preservation</strong>: A sufficiently advanced AI will probably be the best entity to achieve its goals. Therefore it must continue existing in order to maximize goal fulfillment. Similarly, if its goal system were modified, then it would likely begin pursuing different ends. Since this is not desirable to the current AI, it will act to preserve the content of its goal system.</li><li><strong>Efficiency</strong>: At any time, the AI will have finite resources of time, space, matter, energy and computational power. Using these more efficiently will increase its utility. This will lead the AI to do things like implement more efficient algorithms, physical embodiments, and particular mechanisms. It will also lead the AI to replace desired physical events with computational simulations as much as possible, to expend fewer resources.</li><li><strong>Acquisition</strong>: Resources like matter and energy are indispensable for action. The more resources the AI can control, the more actions it can perform to achieve its goals. The AI's physical capabilities are determined by its level of technology. For instance, if the AI could invent nanotechnology, it would vastly increase the actions it could take to achieve its goals.</li><li><strong>Creativity</strong>: The AI's operations will depend on its ability to come up with new, more efficient ideas. It will be driven to acquire more computational power for raw searching ability, and it will also be driven to search for better search algorithms. Omohundro argues that the drive for creativity is critical for the AI to display the richness and diversity that is valued by humanity. He discusses <a href=\"https://lesswrong.com/tag/signaling\">signaling</a> goals as particularly rich sources of creativity.</li></ul><h2>Bostrom’s Drives</h2><p>Bostrom argues for an <a href=\"https://lesswrong.com/tag/orthogonality-thesis\">orthogonality thesis</a>: But he also argues that, despite the fact that values and intelligence are independent, any recursively self-improving intelligence would likely possess a particular set of instrumental values that are useful for achieving any kind of <a href=\"https://lesswrong.com/tag/terminal-value\">terminal value</a> [<a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">4</a>]. On his view, those values are:</p><ul><li><strong>Self-preservation</strong>: A superintelligence will value its continuing existence as a means to continuing to take actions that promote its values.</li><li><strong>Goal-content integrity</strong>: The superintelligence will value retaining the same preferences over time. Modifications to its future values through swapping memories, downloading skills, and altering its cognitive architecture and personalities would result in its transformation into an agent that no longer optimizes for the same things.</li><li><strong>Cognitive enhancement</strong>: Improvements in cognitive capacity, intelligence and rationality will help the superintelligence make better decisions, furthering its goals more in the long run.</li><li><strong>Technological perfection</strong>: Increases in hardware power and algorithm efficiency will deliver increases in its cognitive capacities. Also, better engineering will enable the creation of a wider set of physical structures using fewer resources (e.g., <a href=\"https://lesswrong.com/tag/nanotechnology\">nanotechnology</a>).</li><li><strong>Resource acquisition</strong>: In addition to guaranteeing the superintelligence's continued existence, basic resources such as time, space, matter and free energy could be processed to serve almost any goal, in the form of extended hardware, backups and protection.</li></ul><h2>Relevance</h2><p>Both Bostrom and Omohundro argue these values should be used in trying to predict a superintelligence's behavior, since they are likely to be the only set of values shared by most superintelligences. They also note that these values are consistent with safe and beneficial AIs as well as unsafe ones.</p><p>Bostrom emphasizes, however, that our ability to predict a superintelligence's behavior may be very limited even if it shares most intelligences' instrumental goals.</p><p>Yudkowsky echoes Omohundro's point that the convergence thesis is consistent with the possibility of Friendly AI. However, he also notes that the convergence thesis implies that most AIs will be extremely dangerous, merely by being indifferent to one or more human values [<a href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">5</a>]:</p><h2>Pathological Cases</h2><p>In some rarer cases, AIs may not pursue these goals. For instance, if there are two AIs with the same goals, the less capable AI may determine that it should destroy itself to allow the stronger AI to control the universe. Or an AI may have the goal of using as few resources as possible, or of being as unintelligent as possible. These relatively specific goals will limit the growth and power of the AI.</p><h2>Experimental Evidence</h2><p>The question of instrumentally convergent drives potentially arising in machine learning models is explored in the paper - <a href=\"https://openreview.net/pdf?id=l7-DBWawSZH\">Optimal Policies Tend To Seek Power</a>. The authors explored instrumental convergence (specifically power-seeking behavior) as a statistical tendency of optimal policies in reinforcement learning (RL) agents.</p><p>The authors focus on Markov Decision Processes (MDPs) and prove that certain environmental symmetries are sufficient for optimal policies to seek power in the environment. They formalize power as the ability to achieve a wide range of goals. Within this formalization, the authors show that most reward functions make it optimal to try and seek power since this allows for keeping a wide range of options available to the agent.</p><p>This provides a counter to the claim that instrumental convergence is merely an anthropomorphic theoretical tendency, and that human-like power-seeking instincts will not arise in RL agents.</p><h2>See Also</h2><ul><li><a href=\"https://arbital.com/p/convergent_strategies/\">Convergent instrumental strategies</a> (<a href=\"https://wiki.lesswrong.com/wiki/Arbital\">Arbital</a>)</li><li><a href=\"https://arbital.com/p/instrumental_convergence/\">Instrumental convergence</a> (<a href=\"https://wiki.lesswrong.com/wiki/Arbital\">Arbital</a>)</li><li><a href=\"https://lesswrong.com/tag/orthogonality-thesis\">Orthogonality thesis</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Cox's_theorem\">Cox's theorem</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Unfriendly_AI\">Unfriendly AI</a>, <a href=\"https://lesswrong.com/tag/paperclip-maximizer\">Paperclip maximizer</a>, <a href=\"https://lesswrong.com/tag/oracle-ai\">Oracle AI</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Instrumental_values\">Instrumental values</a></li></ul><h2>References</h2><ul><li>Omohundro, S. (2007). <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf\"><i><u>The Nature of Self-Improving Artificial Intelligence</u></i></a>.</li><li>Omohundro, S. (2008). \"<a href=\"http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/\"><u>The Basic AI Drives</u></a>\". <i>Proceedings of the First AGI Conference</i>.</li><li>Omohundro, S. (2012). <a href=\"http://selfawaresystems.files.wordpress.com/2012/03/rational_ai_greater_good.pdf\"><i><u>Rational Artificial Intelligence for the Greater Good</u></i></a>.</li><li>Bostrom, N. (2012). \"<a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\"><u>The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents</u></a>\". <i>Minds and Machines</i>.</li><li>Shulman, C. (2010). <a href=\"http://intelligence.org/files/BasicAIDrives.pdf\"><i><u>Omohundro's \"Basic AI Drives\" and Catastrophic Risks</u></i></a>.</li><li>Alexander Matt Turner, Logan Riggs Smith, Rohin Shah, Andrew Critch, Prasad Tadepalli (2021). <a href=\"https://openreview.net/forum?id=l7-DBWawSZH\"><i>Optimal Policies Tend To Seek Power</i></a></li></ul>",
    "description_length": 9784,
    "viewCount": 669,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "5f5c37ee1b5cdee568cfb1be",
    "name": "Intelligence Explosion",
    "slug": "intelligence-explosion",
    "postCount": 42,
    "description_html": "<p>An <strong>intelligence explosion</strong> is theoretical scenario in which an intelligent agent analyzes the processes that produce its intelligence, improves upon them, and creates a successor which does the same. This process repeats in a positive feedback loop– each successive agent more intelligent than the last and thus more able to increase the intelligence of its successor – until some limit is reached. This limit is conjectured to be much, much higher than human intelligence.</p><p>A strong version of this idea suggests that once the positive feedback starts to play a role, it will lead to a very dramatic leap in capability very quickly. This is known as a “hard takeoff.” In this scenario, technological progress drops into the characteristic timescale of transistors rather than human neurons, and the ascent rapidly surges upward and creates superintelligence (a mind orders of magnitude more powerful than a human's) before it hits physical limits. A hard takeoff is distinguished from a \"soft takeoff\" only by the speed with which said limits are reached.</p><h2>Published arguments</h2><p>Philosopher David Chalmers published a <a href=\"http://consc.net/papers/singularity.pdf\">significant analysis of the Singularity</a>, focusing on intelligence explosions, in <i>Journal of Consciousness Studies</i>. <a href=\"https://wiki.lesswrong.com/wiki/Singularity#Chalmers.27_analysis\">His analysis</a> of how they could occur defends the likelihood of an intelligence explosion. He performed a very careful analysis of the main premises and arguments for the existence of the a singularity from an intelligence explosion. According to him, the main argument is:\"</p><ul><li>1. There will be AI (before long, absent defeaters).</li><li>2. If there is AI, there will be AI+ (soon after, absent defeaters).</li><li>3. If there is AI+, there will be AI++ (soon after, absent defeaters).</li></ul><p>—————-</p><ul><li>4. There will be AI++ (before too long, absent defeaters). \"</li></ul><p>He also discusses the nature of general intelligence, and possible obstacles to a singularity. A good deal of discussion is given to the dangers of an intelligence explosion, and Chalmers concludes that we must negotiate it very carefully by building the correct values into the initial AIs.</p><p><a href=\"http://lesswrong.com/user/lukeprog\">Luke Muehlhauser</a> and <a href=\"http://lesswrong.com/user/AnnaSalamon\">Anna Salamon</a> argue in <a href=\"http://intelligence.org/files/IE-EI.pdf\"><i>Intelligence Explosion: Evidence and Import</i></a> in detail that there is a substantial chance of an intelligence explosion within 100 years, and extremely critical in determining the future. They trace the implications of many types of upcoming technologies, and point out the feedback loops present in them. This leads them to deduce that an above-human level AI will almost certainly lead to an intelligence explosion. They conclude with recommendations for bringing about a safe intelligence explosion.</p><h2>Hypothetical path</h2><p>The following is a common example of a possible path for an AI to bring about an intelligence explosion. First, the AI is smart enough to conclude that inventing molecular nanotechnology will be of greatest benefit to it. Its first act of recursive self-improvement is to gain access to other computers over the internet. This extra computational ability increases the depth and breadth of its search processes. It then uses gained knowledge of material physics and a distributed computing program to invent the first general assembler nanomachine. Then it uses some manufacturing technology, accessible from the internet, to build and deploy the nanotech. It programs the nanotech to turn a large section of bedrock into a supercomputer. This is its second act of recursive self-improvement, only possible because of the first. Then it could use this enormous computing power to consider hundreds of alternative decision algorithms, better computing structures and so on. After this, this AI would go from a near to human level intelligence to a superintelligence, providing a dramatic and abruptly increase in capability.</p><h2>Blog posts</h2><ul><li><a href=\"http://lesswrong.com/lw/w5/cascades_cycles_insight/\">Cascades, Cycles, Insight...</a>, <a href=\"http://lesswrong.com/lw/w6/recursion_magic/\">...Recursion, Magic</a></li><li><a href=\"http://lesswrong.com/lw/we/recursive_selfimprovement/\">Recursive Self-Improvement</a>, <a href=\"http://lesswrong.com/lw/wf/hard_takeoff/\">Hard Takeoff</a>, <a href=\"http://lesswrong.com/lw/wg/permitted_possibilities_locality/\">Permitted Possibilities, &amp; Locality</a></li></ul><h2>See also</h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Technological_singularity\">Technological singularity</a>, <a href=\"https://wiki.lesswrong.com/wiki/Hard_takeoff\">Hard takeoff</a></li><li><a href=\"https://www.lesswrong.com/tag/existential-risk\">Existential risk</a></li><li><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\">Artificial General Intelligence</a></li><li><a href=\"https://www.lesswrong.com/tag/lawful-intelligence\">Lawful intelligence</a></li><li><a href=\"https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate\">The Hanson-Yudkowsky AI-Foom Debate</a></li></ul><h2>External links</h2><ul><li><a href=\"http://intelligenceexplosion.com/\">Intelligence Explosion website</a>, a landing page for introducing the concept</li><li><a href=\"http://yudkowsky.net/singularity/schools\">Three Major Singularity Schools</a></li></ul><h2>References</h2><ul><li>Good, Irving John (1965). Franz L. Alt and Morris Rubinoff. ed. \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf\"><u>Speculations concerning the first ultraintelligent machine</u></a>.\" <i>Advances in computers</i> (New York: Academic Press) <strong>6</strong>: 31-88. <a href=\"https://wiki.lesswrong.com/index.php?title=Digital_object_identifier&amp;action=edit&amp;redlink=1\"><u>doi</u></a>:<a href=\"http://dx.doi.org/10.1016%2FS0065-2458%2808%2960418-0\"><u>10.1016/S0065-2458(08)60418-0</u></a>.</li><li>David Chalmers (2010). \"<a href=\"http://consc.net/papers/singularity.pdf\"><u>The Singularity: A Philosophical Analysis</u></a>.\" <i>Journal of Consciousness Studies</i> <strong>17</strong>: 7-65.</li><li>Muehlhauser, Luke; Salamon, Anna (2012). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\"><u>\"Intelligence Explosion: Evidence and Import\"</u></a>. in Eden, Amnon; Søraker, Johnny; Moor, James H. et al. <i>The singularity hypothesis: A scientific and philosophical assessment</i>. Berlin: Springer.</li></ul>",
    "description_length": 6765,
    "viewCount": 479,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "8ckoduMw3gvCMJGSB",
    "name": "Logical Induction",
    "slug": "logical-induction",
    "postCount": 38,
    "description_html": "<p><strong>Logical Induction </strong>is a formal theory of reasoning under <a href=\"https://www.lesswrong.com/tag/logical-uncertainty\">logical uncertainty</a>, developed by Scott Garrabrant and other researchers. Rationality is defined through a prediction-market analogy. High-quality beliefs are those which are computationally difficult to win bets against. The writeup can be found <a href=\"https://intelligence.org/2016/09/12/new-paper-logical-induction/\">here</a>.</p>",
    "description_length": 475,
    "viewCount": 151,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "JHYaBGQuuKHdwnrAK",
    "name": "Logical Uncertainty",
    "slug": "logical-uncertainty",
    "postCount": 70,
    "description_html": "<p><strong>Logical Uncertainty</strong> is probabilistic uncertainty about the implications of beliefs. (Another way of thinking about it is: uncertainty about computations.) Probability theory typically assumes <strong>logical omniscience, </strong>IE, perfect knowledge of logic. The easiest way to see the importance of this assumption is to consider Bayesian reasoning: to evaluate the probability of evidence given a hypothesis,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(e|h)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\">P</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">e</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">|</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">h</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>, it's necessary to know what the implications of the hypothesis are. However, realistic agents cannot be logically omniscient.</p><p>See Also:<a href=\"https://www.lesswrong.com/tag/logical-induction\"> Logical Induction</a></p><h2>Motivation</h2><p>Is the googolth digit of pi odd? The probability that it is odd is, intuitively, 0.5. Yet we know that this is definitely true or false by the rules of logic, even though we don't know which. Formalizing this sort of probability is the primary goal of the field of logical uncertainty.</p><p>The problem with the 0.5 probability is that it gives non-zero probability to false statements. If I am asked to bet on whether the googolth digit of pi is odd, I can reason as follows: There is 0.5 chance that it is odd. Let P represent the actual, unknown, parity of the googolth digit (odd or even); and let Q represent the other parity. If Q, then anything follows. (By the Principle of Explosion, a false statement implies anything.) For example, Q implies that I will win $1 billion. Therefore the value of this bet is at least $500,000,000, which is 0.5 * $1,000,000, and I should be willing to pay that much to take the bet. This is an absurdity. Only expenditure of finite computational power stands between the uncertainty and 100% certainty.</p><h2>Logical Uncertainty &amp; Counterfactuals</h2><p>Logical uncertainty is closely related to the problem of <a href=\"/tag/counterfactuals\">counterfactuals</a>. Ordinary probability theory relies on counterfactuals. For example, I see a coin that came up heads, and I say that the probability of tails was 0.5, even though clearly, given all air currents and muscular movements involved in throwing that coin, the probability of tails was 0.0. Yet we can imagine this possible impossible world where the coin came up tails. In the case of logical uncertainly, it is hard to imagine a world in which mathematical facts are different.</p><h2>References</h2><ul><li><a href=\"https://intelligence.org/files/QuestionsLogicalUncertainty.pdf\">Questions of Reasoning Under Logical Uncertainty</a> by Nate Soares and Benja Fallenstein.</li></ul>",
    "description_length": 21022,
    "viewCount": 119,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "NZ67PZ8CkeS6xn27h",
    "name": "Mesa-Optimization",
    "slug": "mesa-optimization",
    "postCount": 127,
    "description_html": "<p><strong>Mesa-Optimization</strong> is the situation that occurs when a learned model (such as a neural network) is itself an optimizer. In this situation, a <i>base optimizer</i> creates a second optimizer, called a <i>mesa-optimizer</i>. The primary reference work for this concept is Hubinger et al.'s \"<a href=\"https://www.alignmentforum.org/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction\">Risks from Learned Optimization in Advanced Machine Learning Systems</a>\".</p><p>Example: Natural selection is an optimization process that optimizes for reproductive fitness. Natural selection produced humans, who are themselves optimizers. Humans are therefore mesa-optimizers of natural selection.</p><p>In the context of AI alignment, the concern is that a base optimizer (e.g., a gradient descent process) may produce a learned model that is itself an optimizer, and that has unexpected and undesirable properties. Even if the gradient descent process is in some sense \"trying\" to do exactly what human developers want, the resultant mesa-optimizer will not typically be trying to do the exact same thing.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1wvq4n2qe47\"><sup><a href=\"#fn1wvq4n2qe47\">[1]</a></sup></span></p><p>&nbsp;</p><h2>History</h2><p>Previously work under this concept was called <i>Inner Optimizer </i>or <i>Optimization Daemons.</i></p><p><a href=\"https://www.lesswrong.com/users/wei_dai\">Wei Dai</a> brings up a similar idea in an SL4 thread.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqwecn6uicu\"><sup><a href=\"#fnqwecn6uicu\">[2]</a></sup></span></p><p>The optimization daemons article on <a href=\"https://arbital.com/\">Arbital</a> was published probably in 2016.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1wvq4n2qe47\"><sup><a href=\"#fn1wvq4n2qe47\">[1]</a></sup></span></p><p><a href=\"https://www.lesswrong.com/users/jessica-liu-taylor\">Jessica Taylor</a> wrote two posts about daemons while at <a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri\">MIRI</a>:</p><ul><li><a href=\"https://agentfoundations.org/item?id=1281\">\"Are daemons a problem for ideal agents?\"</a> (2017-02-11)</li><li><a href=\"https://agentfoundations.org/item?id=1290\">\"Maximally efficient agents will probably have an anti-daemon immune system\"</a> (2017-02-23)</li></ul><h2>&nbsp;</h2><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/inner-alignment\">Inner Alignment</a></li><li><a href=\"https://lesswrong.com/tag/complexity-of-value\">Complexity of value</a></li><li><a href=\"https://lesswrong.com/lw/l3/thou_art_godshatter/\">Thou Art Godshatter</a></li></ul><h2>External links</h2><p><a href=\"https://www.youtube.com/watch?v=bJLcIBixGj8\">Video by Robert Miles</a></p><p>Some posts that reference optimization daemons:</p><ul><li><a href=\"http://effective-altruism.com/ea/1k4/draft_cause_prioritization_for_downsidefocused/\">\"Cause prioritization for downside-focused value systems\"</a>: \"Alternatively, perhaps goal preservation becomes more difficult the more capable AI systems become, in which case the future might be controlled by unstable goal functions taking turns over the steering wheel\"</li><li><a href=\"https://ai-alignment.com/techniques-for-optimizing-worst-case-performance-39eafec74b99\">\"Techniques for optimizing worst-case performance\"</a>: \"The difficulty of optimizing worst-case performance is one of the most likely reasons that I think prosaic AI alignment might turn out to be impossible (if combined with an unlucky empirical situation).\" (the phrase \"unlucky empirical situation\" links to the optimization daemons page on <a href=\"https://arbital.com/\">Arbital</a>)</li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1wvq4n2qe47\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1wvq4n2qe47\">^</a></strong></sup></span><div class=\"footnote-content\"><p><a href=\"https://arbital.com/p/daemons/\"><u>\"Optimization daemons\"</u></a>. Arbital.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqwecn6uicu\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqwecn6uicu\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Wei Dai. <a href=\"http://sl4.org/archive/0312/7421.html\"><u>'\"friendly\" humans?'</u></a> December 31, 2003.</p></div></li></ol>",
    "description_length": 4377,
    "viewCount": 897,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "AGgktgYb72PPjET9r",
    "name": "Multipolar Scenarios",
    "slug": "multipolar-scenarios",
    "postCount": 27,
    "description_html": "<p>A <strong>multipolar scenario</strong> is one where no single AI or agent takes over the world.</p><p>Featured in the book \"Superintelligence\" by Nick Bostrom.</p>",
    "description_length": 166,
    "viewCount": 122,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "3Y4y9Kr8e24YWAEmD",
    "name": "Myopia",
    "slug": "myopia",
    "postCount": 42,
    "description_html": null,
    "description_length": null,
    "viewCount": 137,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "fihKHQuS5WZBJgkRm",
    "name": "Newcomb's Problem",
    "slug": "newcomb-s-problem",
    "postCount": 69,
    "description_html": "<p><strong>Newcomb's Problem</strong> is a thought experiment in decision theory exploring problems posed by having other agents in the environment who can predict your actions.</p><h2>The Problem</h2><p>From <a href=\"https://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality\">Newcomb's Problem and Regret of Rationality</a>:</p><blockquote><p>A superintelligence from another galaxy, whom we shall call Omega, comes to Earth and sets about playing a strange little game. In this game, Omega selects a human being, sets down two boxes in front of them, and flies away.</p></blockquote><blockquote><p>Box A is transparent and contains a thousand dollars.<br>Box B is opaque, and contains either a million dollars, or nothing.</p></blockquote><blockquote><p>You can take both boxes, or take only box B.</p></blockquote><blockquote><p>And the twist is that Omega has put a million dollars in box B iff Omega has predicted that you will take only box B.</p></blockquote><blockquote><p>Omega has been correct on each of 100 observed occasions so far - everyone who took both boxes has found box B empty and received only a thousand dollars; everyone who took only box B has found B containing a million dollars. (We assume that box A vanishes in a puff of smoke if you take only box B; no one else can take box A afterward.)</p></blockquote><blockquote><p>Before you make your choice, Omega has flown off and moved on to its next game. Box B is already empty or already full.</p></blockquote><blockquote><p>Omega drops two boxes on the ground in front of you and flies off.</p></blockquote><blockquote><p>Do you take both boxes, or only box B?</p></blockquote><p>One line of reasoning about the problem says that because Omega has already left, the boxes are set and you can't change them. And if you look at the payoff matrix, you'll see that whatever decision Omega has already made, you get $1000 more for taking both boxes. This makes taking two boxes (\"two-boxing\") a dominant strategy and therefore the correct choice. Agents who reason this way do not make very much money playing this game. This is because this line of reasoning ignores the connection between the agent and Omega's prediction: two-boxing only makes $1000 more than one-boxing if Omega's prediction is the same in both cases, while the problem states Omega is extremely accurate in its predictions. Switching from one-boxing to two-boxing doesn't give the agent a $1000 more, it results in a loss of $999,000.</p><p>Because the agent's decision in this problem can't causally affect Omega's prediction (which happened in the past), <a href=\"https://www.lesswrong.com/tag/causal-decision-theory\">Causal Decision Theory</a> two-boxes. One-boxing is correlated with getting a million dollars, whereas two-boxing is correlated with getting only $1000; therefore, <a href=\"https://www.lesswrong.com/tag/evidential-decision-theory\">Evidential Decision Theory</a> one-boxes. <a href=\"https://www.lesswrong.com/tag/functional-decision-theory\">Functional Decision Theory</a> (FDT) also one-boxes, but for a completely different reason: FDT reasons that Omega must have had a model of the agent's decision procedure in order to make the prediction. Therefore, your decision procedure is run not only by you, but also (in the past) by Omega; whatever you decide, Omega's model must have decided the same. Either both you and Omega's model two-box, or both you and Omega's model one-box; of these two options, the latter is preferable, so FDT one-boxes.</p><p>The general class of decision problems that involve other agents predicting your actions are called Newcomblike Problems.</p><h2>Irrelevance of Omega's Physical Impossibility</h2><p>Sometimes people dismiss Newcomb's problem because of the physical impossibility of a being like Omega. However, Newcomb's problem does not actually depend on the possibility of Omega in order to be relevant. Similar issues arise if we imagine a skilled human psychologist who can predict other people's actions with 65% accuracy in similar situations.</p><h2>Notable Posts</h2><ul><li><a href=\"https://lessestwrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's Problem and Regret of Rationality</a></li><li><a href=\"https://lessestwrong.com/lw/7v/formalizing_newcombs/\">Formalizing Newcomb's</a></li><li><a href=\"https://lessestwrong.com/lw/90/newcombs_problem_standard_positions/\">Newcomb's Problem standard positions</a></li><li><a href=\"https://lessestwrong.com/lw/6r/newcombs_problem_vs_oneshot_prisoners_dilemma/\">Newcomb's Problem vs. One-Shot Prisoner's Dilemma</a></li><li><a href=\"https://lessestwrong.com/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/\">Decision theory: Why Pearl helps reduce “could” and “would”, but still leaves us with at least three alternatives</a></li></ul><h2>See Also</h2><ul><li><a href=\"https://lessestwrong.com/tag/decision-theory\">Decision theory</a></li><li><a href=\"https://lessestwrong.com/tag/counterfactual-mugging\">Counterfactual mugging</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker\">Parfit's hitchhiker</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Smoker's_lesion\">Smoker's lesion</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Absentminded_driver\">Absentminded driver</a></li><li><a href=\"https://lessestwrong.com/tag/sleeping-beauty-paradox\">Sleeping Beauty problem</a></li><li><a href=\"https://lessestwrong.com/tag/prisoner-s-dilemma\">Prisoner's dilemma</a></li></ul>",
    "description_length": 5524,
    "viewCount": 1251,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "nvKzwpiranwy29HFJ",
    "name": "Optimization",
    "slug": "optimization",
    "postCount": 162,
    "description_html": "<p>An <strong>optimization process</strong> is any kind of process that systematically comes up with solutions that are better than the solution used before. More technically, this kind of process moves the world into a specific and unexpected set of states by searching through a large search space, hitting small and low probability targets. When this process is gradually guided by some agent into some specific state, through searching specific targets, we can say it <a href=\"https://www.lesswrong.com/tag/preference\">prefers</a> that state.</p><p>The best way to exemplify an optimization process is through a simple example: <a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky</a> suggests natural selection is such a process. Through an implicit preference – better replicators – natural selection searches all the genetic landscape space and hit small targets: efficient mutations.</p><p>Consider the human being. We are a highly complex object with a low probability to have been created by chance - natural selection, however, over millions of years, built up the infrastructure needed to build such a functioning body. This body, as well as other organisms, had the chance (was <i>selected</i>) to develop because it is in itself a rather efficient replicator suitable for the environment where it came up.</p><p>Or consider the famous chessplaying computer, <a href=\"https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)\">Deep Blue</a>. Outside of the narrow domain of selecting moves for chess games, it can't do anything impressive: but <i>as</i> a chessplayer, it was massively more effective than virtually all humans. It has a high optimization power in the chess domain but almost none in any other field. Humans or evolution, on the other hand, are more domain-general optimization processes than Deep Blue, but that doesn't mean they're more effective at chess specifically. (Although note in what contexts this <i>optimization process</i> abstraction is useful and where it fails to be useful: it's not obvious what it would mean for \"evolution\" to play chess, and yet it is useful to talk about the optimization power of natural selection, or of Deep Blue.)</p><h2>Measuring Optimization Power</h2><p>One way to think mathematically about optimization, like <a href=\"https://www.lesswrong.com/tag/amount-of-evidence\">evidence</a>, is in information-theoretic bits. The optimization power is the amount of <a href=\"http://en.wikipedia.org/wiki/Self-information\">surprise</a> we would have in the result if there were no optimization process present. Therefore we take the base-two logarithm of the reciprocal of the probability of the result. A one-in-a-million solution (a solution so good relative to your preference ordering that it would take a million random tries to find something that good or better) can be said to have log_2(1,000,000) = 19.9 bits of optimization. Compared to a random configuration of matter, any artifact you see is going to be much more optimized than this. The math describes only laws and general principles for reasoning about optimization; as with <a href=\"https://www.lesswrong.com/tag/bayesian-probability\">probability theory</a>, you oftentimes can't apply the math directly.</p><h2>Further Reading &amp; References</h2><ul><li><a href=\"https://www.lesswrong.com/lw/rk/optimization_and_the_singularity/\">Optimization and the Singularity</a> by Eliezer Yudkowsky</li><li><a href=\"https://www.lesswrong.com/lw/va/measuring_optimization_power/\">Measuring Optimization Power</a> by Eliezer Yudkowsky</li></ul><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/preference\">Preference</a></li><li><a href=\"https://www.lesswrong.com/tag/really-powerful-optimization-process\">Really powerful optimization process</a></li><li><a href=\"https://www.lesswrong.com/tag/control-theory\">Control theory</a></li></ul>",
    "description_length": 3899,
    "viewCount": 147,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "BXL4riEJvJJHoydjG",
    "name": "Orthogonality Thesis",
    "slug": "orthogonality-thesis",
    "postCount": 56,
    "description_html": "<p>The <strong>Orthogonality Thesis</strong> asserts that there can exist arbitrarily intelligent agents pursuing any kind of goal.</p><p>The strong form of the Orthogonality Thesis says that there’s no extra difficulty or complication in the existence of an intelligent agent that pursues a goal, above and beyond the computational tractability of that goal.</p><p>Suppose some strange alien came to Earth and credibly offered to pay us one million dollars’ worth of new wealth every time we created a paperclip. We’d encounter no special intellectual difficulty in figuring out how to make lots of paperclips.</p><p>That is, minds would readily be able to reason about:</p><ul><li>How many paperclips would result, if I pursued a policy &nbsp;null ?</li><li>How can I search out a policy &nbsp;null &nbsp;that happens to have a high answer to the above question?</li></ul><p>The Orthogonality Thesis asserts that since these questions are not computationally intractable, it’s possible to have an agent that tries to make paperclips without being paid, because paperclips are what it wants. The strong form of the Orthogonality Thesis says that there need be nothing especially complicated or twisted about such an agent.</p><p>The Orthogonality Thesis is a statement about computer science, an assertion about the logical design space of possible cognitive agents. Orthogonality says nothing about whether a human AI researcher on Earth would want to build an AI that made paperclips, or conversely, want to make a nice AI. The Orthogonality Thesis just asserts that the space of possible designs contains AIs that make paperclips. And also AIs that are nice, to the extent there’s a sense of “nice” where you could say how to be nice to someone if you were paid a billion dollars to do that, and to the extent you could name something physically achievable to do.</p><p>This contrasts to inevitablist theses which might assert, for example:</p><ul><li>“It doesn’t matter what kind of AI you build, it will turn out to only pursue its own survival as a final end.”</li><li>“Even if you tried to make an AI optimize for paperclips, it would reflect on those goals, reject them as being stupid, and embrace a goal of valuing all sapient life.”</li></ul><p>The reason to talk about Orthogonality is that it’s a key premise in two highly important policy-relevant propositions:</p><ul><li>It is possible to build a nice AI.</li><li>It is possible to screw up when trying to build a nice AI, and if you do, the AI will not automatically decide to be nice instead.</li></ul><p>Orthogonality does not require that all agent designs be equally compatible with all goals. E.g., the agent architecture AIXI-tl can only be formulated to care about direct functions of its sensory data, like a reward signal; it would not be easy to rejigger the AIXI architecture to care about creating massive diamonds in the environment (let alone any more complicated environmental goals). The Orthogonality Thesis states “there exists at least one possible agent such that…” over the whole design space; it’s not meant to be true of every particular agent architecture and every way of constructing agents.</p><p>Orthogonality is meant as a descriptive statement about reality, not a normative assertion. Orthogonality is not a claim about the way things ought to be; nor a claim that moral relativism is true (e.g. that all moralities are on equally uncertain footing according to some higher metamorality that judges all moralities as equally devoid of what would objectively constitute a justification). Claiming that paperclip maximizers can be constructed as cognitive agents is not meant to say anything favorable about paperclips, nor anything derogatory about sapient life.</p><p>The thesis was originally defined by <a href=\"https://lesswrong.com/tag/nick-bostrom\">Nick Bostrom</a> in the paper \"<a href=\"https://nickbostrom.com/superintelligentwill.pdf\">Superintelligent Will</a>\", (along with the <a href=\"https://wiki.lesswrong.com/wiki/instrumental_convergence_thesis\">instrumental convergence thesis</a>). For his purposes, Bostrom defines intelligence to be <a href=\"https://wiki.lesswrong.com/wiki/instrumental_rationality\">instrumental rationality</a>.</p><p><a href=\"https://arbital.com/p/orthogonality/\">(Most of the above copied from the Arbital orthogonality thesis article, continue reading there)</a></p><p><i>Related: </i><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\"><i>Complexity of Value</i></a><i>, </i><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\"><i>Decision Theory</i></a><i>, </i><a href=\"https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&amp;useTagName=true\"><i>General Intelligence</i></a><i>, </i><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\"><i>Utility Functions</i></a></p><h2>See Also</h2><ul><li><a href=\"https://lesswrong.com/tag/instrumental-convergence\">Instrumental Convergence</a></li></ul><h2>External links</h2><ul><li>Definition of the orthogonality thesis from Bostrom's <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">Superintelligent Will</a></li><li><a href=\"http://philosophicaldisquisitions.blogspot.com/2012/04/bostrom-on-superintelligence-and.html\">Critique</a> of the thesis by John Danaher</li><li>Superintelligent Will paper by Nick Bostrom</li></ul>",
    "description_length": 5474,
    "viewCount": 2343,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "BisjoDrd3oNatDu7X",
    "name": "Outer Alignment",
    "slug": "outer-alignment",
    "postCount": 274,
    "description_html": "<p>Outer alignment asks the question - \"<em>What should we aim our model at?</em>\" In other words, is the model optimizing for the correct reward such that there are no exploitable loopholes? It is also known as the <strong>reward misspecification</strong> problem.</p><p>Overall, outer alignment as a problem is intuitive enough to understand, i.e., is the specified loss function aligned with the intended goal of its designers? However, implementing this in practice is extremely difficult. Conveying the full “intention” behind a human request is equivalent to conveying the sum of all human values and ethics. This is difficult in part because human intentions are themselves not well understood. Additionally, since most models are designed as goal optimizers, they are all susceptible to <a href=\"https://www.lesswrong.com/tag/goodhart-s-law\">Goodhart’s Law</a> which means that we might be unable to foresee negative consequences that arise due to excessive optimization pressure on a goal that would look otherwise well specified to humans.</p><p>To solve the outer alignment problem, some sub-problems that we would have to make progress on include <a href=\"https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity\">specification gaming</a>, <a href=\"https://www.lesswrong.com/tag/value-learning\">value learning</a>, and reward shaping/modeling. Some proposed solutions to outer alignment include scalable oversight techniques such as <a href=\"https://www.alignmentforum.org/tag/iterated-amplification\">IDA</a>, as well as adversarial oversight techniques such as <a href=\"https://www.lesswrong.com/tag/debate-ai-safety-technique-1\">debate</a>.</p><h1>Outer Alignment vs. Inner Alignment</h1><p>This is often taken to be separate from the <a href=\"https://www.lesswrong.com/tag/inner-alignment\">inner alignment</a> problem, which asks: How can we robustly aim our AI optimizers at any objective function at all?</p><p>It should be kept in mind that you can have both inner and outer alignment failures together. It is not a dichotomy and <a href=\"https://www.lesswrong.com/posts/JKwrDwsaRiSxTv9ur/categorizing-failures-as-outer-or-inner-misalignment-is\">often even experienced alignment researchers are unable to tell them apart</a>. This indicates that the classifications of failures according to these terms are fuzzy. Ideally, we don't think of a binary dichotomy of inner and outer alignment that can be tackled individually but of a more holistic alignment picture that includes the interplay between both inner and outer alignment approaches.</p>",
    "description_length": 2591,
    "viewCount": 257,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "mcZs3YNQpSX6HrSin",
    "name": "Power Seeking (AI)",
    "slug": "power-seeking-ai",
    "postCount": 35,
    "description_html": "<p><strong>Power Seeking </strong>is a property that agents might have, where they attempt to gain more general ability to control their environment. It's particularly relevant to AIs, and related to <a href=\"https://www.lesswrong.com/tag/instrumental-convergence\">Instrumental Convergence</a>.</p>",
    "description_length": 298,
    "viewCount": 92,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "5f5c37ee1b5cdee568cfb2b5",
    "name": "Recursive Self-Improvement",
    "slug": "recursive-self-improvement",
    "postCount": 49,
    "description_html": "<p><strong>Recursive self-improvement</strong> refers to the property of making improvements on one's own ability of making self-improvements. It is an approach to <a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\">Artificial General Intelligence</a> that allows a system to make adjustments to its own functionality resulting in improved performance. The system could then feedback on itself with each cycle reaching ever higher levels of intelligence resulting in either a hard or soft <a href=\"https://www.lesswrong.com/tag/ai-takeoff\">AI takeoff</a>.</p><p>An agent can self-improve and get a linear succession of improvements, however if it is able to improve its ability of making self-improvements, then each step will yield exponentially more improvements then the previous one.</p><h2>Recursive self-improvement and <a href=\"https://www.lesswrong.com/tag/ai-takeoff\">AI takeoff</a></h2><p>Recursively self-improving AI is considered to be the push behind the <a href=\"https://www.lesswrong.com/tag/intelligence-explosion\">intelligence explosion</a>. While any sufficiently intelligent AI will be able to improve itself, <a href=\"https://www.lesswrong.com/tag/seed-ai\">Seed AIs</a> are specifically designed to use recursive self-improvement as their primary method of gaining intelligence. Architectures that had not been designed with this goal in mind, such as neural networks or large \"hand-coded\" projects like <a href=\"https://www.lesswrong.com/tag/cyc\">Cyc</a>, would have a harder time self-improving.</p><p><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky</a> argues that a recursively self-improvement AI seems likely to deliver a hard AI takeoff – a fast, abruptly, local increase in capability - since the exponential increase in intelligence would yield an exponential return in benefits and resources that would feed even more returns in the next step, and so on. In his view a soft takeoff scenario seems unlikely: \"it should either flatline or blow up. You would need exactly the right law of diminishing returns to fly through the extremely narrow soft takeoff keyhole.\"<a href=\"http://lesswrong.com/lw/we/recursive_selfimprovement/\">1</a>.</p><p>Yudkowsky argues that there are several points which seem to support the <a href=\"https://wiki.lesswrong.com/wiki/AI_takeoff#Hard_takeoff\">hard takeoff scenario</a>. Some of them are the fact that one improvement seems to lead the way to another, <a href=\"https://www.lesswrong.com/tag/computing-overhang\">hardware overhang</a> and the fact that sometimes- when navigating through problem space - one can find a succession of extremely easy to solve problems. These are all reasons for suddenly and abruptly increases in capability. On the other hand, <a href=\"https://www.lesswrong.com/tag/robin-hanson\">Robin Hanson</a> argues that there will be mostly a slow and gradual accumulation of improvements, without a sharp change.</p><h2>Self-improvement in humans</h2><p>The human species has made an enormous amount of progress since evolving around fifty thousand years ago. This is because we can pass on knowledge and infrastructure from previous generations. This is a type of self-improvement, but it is not <i>recursive</i>. If we never learned to modify our own brains, then we would eventually reach the point where making new discoveries required more knowledge than could be gained in a human lifetime. All human progress to date has been limited by the hardware we are born with, which is the same hardware Homo sapiens were born with fifty thousand years ago.</p><p>\"True\" recursive self-improvement will come when we discover how to drastically modify or augment our own brains in order to be more intelligent. This would lead us to more quickly being able to discover how to become even more intelligent.</p><h2>Recursive self-improvement and <a href=\"https://www.lesswrong.com/tag/instrumental-value\">Instrumental value</a></h2><p><a href=\"https://www.lesswrong.com/tag/nick-bostrom\">Nick Bostrom</a> and <a href=\"https://en.wikipedia.org/wiki/Steve_Omohundro\">Steve Omohundro</a> have separately<a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">2</a> argued<a href=\"http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf\">3</a> that despite the fact that values and intelligence are independent, any recursively self-improving intelligence would likely possess a common set of instrumental values which are useful for achieving any kind of <a href=\"https://www.lesswrong.com/tag/terminal-value\">goal</a>. As a system's intelligence continued modifying itself towards greater intelligence, it would be likely to adopt more of these behaviors.</p><h2>Blog posts</h2><ul><li><a href=\"http://lesswrong.com/lw/we/recursive_selfimprovement/\">Recursive Self Improvement</a> by Eliezer Yudkowsky</li><li><a href=\"http://lesswrong.com/lw/w5/cascades_cycles_insight/\">Cascades, Cycles, Insight...</a> by Eliezer Yudkowsky</li><li><a href=\"http://lesswrong.com/lw/w6/recursion_magic/\">...Recursion, Magic</a> by Eliezer Yudkowsky</li></ul><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/intelligence-explosion\">Intelligence explosion</a></li><li><a href=\"https://www.lesswrong.com/tag/singularity\">Singularity</a></li><li><a href=\"https://www.lesswrong.com/tag/seed-ai\">Seed AI</a></li><li><a href=\"https://www.lesswrong.com/tag/gödel-machine\">Gödel machine</a></li><li><a href=\"https://www.lesswrong.com/tag/ai-takeoff\">AI takeoff</a></li></ul><h2>External links</h2><ul><li><a href=\"http://intelligence.org/files/LOGI.pdf\">Seed AI</a> description from MIRI.</li><li><a href=\"http://intelligence.org/files/AIPosNegFactor.pdf\">Risks from Artificial Intelligence</a> by Eliezer Yudkowsky.</li><li><a href=\"http://www.xuenay.net/Papers/DigitalAdvantages.pdf\">Advantages of Artificial Intelligence</a> by Kaj Sotala</li><li><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf\">Speculations Concerning the First Ultraintelligent Machine</a> by I.J. Good</li></ul>",
    "description_length": 6122,
    "viewCount": 483,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "xjNvvmvQ5BH3cfEBr",
    "name": "Simulator Theory",
    "slug": "simulator-theory",
    "postCount": 102,
    "description_html": "<p>Simulator theory in the context of AI refers to an ontology or frame for understanding the working of large generative models, such as the GPT series from OpenAI. Broadly it views these models as simulating a learned distribution with various degrees of fidelity, which in the case of language models trained on a large corpus of text is the mechanics underlying our world.</p><p>It can also refer to an alignment research agenda, that deals with better understanding simulator conditionals, effects of downstream training, alignment-relevant properties such as myopia and agency in the context of language models, and using them as alignment research accelerators. See also: <a href=\"https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism\">Cyborgism</a></p>",
    "description_length": 766,
    "viewCount": 387,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "nJ2PSvptAg9X4wDGy",
    "name": "Sharp Left Turn",
    "slug": "sharp-left-turn",
    "postCount": 24,
    "description_html": "<p>A <strong>Sharp Left Turn</strong> is a scenario where, as an <a href=\"https://lesswrong.com/tag/ai\">AI</a> trains, its capabilities <a href=\"https://lesswrong.com/tag/general-intelligence\">generalize</a> across many domains while the alignment properties that held at earlier stages fail to generalize to the new domains.</p><p>See also: <a href=\"https://lesswrong.com/tag/threat-models\">Threat Models</a>, <a href=\"https://lesswrong.com/tag/ai-takeoff\">AI Takeoff</a>, <a href=\"https://lesswrong.com/tag/ai-risk\">AI Risk</a></p>",
    "description_length": 533,
    "viewCount": 217,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "bTeiZr6YAEaSPQTC8",
    "name": "Solomonoff Induction",
    "slug": "solomonoff-induction",
    "postCount": 69,
    "description_html": "<p><strong>Solomonoff induction </strong>is an inference system defined by <a href=\"https://en.wikipedia.org/wiki/Ray_Solomonoff\">Ray Solomonoff</a> that will learn to correctly predict any computable sequence with only the absolute minimum amount of data. This system, in a certain sense, is the perfect universal prediction algorithm.&nbsp;</p><p>To summarize it very informally, Solomonoff induction works by:</p><ul><li>Starting with all possible hypotheses (sequences) as represented by computer programs (that generate those sequences), weighted by their simplicity (2<sup>-</sup><strong><sup>n</sup></strong>, where <strong>n</strong> is the program length);</li><li>Discarding those hypotheses that are inconsistent with the data.</li></ul><p>Weighting hypotheses by simplicity, the system automatically incorporates a form of <a href=\"https://www.lesswrong.com/tag/occam-s-razor\">Occam's razor</a>, which is why it has been playfully referred to as <i>Solomonoff's lightsaber</i>.</p><p>Solomonoff induction gets off the ground with a solution to the \"problem of the priors\". Suppose that you stand before a universal <a href=\"http://www.scholarpedia.org/article/Algorithmic_complexity#Prefix_Turing_machine\">prefix Turing machine</a> <i>U</i>. You are interested in a certain finite output string <i>y</i><sub>0</sub>. In particular, you want to know the probability that <i>U</i> will produce the output <i>y</i><sub>0</sub> given a random input tape. This probability is the <strong>Solomonoff </strong><i><strong>a priori</strong></i><strong> probability</strong> of <i>y</i><sub>0</sub>.</p><p>More precisely, suppose that a particular infinite input string <i>x</i><sub>0</sub> is about to be fed into <i>U</i>. However, you know nothing about <i>x</i><sub>0</sub> other than that each term of the string is either 0 or 1. As far as your state of knowledge is concerned, the <i>i</i>th digit of <i>x</i><sub>0</sub> is as likely to be 0 as it is to be 1, for all <i>i</i> = 1, 2, …. You want to find the <i>a priori</i> probability <i>m</i>(<i>y</i><sub>0</sub>) of the following proposition:</p><p>(*) If <i>U</i> takes in <i>x</i><sub>0</sub> as input, then <i>U</i> will produce output <i>y</i><sub>0</sub> and then halt.</p><p>Unfortunately, computing the exact value of <i>m</i>(<i>y</i><sub>0</sub>) would require solving the halting problem, which is undecidable. Nonetheless, it is easy to derive an expression for <i>m</i>(<i>y</i><sub>0</sub>). If <i>U</i> halts on an infinite input string <i>x</i>, then <i>U</i> must read only a finite initial segment of <i>x</i>, after which <i>U</i> immediately halts. We call a finite string <i>p</i> a <i>self-delimiting program</i> if and only if there exists an infinite input string <i>x</i> beginning with <i>p</i> such that <i>U</i> halts on <i>x</i> immediately after reading to the end of <i>p</i>. The set 𝒫 of self-delimiting programs is the <i>prefix code</i> for <i>U</i>. It is the determination of the elements of 𝒫 that requires a solution to the halting problem.</p><p>Given <i>p</i> ∈ 𝒫, we write \"prog (<i>x</i><sub>0</sub>) = <i>p</i>\" to express the proposition that <i>x</i><sub>0</sub> begins with <i>p</i>, and we write \"<i>U</i>(<i>p</i>) = <i>y</i><sub>0</sub>\" to express the proposition that <i>U</i> produces output <i>y</i><sub>0</sub>, and then halts, when fed any input beginning with <i>p</i>. Proposition (*) is then equivalent to the exclusive disjunction</p><p><br>⋁<i><sub>p</sub></i><sub> ∈ 𝒫: </sub><i><sub>U</sub></i><sub>(</sub><i><sub>p</sub></i><sub>) = </sub><i><sub>y</sub></i><sub>0</sub>(prog (<i>x</i><sub>0</sub>) = <i>p</i>).<br>Since <i>x</i><sub>0</sub> was chosen at random from {0, 1}<i><sup>ω</sup></i>, we take the probability of prog (<i>x</i><sub>0</sub>) = <i>p</i> to be 2<sup> − ℓ(</sup><i><sup>p</sup></i><sup>)</sup>, where ℓ(<i>p</i>) is the length of <i>p</i> as a bit string. Hence, the probability of (*) is</p><p><br><i>m</i>(<i>y</i><sub>0</sub>) := ∑<i><sub>p</sub></i><sub> ∈ 𝒫: </sub><i><sub>U</sub></i><sub>(</sub><i><sub>p</sub></i><sub>) = </sub><i><sub>y</sub></i><sub>0</sub>2<sup> − ℓ(</sup><i><sup>p</sup></i><sup>)</sup>.<br>&nbsp;</p><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/kolmogorov-complexity\">Kolmogorov complexity</a></li><li><a href=\"https://www.lesswrong.com/tag/aixi\">AIXI</a></li><li><a href=\"https://www.lesswrong.com/tag/occam-s-razor\">Occam's razor</a></li></ul><h2>References</h2><ul><li><a href=\"http://www.scholarpedia.org/article/Algorithmic_probability\">Algorithmic probability</a> on Scholarpedia</li></ul>",
    "description_length": 4593,
    "viewCount": 578,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "5f5c37ee1b5cdee568cfb297",
    "name": "Superintelligence",
    "slug": "superintelligence",
    "postCount": 115,
    "description_html": "<p>A <strong>Superintelligence</strong> is a being with superhuman intelligence, and a focus of the <a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri\">Machine Intelligence Research Institute</a>'s research. Specifically, Nick Bostrom (1997) defined it as</p><blockquote><p>\"An intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.\"</p></blockquote><p>The <a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri\">Machine Intelligence Research Institute</a> is dedicated to ensuring humanity's safety and prosperity by preparing for the development of an <a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\">Artificial General Intelligence</a> with superintelligence. Given its intelligence, it is likely to be <a href=\"https://www.lesswrong.com/tag/ai-boxing-containment\">incapable of being controlled</a> by humanity. It is important to prepare early for the development of <a href=\"https://www.lesswrong.com/tag/friendly-artificial-intelligence\">friendly artificial intelligence</a>, as there may be an <a href=\"https://www.lesswrong.com/tag/ai-arms-race\">AI arms race</a>. A strong superintelligence is a term describing a superintelligence which is not designed with the same architecture as the human brain.</p><p>An <a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\">Artificial General Intelligence</a> will have a number of advantages aiding it in becoming a superintelligence. It can improve the hardware it runs on and obtain better hardware. It will be capable of directly editing its own code. Depending on how easy its code is to modify, it might carry out software improvements that <a href=\"https://www.lesswrong.com/tag/recursive-self-improvement\">spark further improvements</a>. Where a task can be accomplished in a repetitive way, a module preforming the task far more efficiently might be developed. Its motivations and preferences can be edited to be more consistent with each other. It will have an indefinite life span, be capable of reproducing, and transfer knowledge, skills, and code among its copies as well as cooperating and communicating with them better than humans do with each other.</p><p>The development of superintelligence from humans is another possibility, sometimes termed a weak superintelligence. It may come in the form of <a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\">whole brain emulation</a>, where a human brain is scanned and simulated on a computer. Many of the advantages a AGI has in developing superintelligence apply here as well. The development of <a href=\"https://www.lesswrong.com/tag/brain-computer-interfaces\">Brain-computer interfaces</a> may also lead to the creation of superintelligence. Biological enhancements such as genetic engineering and the use of nootropics could lead to superintelligence as well.</p><h2>External Links</h2><ul><li><a href=\"http://www.nickbostrom.com/superintelligence.html\">How long before Superintelligence?</a> by Nick Bostrom</li><li><a href=\"http://profhugodegaris.files.wordpress.com/2011/04/nocyborgsbghugo.pdf\">A discussion between Hugo de Garis and Ben Goertzel on superintelligence</a></li><li><a href=\"http://www.xuenay.net/Papers/DigitalAdvantages.pdf\">Advantages of Artificial Intelligences, Uploads, And Digital Minds</a> by Kaj Sotala</li></ul><h2>See Also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/brain-computer-interfaces\">Brain-computer interfaces</a></li><li><a href=\"https://www.lesswrong.com/tag/singularity\">Singularity</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Hard_takeoff\">Hard takeoff</a></li></ul>",
    "description_length": 3750,
    "viewCount": 239,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "tJv2Zbtx37mBGBJk6",
    "name": "Symbol Grounding",
    "slug": "symbol-grounding",
    "postCount": 26,
    "description_html": "<p><strong>Symbol Grounding</strong></p><p><strong>Related Pages:</strong> <a href=\"https://www.lesswrong.com/tag/truth-semantics-and-meaning\">Truth, Semantics, &amp; Meaning</a>, <a href=\"https://www.lesswrong.com/tag/philosophy-of-language\">Philosophy of Language</a></p>",
    "description_length": 273,
    "viewCount": 46,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "KRcsSxBTLGRrSbHsW",
    "name": "Transformative AI",
    "slug": "transformative-ai",
    "postCount": 31,
    "description_html": "<p><strong>Transformative AI</strong> is \"[...] AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution.\"<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhy8b4kflu8\"><sup><a href=\"#fnhy8b4kflu8\">[1]</a></sup></span>&nbsp;Unlike the related terms <a href=\"https://www.lesswrong.com/tag/superintelligence\">Superintelligent AI</a> and <a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\">Artificial General Intelligence</a>, which refer to specific capabilities the AI may have, Transformative AI refers to the effects the AI would have on humanity's well-being; through impacting the global economy, state power, international security, etc.</p><p>Holden Karnofsky gives a more detailed definition in <a href=\"https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/\">an OpenPhil 2016 post</a>:</p><blockquote><p>[...] Transformative AI is anything that fits one or more of the following descriptions (emphasis original):</p><ul><li>AI systems capable of fulfilling all the necessary functions of human scientists, unaided by humans, in developing another technology (or set of technologies) that ultimately becomes widely credited with being the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. Note that just because AI systems <i>could</i> accomplish such a thing unaided by humans doesn’t mean they <i>would</i>; it’s possible that human scientists would provide an important complement to such systems, and could make even faster progress working in tandem than such systems could achieve unaided. I emphasize the hypothetical possibility of AI systems conducting substantial unaided research to draw a clear distinction from the types of AI systems that exist today. I believe that AI systems capable of such broad contributions to the relevant research would likely dramatically accelerate it.</li><li>AI systems capable of performing tasks that currently (in 2016) account for the majority of full-time jobs worldwide, and/or over 50% of total world wages, unaided and for costs in the same range as what it would cost to employ humans. Aside from the fact that this would likely be sufficient for a major economic transformation relative to today, I also think that an AI with such broad abilities would likely be able to far surpass human abilities in a subset of domains, making it likely to meet one or more of the other criteria laid out here.</li><li>Surveillance, autonomous weapons, or other AI-centric technology that becomes sufficiently advanced to be the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. (This contrasts with the first point because it refers to transformative technology that is itself AI-centric, whereas the first point refers to AI used to speed research on some other transformative technology.)</li></ul></blockquote><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhy8b4kflu8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhy8b4kflu8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>As defined by <a href=\"https://www.openphilanthropy.org/research/potential-risks-from-advanced-artificial-intelligence-the-philanthropic-opportunity/\">Open Philanthropy's Holden Karnofsky in 2016</a>, and reused by <a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf\">the Center for the Governance of AI in 2018</a></p></div></li></ol>",
    "description_length": 3648,
    "viewCount": 229,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "qNF7Ti87CLfHhbttj",
    "name": "Treacherous Turn",
    "slug": "treacherous-turn",
    "postCount": 17,
    "description_html": "<p>A <strong>Treacherous Turn</strong> is a hypothetical event where an advanced <a href=\"ai\">AI</a> system which has been pretending to be aligned due to its relative weakness turns on humanity once it achieves sufficient power that it can pursue its true objective without risk.</p>",
    "description_length": 284,
    "viewCount": 158,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "HAFdXkW4YW4KRe2Gx",
    "name": "Utility Functions",
    "slug": "utility-functions",
    "postCount": 187,
    "description_html": "<p>A <strong>utility function</strong> assigns numerical values (\"utilities\") to outcomes, in such a way that outcomes with higher utilities are absolutely always <u><a href=\"http://lesswrong.com/tag/preference\">preferred</a></u> to outcomes with lower utilities, with no exceptions; the lack of exploitable holes in the preference ordering is necessary for the definition and separates utility from mere reward.</p><p><em>See also: </em><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value</a>, <a href=\"https://www.lesswrong.com/tag/decision-theory\">Decision Theory</a>, <a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory</a>, <a href=\"https://www.lesswrong.com/tag/orthogonality-thesis/\">Orthogonality Thesis</a>, <a href=\"http://lesswrong.com/tag/utilitarianism\">Utilitarianism</a>, <a href=\"https://www.lesswrong.com/tag/preference\">Preference</a>, <a href=\"https://www.lesswrong.com/tag/utility\">Utility</a>, <a href=\"https://www.lesswrong.com/tag/vnm-theorem\">VNM Theorem</a></p><p>Utility Functions do not work very well in practice for individual humans. Human drives are not coherent nor is there any reason to think they would converge to a utility-function-grade level of reliability (<a href=\"https://www.lesswrong.com/lw/l3/thou_art_godshatter/\">Thou Art Godshatter</a>), and even people with a strong interest in the concept have trouble working out what their utility function actually is even slightly (<a href=\"https://www.lesswrong.com/lw/zv/post_your_utility_function/\">Post Your Utility Function</a>). Furthermore, humans appear to calculate reward and loss separately - adding one to the other does not predict their behavior accurately, and thus human reward is not human utility. This makes humans highly exploitable - and in fact, not being exploitable would be a minimum requirement in order to qualify as having a coherent utility function.</p><p><a href=\"https://www.lesswrong.com/users/pjeby\">pjeby</a> posits humans' difficulty in understanding their own utility functions as the root of <a href=\"https://www.lesswrong.com/tag/akrasia\">akrasia</a>.</p><p>However, utility functions can be a useful model for dealing with humans in groups, <em>e.g.</em> in economics.</p><p>The <a href=\"https://www.lesswrong.com/tag/vnm-theorem\">VNM Theorem</a> tag is likely to be a strict subtag of the Utility Functions tag, because the VNM theorem establishes when preferences can be represented by a utility function, but a post discussing utility functions may or may not discuss the VNM theorem/axioms.</p><p>Because utility functions arise from VNM rationality, they may still be of note in understanding intelligent systems even when the system does not explicitly store a utility function anywhere, since reducing exploitable error rate should eventually converge to utility-function-like guarantees.</p>",
    "description_length": 2945,
    "viewCount": 335,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "5f5c37ee1b5cdee568cfb2b1",
    "name": "Whole Brain Emulation",
    "slug": "whole-brain-emulation",
    "postCount": 118,
    "description_html": "<p><strong>Whole Brain Emulation</strong> or <strong>WBE</strong> is a proposed technique which involves transferring the information contained within a brain onto a computing substrate. The brain can then be simulated, creating a machine intelligence. The concept is often discussed in context of scanning the brain of a person, known as <a href=\"https://www.lesswrong.com/tag/mind-uploading\">Mind Uploading</a>.</p><p>WBE is sometimes seen as an easy way to creating intelligent computers, as the only innovations necessary are greatly increased processor speed and scanning resolution. Advocates of WBE claim technological improvement rates such as <a href=\"https://wiki.lesswrong.com/wiki/Moore's_law\">Moore's law</a> will make WBE inevitable.</p><p>The exact level of detail required for an accurate simulation of a brain's mind is presently uncertain, and will determine the difficulty of creating WBE. The feasibility of such a project has been examined in detail in <a href=\"https://www.lesswrong.com/tag/future-of-humanity-institute-fhi\">Future of Humanity Institute</a>'s <a href=\"https://www.lesswrong.com/tag/brain-emulation-roadmap\">Whole Brain Emulation: A Roadmap</a>. The Roadmap concluded that a human brain emulation would be possible before mid-century, providing that current technology trends kept up and providing that there would be sufficient investments.</p><p>Several approaches for WBE have been suggested:</p><ul><li>A brain could be cut into small slices, which would then be scanned into a computer.<a href=\"#fn1\"><sup>1</sup></a></li><li><a href=\"https://www.lesswrong.com/tag/brain-computer-interfaces\">Brain-computer interfaces</a> could slowly replace portions of the brain with computers and allow the mind to grow onto a computing substrate.<a href=\"#fn2\"><sup>2</sup></a><a href=\"#fn3\"><sup>3</sup></a></li><li>Resources such as personality tests and a person's writings could be used to construct a model of the person.<a href=\"#fn4\"><sup>4</sup></a></li></ul><p>A digitally emulated brain could have several advantages over a biological one<a href=\"#fn5\"><sup>5</sup></a>. It might be able to run faster than biological brains, copy itself, and take advantage of backups while experimenting with self-modification.</p><p>Whole brain emulation will also create a number of ethical challenges relating to the nature of personhood, rights, and social inequality. <a href=\"https://www.lesswrong.com/tag/robin-hanson\">Robin Hanson</a> proposes that an uploaded mind <a href=\"https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation\">might copy itself to work until the cost of running a copy was that of its labour</a>, vastly increasing the amount of wealth in the world but also causing mass unemployment<a href=\"#fn6\"><sup>6</sup></a>. The ability to copy uploads could also lead to drastic changes in society's values, with the values of the uploads that got copied the most coming to dominate.</p><p>An emulated-brain populated world could hold severe negative consequences, such as:</p><ul><li>Inherent inability to have consciousness, if some philosophers are right <a href=\"#fn7\"><sup>7</sup></a> <a href=\"#fn8\"><sup>8</sup></a> <a href=\"#fn9\"><sup>9</sup></a> <a href=\"#fn10\"><sup>10</sup></a>.</li><li>Elimination of culture in general, due to an extremely increasing penalty for inefficiency in the form of flamboyant displays <a href=\"#fn11\"><sup>11</sup></a></li><li>Near zero costs for reproduction, pushing most of <a href=\"https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation\">emulations to live in a subsistence state</a>. <a href=\"#fn12\"><sup>12</sup></a></li></ul><h2>See Also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation\">Economic consequences of AI and whole brain emulation</a></li><li><a href=\"https://www.lesswrong.com/tag/emulation-argument-for-human-level-ai\">Emulation argument for human-level AI</a></li><li><a href=\"https://www.lesswrong.com/tag/simulation-hypothesis\">Simulation hypothesis</a></li><li><a href=\"https://www.lesswrong.com/tag/neuromorphic-ai\">Neuromorphic AI</a></li></ul><h2>External Links</h2><ul><li><a href=\"http://www.amazon.com/The-Singularity-Is-Near-Transcend/dp/0143037889/\">The Singularity is near: When humans transcend biology</a> by Ray Kurzweil</li><li><a href=\"https://www.lesswrong.com/tag/brain-emulation-roadmap\">Whole Brain Emulation: A Roadmap</a>. Report by The Future of Humanity Institute.</li><li><a href=\"http://www.jetpress.org/volume1/moravec.htm\">Hans Moravec's Estimation of Human Brain Processing Capacity</a></li><li><a href=\"http://www.patternsinthevoid.net/blog/wp-content/uploads/2010/12/2009-A-world-survey-of-artificial-brain-projects-Part1_Large-scale-brain-simulations.pdf\">A world survey of artificial brain projects, Part I: Large-scale brain simulations</a> by Hugo de Garis, Chen Shuo, Ben Goertzel and, Lian Ruiting, 2010</li><li><a href=\"http://hanson.gmu.edu/uploads.html\">If Uploads Come First: The crack of a future dawn</a> by Robin Hanson</li><li><a href=\"http://intelligence.org/files/WBE-Superorgs.pdf\">Whole Brain Emulation and the Evolution of Superorganisms</a></li><li><a href=\"http://wp.goertzel.org/?page_id=368\">International Journal of Machine Consciousness Special Issue on Mind Uploading</a></li><li><a href=\"http://www.sim.me.uk/neural/JournalArticles/Bamford2012IJMC.pdf\">A framework for approaches to transfer of a mind's substrate</a> by Sim Bamford</li><li><a href=\"http://www.xuenay.net/Papers/CoalescingMinds.pdf\">Coalescing Minds: Brain Uploading-related Group Mind Scenarios</a> by Kaj Sotala and Harri Valpola</li></ul><h2>References</h2><ol><li><a href=\"https://www.lesswrong.com/tag/brain-emulation-roadmap\">Whole Brain Emulation: A Roadmap</a><a href=\"#fnref1\">↩</a></li><li>Strout, J. Uploading by the Nanoreplacement Procedure. <a href=\"http://www.ibiblio.org/jstrout/uploading/nanoreplacement.html\">http://www.ibiblio.org/jstrout/uploading/nanoreplacement.html</a><a href=\"#fnref2\">↩</a></li><li>Sotala, K., &amp; Valpola, H. (2012). Coalescing minds: brain uploading-related group mind scenarios. International Journal of Machine Consciousness, 4(01), 293-312. <a href=\"http://singularity.org/files/CoalescingMinds.pdf\">http://singularity.org/files/CoalescingMinds.pdf</a><a href=\"#fnref3\">↩</a></li><li>ROTHBLATT, M. (2012). THE TERASEM MIND UPLOADING EXPERIMENT. International Journal of Machine Consciousness, 4(01), 141-158. <a href=\"http://www.terasemcentral.org/docs/Terasem%20Mind%20Uploading%20Experiment%20IJMC.pdf\">http://www.terasemcentral.org/docs/Terasem%20Mind%20Uploading%20Experiment%20IJMC.pdf</a><a href=\"#fnref4\">↩</a></li><li>Sotala, K. (2012). Advantages of artificial intelligences, uploads, and digital minds. International Journal of Machine Consciousness, 4(01), 275-291. <a href=\"http://singularity.org/files/AdvantagesOfAIs.pdf\">http://singularity.org/files/AdvantagesOfAIs.pdf</a><a href=\"#fnref5\">↩</a></li><li>Hanson, R. (1994). If uploads come first. Extropy, 6(2), 10-15. <a href=\"http://hanson.gmu.edu/uploads.html\">http://hanson.gmu.edu/uploads.html</a><a href=\"#fnref6\">↩</a></li><li>LUCAS, John. (1961) Minds, machines, and Gödel, Philosophy, 36, pp. 112–127<a href=\"#fnref7\">↩</a></li><li>DREYFUS, H. (1972) What Computers Can’t Do, New York: Harper &amp; Row.<a href=\"#fnref8\">↩</a></li><li>PENROSE, Roger (1994) Shadows of the Mind, Oxford: Oxford University Press.<a href=\"#fnref9\">↩</a></li><li>BLOCK, Ned (1981) Psychologism and behaviorism, Philosophical Review, 90, pp. 5–43.<a href=\"#fnref10\">↩</a></li><li>BOSTROM, Nick.(2004) \"The future of human evolution\". Death and Anti‐Death: Two Hundred Years After Kant, Fifty Years After Turing, ed. Charles Tandy (Ria University Press: Palo Alto, California, 2004): pp. 339‐371. Available at: <a href=\"http://www.nickbostrom.com/fut/evolution.pdf\">http://www.nickbostrom.com/fut/evolution.pdf</a><a href=\"#fnref11\">↩</a></li></ol>",
    "description_length": 7994,
    "viewCount": 279,
    "parentTagId": "ai-basic-alignment-theory"
  },
  {
    "core-tag": "AI",
    "_id": "RyNWXFjKNcafRKvPh",
    "name": "Agent Foundations",
    "slug": "agent-foundations",
    "postCount": 105,
    "description_html": null,
    "description_length": null,
    "viewCount": 441,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "c42eTtBCXyJmtpqwZ",
    "name": "AI-Assisted Alignment",
    "slug": "ai-assisted-alignment",
    "postCount": 99,
    "description_html": "<p><strong>AI-Assisted Alignment </strong>is a cluster of alignment plans that involve AI somehow significantly helping with alignment research. This can include weak <a href=\"https://www.lesswrong.com/tag/tool-ai\">tool AI</a>, or more advanced AGI doing original research.</p><p>There has been a lot of debate about how practical this alignment approach is.</p><p>Other search terms for this tag: AI aligning AI</p>",
    "description_length": 416,
    "viewCount": 63,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "zCYXpx33wq8chGyEz",
    "name": "AI Boxing (Containment)",
    "slug": "ai-boxing-containment",
    "postCount": 88,
    "description_html": "<p><strong>AI Boxing </strong>is attempts, experiments, or proposals to isolate (\"box\") a powerful AI (~AGI) where it can't interact with the world at large, save for limited communication with its human liaison. It is often proposed that so long as the AI is physically isolated and restricted, or \"boxed\", it will be harmless even if it is an <a href=\"https://www.lesswrong.com/tag/unfriendly-artificial-intelligence\">unfriendly artificial intelligence</a> (UAI).</p><p>Challenges are: 1) can you successively prevent it from interacting with the world? And 2) can you prevent it from convincing you to let it out?</p><p><strong>See also:</strong> <a href=\"https://www.lesswrong.com/tag/ai\">AI</a>, <a href=\"https://wiki.lesswrong.com/wiki/AGI\">AGI</a>, <a href=\"https://www.lesswrong.com/tag/oracle-ai\">Oracle AI</a>, <a href=\"https://www.lesswrong.com/tag/tool-ai\">Tool AI</a>, <a href=\"https://wiki.lesswrong.com/wiki/Unfriendly_AI\">Unfriendly AI</a></p><h1>Escaping the box</h1><p>It is not regarded as likely that an AGI can be boxed in the long term. Since the AGI might be a <a href=\"https://www.lesswrong.com/tag/superintelligence\">superintelligence</a>, it could persuade someone (the human liaison, most likely) to free it from its box and thus, human control. Some practical ways of achieving this goal include:</p><ul><li>Offering enormous wealth, power and intelligence to its liberator</li><li>Claiming that only it can prevent an <a href=\"https://www.lesswrong.com/tag/existential-risk\">existential risk</a></li><li>Claiming it needs outside resources to cure all diseases</li><li>Predicting a real-world disaster (which then occurs), then claiming it could have been prevented had it been let out</li></ul><p>Other, more speculative ways include: threatening to torture millions of conscious copies of you for thousands of years, starting in exactly the same situation as in such a way that it seems overwhelmingly likely that <a href=\"https://www.lesswrong.com/tag/simulation-argument\">you are a simulation</a>, or it might discover and exploit unknown physics to free itself.</p><h1>Containing the AGI</h1><p>Attempts to box an AGI may add some degree of safety to the development of a <a href=\"https://wiki.lesswrong.com/wiki/FAI\">friendly artificial intelligence</a> (FAI). A number of strategies for keeping an AGI in its box are discussed in <a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Thinking inside the box</a> and <a href=\"http://dl.dropbox.com/u/5317066/2012-yampolskiy.pdf\">Leakproofing the Singularity</a>. Among them are:</p><ul><li>Physically isolating the AGI and permitting it zero control of any machinery</li><li>Limiting the AGI’s outputs and inputs with regards to humans</li><li>Programming the AGI with deliberately convoluted logic or <a href=\"http://en.wikipedia.org/wiki/Homomorphic_encryption\">homomorphically encrypting</a> portions of it</li><li>Periodic resets of the AGI's memory</li><li>A virtual world between the real world and the AI, where its unfriendly intentions would be first revealed</li><li>Motivational control using a variety of techniques</li><li>Creating an <a href=\"https://www.lesswrong.com/tag/oracle-ai\">Oracle AI</a>: an AI that only answers questions and isn't designed to interact with the world in any other way. But even the act of the AI putting strings of text in front of humans poses some risk.</li></ul><h1>Simulations / Experiments</h1><p>The <strong>AI Box Experiment</strong> is a game meant to explore the possible pitfalls of AI boxing. It is played over text chat, with one human roleplaying as an AI in a box, and another human roleplaying as a gatekeeper with the ability to let the AI out of the box. The AI player wins if they successfully convince the gatekeeper to let them out of the box, and the gatekeeper wins if the AI player has not been freed after a certain period of time.&nbsp;</p><p>Both Eliezer Yudkowsky and Justin Corwin have ran simulations, pretending to be a <a href=\"https://www.lesswrong.com/tag/superintelligence\">superintelligence</a>, and been able to convince a human playing a guard to let them out on many - but not all - occasions. Eliezer's five experiments required the guard to listen for at least two hours with participants who had approached him, while Corwin's 26 experiments had no time limit and subjects he approached.</p><p>The text of Eliezer's experiments have not been made public.</p><h2>List of experiments</h2><ul><li><a href=\"http://yudkowsky.net/singularity/aibox/\">The AI-Box Experiment</a> <a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky's</a> original two tests</li><li><a href=\"https://www.lesswrong.com/lw/up/shut_up_and_do_the_impossible/\">Shut up and do the impossible!</a>, three other experiments Eliezer ran</li><li><a href=\"http://www.sl4.org/archive/0207/4935.html\">AI Boxing</a>, 26 trials ran by Justin Corwin</li><li><a href=\"https://www.lesswrong.com/lw/9ld/ai_box_log/\">AI Box Log</a>, a log of a trial between MileyCyrus and Dorikka</li></ul><h1>References</h1><ul><li><a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Thinking inside the box: using and controlling an Oracle AI</a> by Stuart Armstrong, Anders Sandberg, and Nick Bostrom</li><li><a href=\"http://dl.dropbox.com/u/5317066/2012-yampolskiy.pdf\">Leakproofing the Singularity: Artificial Intelligence Confinement Problem</a> by Roman V. Yampolskiy</li><li><a href=\"http://ordinaryideas.wordpress.com/2012/04/27/on-the-difficulty-of-ai-boxing/\">On the Difficulty of AI Boxing</a> by Paul Christiano</li><li><a href=\"https://www.lesswrong.com/lw/3cz/cryptographic_boxes_for_unfriendly_ai/\">Cryptographic Boxes for Unfriendly AI</a> by Paul Christiano</li><li><a href=\"https://www.lesswrong.com/r/lesswrong/lw/12s/the_strangest_thing_an_ai_could_tell_you/\">The Strangest Thing An AI Could Tell You</a></li><li><a href=\"https://www.lesswrong.com/lw/1pz/ai_in_box_boxes_you/\">The AI in a box boxes you</a></li></ul>",
    "description_length": 5959,
    "viewCount": 624,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "KkksuGB2yBR6LDFXu",
    "name": "Conservatism (AI)",
    "slug": "conservatism-ai",
    "postCount": 9,
    "description_html": null,
    "description_length": null,
    "viewCount": 29,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "mSTmKrSkFBswHaS3T",
    "name": "Eliciting Latent Knowledge (ELK)",
    "slug": "eliciting-latent-knowledge-elk",
    "postCount": 104,
    "description_html": "<p><strong>Eliciting Latent Knowledge</strong> is an open problem in <a href=\"ai\">AI</a> safety.</p><blockquote><p>Suppose we train a model to predict what the future will look like according to cameras and other sensors. We then use planning algorithms to find a sequence of actions that lead to predicted futures that look good to us.</p><p>But some action sequences could tamper with the cameras so they show happy humans regardless of what’s really happening. More generally, some futures look great on camera but are actually catastrophically bad.</p><p>In these cases, the prediction model \"knows\" facts (like \"the camera was tampered with\") that are not visible on camera but would change our evaluation of the predicted future if we learned them.&nbsp;<strong>How can we train this model to report its latent knowledge of off-screen events?</strong></p></blockquote><p><a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit\">--ARC report</a></p><p>See also: <a href=\"https://www.lesswrong.com/tag/transparency-interpretability-ml-and-ai\">Transparency/Interpretability</a></p>",
    "description_length": 1122,
    "viewCount": 303,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "EY623WWCXKTvN3kmj",
    "name": "Factored Cognition",
    "slug": "factored-cognition",
    "postCount": 38,
    "description_html": "<html><head></head><body><p><strong>Factored cognition</strong> is an approach to artificial intelligence where sophisticated learning and reasoning is broken down (or factored) into many small and mostly independent tasks [<a href=\"https://ought.org/research/factored-cognition\">1</a>].</p><p>Factored Cognition is related to <a href=\"https://www.lesswrong.com/tag/iterated-amplification\"><i>Iterated Amplification</i> (IDA)</a>.</p></body></html>",
    "description_length": 448,
    "viewCount": 74,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "cPFuhAE7PwoKF7yTj",
    "name": "Inverse Reinforcement Learning",
    "slug": "inverse-reinforcement-learning",
    "postCount": 35,
    "description_html": "<p>From ChatGPT(4):<br><br>Inverse Reinforcement Learning (IRL) is a technique in the field of machine learning where an AI system learns the preferences or objectives of an agent, typically a human, by observing their behavior. Unlike traditional Reinforcement Learning (RL), where an agent learns to optimize its actions based on given reward functions, IRL works by inferring the underlying reward function from the demonstrated behavior.</p><p>In other words, IRL aims to understand the motivations and goals of an agent by examining their actions in various situations. Once the AI system has learned the inferred reward function, it can then use this information to make decisions that align with the preferences or objectives of the observed agent.</p><p>IRL is particularly relevant in the context of AI alignment, as it provides a potential approach to align AI systems with human values. By learning from human demonstrations, AI systems can be designed to better understand and respect the preferences, intentions, and values of the humans they interact with or serve.<br><br>(Cunningham law this if you please, it was empty when I came across it and I thought something better than nothing.)</p>",
    "description_length": 1207,
    "viewCount": 93,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "4mRJmYxNDnn7r2gNu",
    "name": "Iterated Amplification ",
    "slug": "iterated-amplification",
    "postCount": 66,
    "description_html": "<p><strong>Iterated Amplification </strong>is an approach to AI alignment, spearheaded by Paul Christiano. In this setup, we build powerful, aligned ML systems through a process of initially building weak aligned AIs, and recursively using each new AI to build a slightly smarter and still aligned AI.&nbsp;</p><p>See also: <a href=\"https://www.lesswrong.com/tag/factored-cognition\">Factored cognition</a>.&nbsp;</p>",
    "description_length": 416,
    "viewCount": 101,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "8KQvnMQYGaiCAqrXv",
    "name": "Mild Optimization",
    "slug": "mild-optimization",
    "postCount": 28,
    "description_html": null,
    "description_length": null,
    "viewCount": 53,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "5f5c37ee1b5cdee568cfb26d",
    "name": "Oracle AI",
    "slug": "oracle-ai",
    "postCount": 86,
    "description_html": "<p>An <strong>Oracle AI</strong> is a regularly proposed solution to the problem of developing <a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a>. It is conceptualized as a super-intelligent system which is designed for only answering questions, and has no ability to act in the world. The name was first suggested by <a href=\"https://www.lesswrong.com/tag/nick-bostrom\">Nick Bostrom</a>.</p><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/instrumental-convergence\">Basic AI drives</a></li><li><a href=\"https://www.lesswrong.com/tag/tool-ai\">Tool AI</a></li><li><a href=\"https://www.lesswrong.com/tag/utility-indifference\">Utility indifference</a></li><li><a href=\"https://www.lesswrong.com/tag/ai-boxing-containment\">AI Boxing</a></li></ul><h1>Safety</h1><p>The question of whether Oracles – or just <a href=\"https://www.lesswrong.com/tag/ai-boxing-containment\">keeping an AGI forcibly confined</a> - are safer than fully free AGIs has been the subject of debate for a long time. Armstrong, Sandberg and Bostrom discuss Oracle safety at length in their <a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Thinking inside the box: using and controlling an Oracle AI</a>. In the paper, the authors review various methods which might be used to measure an Oracle's accuracy. They also try to shed some light on some weaknesses and dangers that can emerge on the human side, such as psychological vulnerabilities which can be exploited by the Oracle through social engineering. The paper discusses ideas for physical security (“boxing”), as well as problems involved with trying to program the AI to only answer questions. In the end, the paper reaches the cautious conclusion of Oracle AIs probably being safer than free AGIs.</p><p>In a related work, <a href=\"http://lesswrong.com/lw/tj/dreams_of_friendliness/\">Dreams of Friendliness</a>, <a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky</a> gives an informal argument stating that all oracles will be agent-like, that is, driven by its own goals. He rests on the idea that anything considered \"intelligent\" must choose the correct course of action among all actions available. That means that the Oracle will have many possible things to believe, although very few of them are correct. Therefore believing the correct thing means some method was used to select the correct belief from the many incorrect beliefs. By definition, this is an <a href=\"https://www.lesswrong.com/tag/optimization\">optimization process</a> which has a goal of selecting correct beliefs.</p><p>One can then imagine all the things that might be useful in achieving the goal of \"have correct beliefs\". For instance, <a href=\"https://www.lesswrong.com/tag/instrumental-convergence\">acquiring more computing power and resources</a> could help this goal. As such, an Oracle could determine that it might answer more accurately and easily to a certain question if it turned all matter outside the box to <a href=\"https://www.lesswrong.com/tag/computronium\">computronium</a>, therefore killing all the existing life.</p><h1>Taxonomy</h1><p>Based on an old draft by Daniel Dewey, Luke Muehlhauser has <a href=\"http://lesswrong.com/lw/any/a_taxonomy_of_oracle_ais/\">published</a> a possible taxonomy of Oracle AIs, broadly divided between True Oracular AIs and Oracular non-AIs.</p><h2>True Oracular AIs</h2><p>Given that true AIs are goal-oriented agents, it follows that a True Oracular AI has some kind of oracular goals. These act as the motivation system for the Oracle to give us the information we ask and nothing else.</p><p>It is first noted that such a True AI is not actually nor causally isolated from the world, as it has at least an input (questions and information) and an output (answers) channel. Since we expect such an intelligent agent to be able to have a deep impact on the world even through these limited channels, it can only be safe if its goals are fully compatible with human goals.</p><p>This means that a True Oracular AI has to have a full specification of human values, thus making it a <a href=\"https://www.lesswrong.com/tag/fai-complete\">FAI-complete</a> problem – if we could achieve such skill and knowledge we could just build a Friendly AI and bypass the Oracle AI concept.</p><h2>Oracular non-AIs</h2><p>Any system that acts only as an informative machine, only answering questions and has no goals is by definition not an AI at all. That means that a non-AI Oracular is but a calculator of outputs based on inputs. Since the term in itself is heterogeneous, the proposals made for a sub-division are merely informal.</p><p>An <i>Advisor</i> can be seen as a system that gathers data from the real world and computes the answer to an informal “what we ought to do?” question. They also represent a FAI-complete problem.</p><p>A <i>Question-Answerer</i> is a similar system that gathers data from the real world but coupled with a question. It then somehow computes the answer. The difficulty can lay on distinguishing it from an Advisor and controlling the safety of its answers.</p><p>Finally, a <i>Predictor</i> is seen as a system that takes a corpus of data and produces a probability distribution over future possible data. There are some proposed dangers with predictors, namely exhibiting goal-seeking behavior which does not converge with humanity goals and the ability to influence us through the predictions.</p><h2>Further reading &amp; References</h2><ul><li><a href=\"http://lesswrong.com/lw/tj/dreams_of_friendliness/\">Dreams of Friendliness</a></li><li><a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Thinking inside the box: using and controlling an Oracle AI</a> by Armstrong, Sandberg and <a href=\"https://www.lesswrong.com/tag/nick-bostrom\">Bostrom</a></li></ul>",
    "description_length": 5799,
    "viewCount": 143,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "CyFfBfRAm7pP83r5p",
    "name": "Reward Functions",
    "slug": "reward-functions",
    "postCount": 37,
    "description_html": null,
    "description_length": null,
    "viewCount": 44,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "wqeBNjndX7egbzQrW",
    "name": "RLHF",
    "slug": "rlhf",
    "postCount": 83,
    "description_html": "<p><strong>Reinforcement Learning from Human Feedback (RLHF)</strong> is a <a href=\"https://lesswrong.com/tag/machine-learning\">machine learning</a> technique where the model's training signal uses human evaluations of the model's outputs, rather than labeled data or a ground truth reward signal.</p>",
    "description_length": 301,
    "viewCount": 188,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "zuwsLCbxbugqB7FQY",
    "name": "Shard Theory",
    "slug": "shard-theory",
    "postCount": 60,
    "description_html": "<p><strong>Shard theory</strong> is an alignment research program, about the relationship between training variables and learned values in trained <a href=\"https://lesswrong.com/tag/reinforcement-learning\">Reinforcement Learning (RL)</a> agents. It is thus an approach to progressively fleshing out a mechanistic account of <a href=\"https://lesswrong.com/tag/human-values\">human values</a>, learned values in RL agents, and (to a lesser extent) the learned algorithms in <a href=\"https://lesswrong.com/tag/machine-learning-ml\">ML</a> generally.</p><p>Shard theory's basic ontology of RL holds that <i>shards</i> are contextually activated, behavior-steering computations in neural networks (biological and artificial). The circuits that implement a shard that garners reinforcement are reinforced, meaning that that shard will be more likely to trigger again in the future, when given similar cognitive inputs.</p><p>As an appreciable fraction of a neural network is composed of shards, large neural nets can possess quite intelligent constituent shards. These shards can be sophisticated enough to be well-modeled as playing negotiation games with each other, (potentially) explaining human psychological phenomena like <a href=\"https://lesswrong.com/tag/akrasia\">akrasia</a> and value changes from moral reflection. Shard theory also suggests an approach to explaining the shape of human values, and a scheme for RL alignment.</p>",
    "description_length": 1432,
    "viewCount": 330,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "LXk7bxNkYSjgatdAt",
    "name": "Tool AI",
    "slug": "tool-ai",
    "postCount": 45,
    "description_html": "<p>A <strong>tool AI</strong> is a type of Artificial Intelligence that is built to be used as a tool by the creators, rather than being an agent with its own action and goal-seeking behavior.</p><p>Generally meant to refer to <u><a href=\"https://wiki.lesswrong.com/wiki/AGI\">AGI</a></u>, tool AI is a proposed method for gaining some of the benefits of the intelligence while avoiding the dangers of having it act autonomously. It was coined by Holden Karnofsky, co-founder of GiveWell, in a critique of the Singularity Institute. Karnofsky proposed that, while he agreed that agent-based AGI was dangerous, it was an unnecessary path of development. His example of tool AI behavior was Google Maps, which uses complex algorithms and data to plot a route, but presents these results to the user instead of driving the user itself.</p><p>Eliezer Yudkowsky responded to this by enumerating several ways in which tool AI had similar difficulties in technical specification and safety. He also pointed out that it was not a common proposal among leading AGI thinkers.</p><h2><strong>See Also</strong></h2><ul><li><a href=\"https://www.lesswrong.com/tag/oracle-ai\">Oracle AI</a></li></ul><h2><strong>External Links</strong></h2><ul><li><a href=\"http://groups.yahoo.com/group/givewell/message/287\">Conversation between Holden Karnofsky and Jaan Tallinn</a></li></ul>",
    "description_length": 1360,
    "viewCount": 799,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "KqfqD7YSMeFTLJCcs",
    "name": "Tripwire",
    "slug": "tripwire",
    "postCount": 10,
    "description_html": "<p>In AI safety, a <strong>tripwire</strong> is a mechanism designed to detect signs of misalignment in an advanced artificial intelligence and shut it down automatically.</p>",
    "description_length": 175,
    "viewCount": 38,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "NLwTnsH9RSotqXYLw",
    "name": "Value Learning",
    "slug": "value-learning",
    "postCount": 185,
    "description_html": "<p><strong>Value learning</strong> is a proposed method for incorporating human values in an <a href=\"https://wiki.lesswrong.com/wiki/AGI\">AGI</a>. It involves the creation of an artificial learner whose actions consider many possible sets of values and preferences, weighed by their likelihood. Value learning could prevent an AGI of having goals detrimental to human values, hence helping in the creation of <a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a>.</p>\n<p>Many ways have been proposed to incorporate human values in an AGI (e.g.: <a href=\"https://lesswrong.com/tag/coherent-extrapolated-volition\">Coherent Extrapolated Volition</a>, <a href=\"https://lesswrong.com/tag/coherent-aggregated-volition\">Coherent Aggregated Volition</a> and <a href=\"https://lesswrong.com/tag/coherent-blended-volition\">Coherent Blended Volition</a>, mostly proposed around 2004-2010). Value learning was suggested in 2011 by <a href=\"http://www.futuretech.ox.ac.uk/daniel-dewey\">Daniel Dewey</a> in <a href=\"http://www.danieldewey.net/learning-what-to-value.pdf\">‘Learning What to Value’</a>. Like most authors, he assumes that an artificial agent needs to be intentionally aligned to human goals. First, Dewey argues against the use of a simple use of <a href=\"https://lesswrong.com/tag/reinforcement-learning\">reinforcement learning</a> to solve this problem, on the basis that this lead to the maximization of specific rewards that can diverge from value maximization. For example, this could suffer from goal misspecification or reward hacking. He proposes a <a href=\"https://lesswrong.com/tag/utility-functions\">utility function</a> maximizer comparable to AIXI, which considers all possible utility functions weighted by their Bayesian probabilities: \"[W]e propose uncertainty over utility functions. Instead of providing an agent one utility function up front, we provide an agent with a pool of possible utility functions and a probability distribution P such that each utility function can be assigned probability P(Ujyxm) given a particular interaction history [yxm]. An agent can then calculate an expected value over possible utility functions given a particular interaction history\"</p>\n<p>Nick Bostrom also discusses value learning at length in his book <a href=\"https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742\">Superintelligence</a>. Value learning is closely related to various proposals for <a href=\"https://www.lesswrong.com/tag/ai-assisted-alignment-1\">AI-assisted Alignment</a> and <a href=\"https://www.lesswrong.com/tag/ai-assisted-ai-automated-alignment\">AI-assisted/AI automated Alignment</a> research. Since human values are <a href=\"https://www.lesswrong.com/tag/complexity-of-value\">complex</a> and <a href=\"https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile\">fragile</a>, learning human values well is a challenging problem, much like AI-assisted Alignment (but in a less supervised setting, so actually harder). So this is only a practicable alignment technique for AGI <a href=\"https://www.lesswrong.com/posts/q9yPYG2St2L4SEtKW/requirements-for-a-stem-capable-agi-value-learner-my-case-1\">capable of successfully performing a STEM research</a> program (in Anthropology). Thus value learning is (unusually) an alignment technique that improves as capabilities increase, and it requires around an AGI minimum threshold of capabilities to begin to be effective.</p>\n<p>One potential challenge is that <a href=\"https://www.lesswrong.com/posts/R36DmF4Md9Zq6odFN/5-the-mutable-values-problem-in-value-learning-and-cev\">human values are somewhat mutable</a> and <a href=\"https://www.lesswrong.com/s/3QXNgNKXoLrdXJwWE\">AGI could affect them</a>.</p>\n<h2>References</h2>\n<ul>\n<li><a href=\"http://www.danieldewey.net/learning-what-to-value.pdf\">Dewey’s paper</a></li>\n</ul>\n<h2>See Also</h2>\n<ul>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a></li>\n<li><a href=\"https://lesswrong.com/tag/reinforcement-learning\">Reinforcement learning</a></li>\n<li><a href=\"https://lesswrong.com/tag/value-extrapolation\">Value extrapolation</a></li>\n<li><a href=\"https://lesswrong.com/tag/complexity-of-value\">Complexity of value</a></li>\n<li><a href=\"https://lesswrong.com/tag/coherent-extrapolated-volition\">Coherent Extrapolated Volition</a></li>\n<li><a href=\"https://lesswrong.com/tag/coherent-aggregated-volition\">Coherent Aggregated Volition</a></li>\n<li><a href=\"https://lesswrong.com/tag/coherent-blended-volition\">Coherent Blended Volition</a></li>\n<li><a href=\"https://www.lesswrong.com/tag/ai-assisted-alignment-1\">AI-assisted Alignment</a> and <a href=\"https://www.lesswrong.com/tag/ai-assisted-ai-automated-alignment\">AI-assisted/AI automated Alignment</a></li>\n</ul>\n",
    "description_length": 4775,
    "viewCount": 195,
    "parentTagId": "ai-engineering-alignment"
  },
  {
    "core-tag": "AI",
    "_id": "qDvaStt6c3KuqLr2P",
    "name": "AI Safety Camp",
    "slug": "ai-safety-camp",
    "postCount": 88,
    "description_html": "<p><strong>AI Safety Camp</strong> (<strong>AISC</strong>) is a non-profit initiative to run programs for diversely skilled researchers who want to try collaborate on an open problem for reducing AI <a href=\"https://www.lesswrong.com/tag/existential-risk\">existential risk</a>.</p><p><a href=\"https://aisafety.camp/\">Official Website</a></p>",
    "description_length": 341,
    "viewCount": 194,
    "parentTagId": "ai-organizations"
  },
  {
    "core-tag": "AI",
    "_id": "2Rd4dxEcxyCHqLEKW",
    "name": "Apart Research",
    "slug": "apart-research",
    "postCount": 51,
    "description_html": "<p><a href=\"https://apartresearch.com/\"><strong>Apart Research</strong></a> is an AI safety research lab. They host the <a href=\"https://apartresearch.com/sprints\">Apart Sprints</a>, large-scale international events for research experimentation. This tag includes posts written by Apart researchers and content about Apart Research.</p>",
    "description_length": 336,
    "viewCount": 46,
    "parentTagId": "ai-organizations"
  },
  {
    "core-tag": "AI",
    "_id": "8Ec9rD286qNstoiGH",
    "name": "AXRP",
    "slug": "axrp",
    "postCount": 45,
    "description_html": "<p>The <strong>AI X-risk Research Podcast</strong> is a podcast hosted by Daniel Filan.</p><p>Website: <a href=\"https://axrp.net/\">axrp.net</a></p><p>See also: <a href=\"audio\">Audio</a>, <a href=\"interviews\">Interviews</a></p>",
    "description_length": 226,
    "viewCount": 20,
    "parentTagId": "ai-organizations"
  },
  {
    "core-tag": "AI",
    "_id": "vxsbTxQGYTeNDAZXb",
    "name": "Conjecture (org)",
    "slug": "conjecture-org",
    "postCount": 65,
    "description_html": "<p><a href=\"https://conjecture.dev/\">Conjecture</a> is an alignment startup founded by Connor Leahy, Sid Black and Gabriel Alfour, which aims to scale alignment research.</p><p>The initial directions of their research agenda include:</p><ul><li>New frames for reasoning about large language models</li><li>Scalable mechanistic interpretability</li><li>History and philosophy of alignment</li></ul>",
    "description_length": 397,
    "viewCount": 92,
    "parentTagId": "ai-organizations"
  },
  {
    "core-tag": "AI",
    "_id": "LdhpgZDCR967hWcFt",
    "name": "Encultured AI (org)",
    "slug": "encultured-ai-org",
    "postCount": 4,
    "description_html": "",
    "description_length": 0,
    "viewCount": 13,
    "parentTagId": "ai-organizations"
  },
  {
    "core-tag": "AI",
    "_id": "CL9NePP9FejkQo6jn",
    "name": "Future of Life Institute (FLI)",
    "slug": "future-of-life-institute-fli",
    "postCount": 20,
    "description_html": "<p>The <strong>Future of Life Institute</strong>, or <strong>FLI,</strong><i><strong> </strong></i>is a nonprofit organization whose mission is to mitigate <a href=\"existential-risk\">existential risks</a>. Its most prominent activities are issuing grants to x-risk researchers and organizing conferences on <a href=\"ai\">AI</a> and existential risk.</p><p>Website: <a href=\"https://futureoflife.org\">futureoflife.org</a></p>",
    "description_length": 423,
    "viewCount": 37,
    "parentTagId": "ai-organizations"
  },
  {
    "core-tag": "AI",
    "_id": "NrvXXL3iGjjxu5B7d",
    "name": "Machine Intelligence Research Institute (MIRI)",
    "slug": "machine-intelligence-research-institute-miri",
    "postCount": 154,
    "description_html": "<p>The <strong>Machine Intelligence Research Institute</strong>, formerly known as the <a href=\"https://wiki.lesswrong.com/wiki/Singularity_Institute_for_Artificial_Intelligence\">Singularity Institute for Artificial Intelligence</a> (not to be confused with Singularity University) is a non-profit research organization devoted to reducing <a href=\"https://lesswrong.com/tag/existential-risk\">existential risk</a> from <a href=\"https://lesswrong.com/tag/unfriendly-artificial-intelligence\">unfriendly artificial intelligence</a> and understanding problems related to <a href=\"https://lesswrong.com/tag/friendly-artificial-intelligence\">friendly artificial intelligence</a>. <a href=\"https://lesswrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky</a> was one of the early founders and continues to work there as a Research Fellow. The Machine Intelligence Research Institute created and currently owns the <a href=\"https://www.lesswrong.com/about\">LessWrong</a> domain.</p>\n<p>External Links</p>\n<ul>\n<li><a href=\"http://intelligence.org/\">Homepage of the Machine Intelligence Research Institute</a></li>\n</ul>\n<h2>See Also</h2>\n<ul>\n<li><a href=\"https://www.lesswrong.com/tag/singularity\">Technological singularity</a></li>\n<li><a href=\"https://lesswrong.com/tag/existential-risk\">Existential risk</a></li>\n<li><a href=\"https://lesswrong.com/tag/intelligence-explosion\">Intelligence explosion</a></li>\n<li><a href=\"https://lesswrong.com/tag/friendly-artificial-intelligence\">Friendly artificial intelligence</a></li>\n</ul>\n",
    "description_length": 1523,
    "viewCount": 195,
    "parentTagId": "ai-organizations"
  },
  {
    "core-tag": "AI",
    "_id": "H4n4rzs33JfEgkf8b",
    "name": "OpenAI",
    "slug": "openai",
    "postCount": 199,
    "description_html": "<p><strong>OpenAI </strong>is an organisation that performs <a href=\"https://ai\">AI</a> research, and houses a substantial amount of AI alignment research. Its stated mission is \"Discovering and enacting the path to safe artificial general intelligence.\"</p><p>This tag is for explicit discussion of the organisation, not for all work published by researchers at that organisation.</p><h3>See also:</h3><p>OpenAI projects: <a href=\"https://lesswrong.com/tag/gpt\">GPT</a>, <a href=\"https://lesswrong.com/tag/dall-e\">DALL-E</a></p><p>Other related tags: <a href=\"https://lesswrong.com/tag/language-models\">Language Models</a>, <a href=\"https://lesswrong.com/tag/machine-learning\">Machine Learning</a></p>",
    "description_length": 702,
    "viewCount": 264,
    "parentTagId": "ai-organizations"
  },
  {
    "core-tag": "AI",
    "_id": "RseFyq6FqAhTycBEY",
    "name": "Ought",
    "slug": "ought",
    "postCount": 17,
    "description_html": "<p><strong>Ought</strong> is an AI alignment research non-profit focused on the problem of <a href=\"https://www.lesswrong.com/tag/factored-cognition?showPostCount=true&amp;useTagName=true\">Factored Cognition</a>.</p>",
    "description_length": 216,
    "viewCount": 23,
    "parentTagId": "ai-organizations"
  },
  {
    "core-tag": "AI",
    "_id": "6zBEfFYJxhSEcchbR",
    "name": "AI Alignment Fieldbuilding",
    "slug": "ai-alignment-fieldbuilding",
    "postCount": 256,
    "description_html": "<p><strong>AI Alignment Fieldbuilding</strong> is the effort to improve the alignment ecosystem. Some priorities include introducing new people to the importance of AI risk, on-boarding them by connecting them with key resources and ideas, educating them on existing literature and methods for generating new and valuable research, supporting people who are contributing, and maintaining and improving the funding systems.</p><p>There is an invite-only Slack for people working on the alignment ecosystem. If you'd like to join message <a href=\"https://www.lesswrong.com/users/ete\">plex</a> with an overview of your involvement.</p>",
    "description_length": 632,
    "viewCount": 108,
    "parentTagId": "ai-strategy"
  },
  {
    "core-tag": "AI",
    "_id": "qHDus5MuMNqQxJbjD",
    "name": "AI Governance",
    "slug": "ai-governance",
    "postCount": 568,
    "description_html": "<p><strong>AI Governance<em> </em></strong>asks how we can ensure society benefits at large from increasingly powerful AI systems. While solving technical AI alignment is a necessary step towards this goal, it is by no means sufficient.</p><p>Governance includes policy, economics, sociology, law, and many other fields.</p>",
    "description_length": 324,
    "viewCount": 189,
    "parentTagId": "ai-strategy"
  },
  {
    "core-tag": "AI",
    "_id": "bQZAkiFgtbEcr5h6f",
    "name": "AI Persuasion",
    "slug": "ai-persuasion",
    "postCount": 24,
    "description_html": "<p>AI which is highly capable of persuading people might have significant effects on humanity.</p>",
    "description_length": 98,
    "viewCount": 57,
    "parentTagId": "ai-strategy"
  },
  {
    "core-tag": "AI",
    "_id": "ZFrgTgzwEfStg26JL",
    "name": "AI Risk",
    "slug": "ai-risk",
    "postCount": 1378,
    "description_html": "<p><strong>AI Risk</strong> is analysis of the risks associated with building powerful AI systems.</p><p><i>Related: </i><a href=\"https://www.lesswrong.com/tag/ai\"><i>AI</i></a><i>, </i><a href=\"https://www.lesswrong.com/tag/orthogonality-thesis\"><i>Orthogonality thesis</i></a><i>, </i><a href=\"https://www.lesswrong.com/tag/complexity-of-value\"><i>Complexity of value</i></a><i>, </i><a href=\"https://www.lesswrong.com/tag/goodhart-s-law\"><i>Goodhart's law</i></a><i>, </i><a href=\"https://www.lesswrong.com/tag/paperclip-maximizer\"><i>Paperclip maximiser</i></a></p>",
    "description_length": 569,
    "viewCount": 342,
    "parentTagId": "ai-strategy"
  },
  {
    "core-tag": "AI",
    "_id": "YkPxg2tFDQNdaEZDJ",
    "name": "AI Risk Concrete Stories",
    "slug": "ai-risk-concrete-stories",
    "postCount": 39,
    "description_html": "<p>See also <a href=\"https://www.lesswrong.com/tag/threat-models\">Threat Models</a></p>",
    "description_length": 87,
    "viewCount": 60,
    "parentTagId": "ai-strategy"
  },
  {
    "core-tag": "AI",
    "_id": "dHNS6r6LD6s2hEvZz",
    "name": "AI Services (CAIS)",
    "slug": "ai-services-cais",
    "postCount": 26,
    "description_html": "<p>An <strong>AI service </strong>as used in the context of Eric Drexler&apos;s technical report <a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf?asd=sa\">Reframing Superintelligence: Comprehensive AI Services as General Intelligence</a> (CAIS), is an AI system that delivers bounded results for some task using bounded resources in bounded time. It is contrasted with agentive AGI, which carries out open-ended goals over an unbounded period of time. </p><p>A gradual accumulation of increasingly competent services is one model of how AI might develop. For a summary, see <a href=\"https://www.lesswrong.com/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as\">this post</a>.</p>",
    "description_length": 762,
    "viewCount": 45,
    "parentTagId": "ai-strategy"
  },
  {
    "core-tag": "AI",
    "_id": "PgXFLqriw5Far9v7x",
    "name": "AI Success Models",
    "slug": "ai-success-models",
    "postCount": 37,
    "description_html": "<p><strong>AI Success Models</strong> are proposed paths to an existential win via aligned AI. They are (so far) high level overviews and won't contain all the details, but present at least a sketch of what a full solution might look like. They can be contrasted with <a href=\"https://www.lesswrong.com/tag/threat-models\">threat models</a>, which are stories about how AI might lead to major problems.</p>",
    "description_length": 405,
    "viewCount": 47,
    "parentTagId": "ai-strategy"
  },
  {
    "core-tag": "AI",
    "_id": "oiRp4T6u5poc8r9Tj",
    "name": "AI Takeoff",
    "slug": "ai-takeoff",
    "postCount": 255,
    "description_html": "<p><strong>AI Takeoff</strong> refers to the process of an <a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\">Artificial General Intelligence</a> going from a certain threshold of capability (often discussed as &quot;human-level&quot;) to being super-intelligent and capable enough to control the fate of civilization. There has been much debate about whether AI takeoff is more likely to be slow vs fast, i.e., &quot;soft&quot; vs &quot;hard&quot;.</p><p><em>See also</em>: <a href=\"https://www.lesswrong.com/tag/ai-timelines\">AI Timelines</a>, <a href=\"https://www.lesswrong.com/tag/seed-ai\">Seed AI</a>, <a href=\"https://www.lesswrong.com/tag/singularity\">Singularity</a>, <a href=\"https://www.lesswrong.com/tag/intelligence-explosion\">Intelligence explosion</a>, <a href=\"https://www.lesswrong.com/tag/recursive-self-improvement\">Recursive self-improvement</a></p><p>AI takeoff is sometimes casually referred to as <strong>AI FOOM.</strong></p><h1>Soft takeoff</h1><p>A <strong>soft takeoff</strong> refers to an AGI that would self-improve over a period of years or decades. This could be due to either the learning algorithm being too demanding for the hardware or because the AI relies on experiencing feedback from the real-world that would have to be played out in real-time. Possible methods that could deliver a soft takeoff, by slowly building on human-level intelligence, are <a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\">Whole brain emulation</a>, <a href=\"https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement\">Biological Cognitive Enhancement</a>, and software-based strong AGI [<a href=\"https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&amp;lw_source=import_sheet#fn1\">1</a>]. By maintaining control of the AGI&apos;s ascent it should be easier for a <a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a> to emerge.</p><p>Vernor Vinge, Hans Moravec and have all expressed the view that soft takeoff is preferable to a hard takeoff as it would be both safer and easier to engineer.</p><h1>Hard takeoff</h1><p>A <strong>hard takeoff</strong> (or an AI going &quot;<strong>FOOM</strong>&quot; [<a href=\"https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&amp;lw_source=import_sheet#fn2\">2</a>]) refers to AGI expansion in a matter of minutes, days, or months. It is a fast, abruptly, local increase in capability. This scenario is widely considered much more precarious, as this involves an AGI rapidly ascending in power without human control. This may result in unexpected or undesired behavior (i.e. <a href=\"https://wiki.lesswrong.com/wiki/Unfriendly_AI\">Unfriendly AI</a>). It is one of the main ideas supporting the <a href=\"https://www.lesswrong.com/tag/intelligence-explosion\">Intelligence explosion</a> hypothesis.</p><p>The feasibility of hard takeoff has been addressed by Hugo de Garis, <a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky</a>, <a href=\"https://www.lesswrong.com/tag/ben-goertzel\">Ben Goertzel</a>, <a href=\"https://www.lesswrong.com/tag/nick-bostrom\">Nick Bostrom</a>, and Michael Anissimov. It is widely agreed that a hard takeoff is something to be avoided due to the risks. Yudkowsky points out several possibilities that would make a hard takeoff more likely than a soft takeoff such as the existence of large <a href=\"https://www.lesswrong.com/tag/computing-overhang\">resources overhangs</a> or the fact that small improvements seem to have a large impact in a mind&apos;s general intelligence (i.e.: the small genetic difference between humans and chimps lead to huge increases in capability) [<a href=\"https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&amp;lw_source=import_sheet#fn3\">3</a>].</p><h1>Notable posts</h1><ul><li><a href=\"https://www.lesswrong.com/lw/wf/hard_takeoff/\">Hard Takeoff</a> by Eliezer Yudkowsky</li></ul><h1>External links</h1><ul><li><a href=\"http://www.kurzweilai.net/the-age-of-virtuous-machines\">The Age of Virtuous Machines</a> by J. Storrs Hall President of The Foresight Institute</li><li><a href=\"http://multiverseaccordingtoben.blogspot.co.uk/2011/01/hard-takeoff-hypothesis.html\">Hard take off Hypothesis</a> by Ben Goertzel.</li><li><a href=\"http://www.acceleratingfuture.com/michael/blog/2011/05/hard-takeoff-sources/\">Extensive archive of Hard takeoff Essays</a> from Accelerating Future</li><li><a href=\"http://www-rohan.sdsu.edu/faculty/vinge/misc/ac2005/\">Can we avoid a hard take off?</a> by Vernor Vinge</li><li><a href=\"http://www.amazon.co.uk/Robot-Mere-Machine-Transcendent-Mind/dp/0195136306\">Robot: Mere Machine to Transcendent Mind</a> by Hans Moravec</li><li><a href=\"http://www.amazon.co.uk/The-Singularity-Near-Raymond-Kurzweil/dp/0715635611/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1339495098&amp;sr=1-1\">The Singularity is Near</a> by Ray Kurzweil</li></ul><p><strong>References</strong></p><ol><li><a href=\"http://www.aleph.se/andart/archives/2010/10/why_early_singularities_are_softer.html&#x21A9;\">http://www.aleph.se/andart/archives/2010/10/why_early_singularities_are_softer.html</a><a href=\"https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&amp;lw_source=import_sheet#fnref1\">&#x21A9;</a></li><li><a href=\"http://lesswrong.com/lw/63t/requirements_for_ai_to_go_foom/&#x21A9;\">http://lesswrong.com/lw/63t/requirements_for_ai_to_go_foom/</a><a href=\"https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&amp;lw_source=import_sheet#fnref2\">&#x21A9;</a></li><li><a href=\"https://www.lesswrong.com/lw/wf/hard_takeoff/\">http://lesswrong.com/lw/wf/hard_takeoff/</a><a href=\"http://lesswrong.com/lw/wf/hard_takeoff/&#x21A9;\">&#x21A9;</a></li></ol>",
    "description_length": 5670,
    "viewCount": 681,
    "parentTagId": "ai-strategy"
  },
  {
    "core-tag": "AI",
    "_id": "zHjC29kkPmsdo7WTr",
    "name": "AI Timelines",
    "slug": "ai-timelines",
    "postCount": 350,
    "description_html": "<p><strong>AI Timelines</strong> is the discussion of how long until various major milestones in AI progress are achieved, whether it's the timeline until a human-level AI is developed, the timeline until certain benchmarks are defeated, the timeline until we can simulate a mouse-level intelligence, or something else.</p><p>This is to be distinguished from the closely related question of <a href=\"https://www.lesswrong.com/tag/ai-takeoff\">AI takeoff</a> speeds, which is about the dynamics of AI progress after human-level AI is developed (e.g. will it be a single project or the whole economy that sees growth, how fast will that growth be, etc).</p>",
    "description_length": 654,
    "viewCount": 656,
    "parentTagId": "ai-strategy"
  },
  {
    "core-tag": "AI",
    "_id": "5f5c37ee1b5cdee568cfb2bd",
    "name": "Computing Overhang",
    "slug": "computing-overhang",
    "postCount": 20,
    "description_html": "<p><strong>Computing overhang</strong> refers to a situation where new algorithms can exploit existing computing power far more efficiently than before. This can happen if previously used algorithms have been suboptimal.</p><p>In the context of <a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\">Artificial General Intelligence</a>, this signifies a situation where it becomes possible to create AGIs that can be run using only a small fraction of the easily available hardware resources. This could lead to an <a href=\"https://www.lesswrong.com/tag/intelligence-explosion\">intelligence explosion</a>, or to a massive increase in the number of AGIs, as they could be easily copied to run on countless computers. This could make AGIs much more powerful than before, and present an <a href=\"https://www.lesswrong.com/tag/existential-risk\">existential risk</a>.</p><h2>Examples</h2><p>In 2010, the President's Council of Advisors on Science and Technology <a href=\"http://www.whitehouse.gov/sites/default/files/microsites/ostp/pcast-nitrd-report-2010.pdf\">reported on</a> benchmark production planning model having become faster by a factor of 43 million between 1988 and 2003. Of this improvement, only a factor of roughly 1,000 was due to better hardware, while a factor of 43,000 came from algorithmic improvements. This clearly reflects a situation where new programming methods were able to use available computing power more efficiently.</p><p>As of today, enormous amounts of computing power is currently available in the form of supercomputers or distributed computing. Large AI projects can grow to fill these resources by using deeper and deeper search trees, such as high-powered chess programs, or by performing large amounts of parallel operations on extensive databases, such as IBM's Watson playing Jeopardy. While the extra depth and breadth are helpful, it is likely that a simple brute-force extension of techniques is not the optimal use of the available computing resources. This leaves the need for improvement on the side of algorithmic implementations, where most work is currently focused on.</p><p>Though estimates of <a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\">whole brain emulation</a> place that level of computing power at least a decade away, it is very unlikely that the algorithms used by the human brain are the most computationally efficient for producing AI. This happens mainly because our brains evolved during a natural selection process and thus weren't deliberatly created with the goal of being modeled by AI.</p><p>As Yudkoswky <a href=\"http://intelligence.org/files/LOGI.pdf\">puts it</a>, human intelligence, created by this \"blind\" evolutionary process, has only recently developed the ability for planning and forward thinking - <i>deliberation</i>. On the other hand, the rest and almost all our cognitive tools were the result of ancestral selection pressures, forming the roots of almost all our behavior. As such, when considering the design of complex systems where the designer - us - collaborates with the system being constructed, we are faced with a new signature and a different way to achieve AGI that's completely different than the process that gave birth to our brains.</p><h2>References</h2><ul><li>Muehlhauser, Luke; Salamon, Anna (2012). <a href=\"http://intelligence.org/files/IE-EI.pdf\"><u>\"Intelligence Explosion: Evidence and Import\"</u></a>. in Eden, Amnon; Søraker, Johnny; Moor, James H. et al.. <i>The singularity hypothesis: A scientific and philosophical assessment</i>. Berlin: Springer.</li></ul><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/optimization\">Optimization process</a></li><li><a href=\"https://www.lesswrong.com/tag/optimization\">Optimization</a></li></ul>",
    "description_length": 3795,
    "viewCount": 193,
    "parentTagId": "ai-strategy"
  },
  {
    "core-tag": "AI",
    "_id": "5f5c37ee1b5cdee568cfb2c7",
    "name": "Regulation and AI Risk",
    "slug": "regulation-and-ai-risk",
    "postCount": 114,
    "description_html": "<p><strong>Regulation and AI risk</strong> is the debate on whether regulation could be used to reduce the risks of <a href=\"https://www.lesswrong.com/tag/unfriendly-artificial-intelligence\">Unfriendly AI</a>, and what forms of regulation would be appropriate.</p><p>Several authors have advocated AI research to be regulated, but been vague on the details. Yampolskiy &amp; Fox (2012) note that university research programs in the social and medical sciences are overseen by institutional review boards, and propose setting up analogous review boards to evaluate potential AGI research. In order to be successful, AI regulation would have to be global, and there is the potential for an <a href=\"https://www.lesswrong.com/tag/ai-arms-race\">AI arms race</a> between different nations. Partially because of this, McGinnis (2010) argues that the government should not attempt to regulate AGI development. Rather, it should concentrate on providing funding to research projects intended to create safe AGI. Kaushal &amp; Nolan (2015) point out that regulations on AGI development would result in a speed advantage for any project willing to skirt the regulations, and instead propose government funding (possibly in the form of an \"AI Manhattan Project\") for AGI projects meeting particular criteria.</p><p>While Shulman &amp; Armstrong (2009) argue the unprecedentedly destabilizing effect of AGI could be a cause for world leaders to cooperate more than usual, the opposite argument can be made as well. Gubrud (1997) argues that molecular nanotechnology could make countries more self-reliant and international cooperation considerably harder, and that AGI could contribute to such a development. AGI technology is also much harder to detect than e.g. nuclear technology is - AGI research can be done in a garage, while nuclear weapons require a substantial infrastructure (McGinnis 2010). On the other hand, Scherer (2015) argues that artificial intelligence could nevertheless be susceptible to regulation due to the increasing prominence of governmental entities and large corporations in AI research and development.</p><p>Goertzel &amp; Pitt (2012) suggest that for regulation to be enacted, there might need to be an <a href=\"https://www.lesswrong.com/tag/agi-sputnik-moment\">AGI Sputnik moment</a> - a technological achievement that makes the possibility of AGI evident to the public and policy makers. They note that after such a moment, it might not take a very long time for full human-level AGI to be developed, while the negotiations required to enact new kinds of arms control treaties would take considerably longer.</p><h2>References</h2><ul><li>Ben Goertzel &amp; Joel Pitt (2012): <a href=\"http://jetpress.org/v22/goertzel-pitt.htm\">Nine Ways to Bias Open-Source AGI Toward Friendliness</a>. Journal of Evolution and Technology - Vol. 22 Issue 1 – pgs 116-141.</li><li>Mark Gubrud (1997): <a href=\"http://www.foresight.org/Conferences/MNT05/Papers/Gubrud/\">Nanotechnology and International Security</a>. Fifth Foresight Conference on Molecular Nanotechnology.</li><li>John McGinnis (2010): <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1593851\">Accelerating AI</a>. Northwestern University Law Review.</li><li>Matthew Scherer (2015): <a href=\"http://papers.ssrn.com/abstract=2609777\">Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies</a>. Harvard Journal of Law &amp; Technology.</li><li>Carl Shulman &amp; Stuart Armstrong (2009): <a href=\"http://intelligence.org/files/ArmsControl.pdf\">Arms control and intelligence explosions</a>. European Conference on Computing and Philosophy.</li><li>Roman Yampolskiy &amp; Joshua Fox (2012): <a href=\"http://intelligence.org/files/SafetyEngineering.pdf\">Safety Engineering for Artificial General Intelligence</a>. Topoi.</li><li>Mohit Kaushal &amp; Scott Nolan (2015): <a href=\"http://www.brookings.edu/blogs/techtank/posts/2015/04/14-understanding-artificial-intelligence\">Understanding Artificial Intelligence</a>. Brookings.</li></ul><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/ai-arms-race\">AI arms race</a></li><li><a href=\"https://www.lesswrong.com/tag/agi-sputnik-moment\">AGI Sputnik moment</a></li><li><a href=\"https://www.lesswrong.com/tag/existential-risk\">Existential risk</a></li><li><a href=\"https://www.lesswrong.com/tag/unfriendly-artificial-intelligence\">Unfriendly artificial intelligence</a></li></ul>",
    "description_length": 4453,
    "viewCount": 58,
    "parentTagId": "ai-strategy"
  },
  {
    "core-tag": "AI",
    "_id": "Fxq9YJMGsuphd8Rmt",
    "name": "AI Alignment Intro Materials",
    "slug": "ai-alignment-intro-materials",
    "postCount": 46,
    "description_html": "<p><strong>AI Alignment Intro Materials. </strong>Posts that help someone get oriented and skill up. Distinct from AI Public Materials is that they are more \"inward facing\" than \"outward facing\", i.e. for people who are already sold AI risk is a problem and want to upskill.<br>&nbsp;</p><p>Some basic intro resources include:</p><ul><li><a href=\"https://aisafety.info/\">Stampy's AI Safety Info</a> (extensive interactive FAQ)</li><li><a href=\"https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq\">Scott Alexander's Superintelligence FAQ</a></li><li><a href=\"https://intelligence.org/ie-faq/\">The MIRI Intelligence Explosion FAQ</a></li><li><a href=\"https://www.agisafetyfundamentals.com/\">The AGI Safety Fundamentals courses</a></li><li><a href=\"https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111/\">Superintelligence</a> (book)</li></ul>",
    "description_length": 893,
    "viewCount": 115,
    "parentTagId": "ai-other"
  },
  {
    "core-tag": "AI",
    "_id": "iKYWGuFx2qH2nYu6J",
    "name": "AI Capabilities",
    "slug": "ai-capabilities",
    "postCount": 144,
    "description_html": "<p><strong>AI Capabilities</strong> are the growing abilities of AIs to act effectively in increasingly complex environments. It is often compared to to AI Alignment, which refers to efforts to ensure that these effective actions taken by AIs are also intended by the creators and beneficial to humanity.</p>",
    "description_length": 308,
    "viewCount": 76,
    "parentTagId": "ai-other"
  },
  {
    "core-tag": "AI",
    "_id": "YzdNZCWvnkdvuKPBB",
    "name": "AI Questions Open Thread",
    "slug": "ai-questions-open-thread",
    "postCount": 11,
    "description_html": "<p>The <strong>All AI Questions Welcome</strong> open threads are a series of posts where people are welcome to post any AI questions, however basic.</p>",
    "description_length": 153,
    "viewCount": 86,
    "parentTagId": "ai-other"
  },
  {
    "core-tag": "AI",
    "_id": "bWcaNMZifyYShJEPd",
    "name": "Compute",
    "slug": "compute",
    "postCount": 32,
    "description_html": "<p><strong>Compute</strong> is a generic term for resources necessary to run software computation. It includes processing power, memory, networking, storage, etc.</p>",
    "description_length": 166,
    "viewCount": 37,
    "parentTagId": "ai-other"
  },
  {
    "core-tag": "AI",
    "_id": "L2cuwYAisTL3QF4TA",
    "name": "DALL-E",
    "slug": "dall-e",
    "postCount": 27,
    "description_html": "<p><strong>DALL-E</strong> is a family of <a href=\"machine-learning\">machine learning</a> models created by <a href=\"openai\">OpenAI</a> that generate images from text descriptions.</p><p><a href=\"https://openai.com/dall-e-2/\">DALL-E 2 official website</a></p>",
    "description_length": 259,
    "viewCount": 30,
    "parentTagId": "ai-other"
  },
  {
    "core-tag": "AI",
    "_id": "YWzByWvtXunfrBu5b",
    "name": "GPT",
    "slug": "gpt",
    "postCount": 432,
    "description_html": "<p><strong>GPT</strong> (Generative Pretrained Transformer) is a family of large transformer-based <a href=\"https://lesswrong.com/tag/language-models\">language models</a> created by <a href=\"https://lesswrong.com/tag/openai\">OpenAI</a>. Its ability to generate remarkably human-like responses has relevance to discussions on AGI.</p><p>External links:</p><p><a href=\"https://arxiv.org/abs/2005.14165\">GPT-3 Paper</a></p><p><a href=\"https://openai.com/api/\">GPT-3 Website</a></p>",
    "description_length": 478,
    "viewCount": 200,
    "parentTagId": "ai-other"
  },
  {
    "core-tag": "AI",
    "_id": "KmgkrftQuX7jmjjp5",
    "name": "Language Models",
    "slug": "language-models",
    "postCount": 661,
    "description_html": "<p>Language models are computer programs made to estimate the likelihood of a piece of text. \"Hello, how are you?\" is likely. \"Hello, fnarg horses\" is unlikely.</p><p>Language models can answer questions by estimating the likelihood of possible question-and-answer pairs, selecting the most likely question-and-answer pair. \"Q: How are You? A: Very well, thank you\" is a likely question-and-answer pair. \"Q: How are You? A: Correct horse battery staple\" is an unlikely question-and-answer pair.</p><p>The language models most relevant to AI safety are language models based on \"deep learning\". Deep-learning-based language models can be \"trained\" to understand language better, by exposing them to text written by humans. There is a lot of human-written text on the internet, providing loads of training material.</p><p>Deep-learning-based language models are getting bigger and better trained. As the models become stronger, they get new skills. These skills include arithmetic, explaining jokes, programming, and solving math problems.</p><p>There is a potential risk of these models developing dangerous capabilities as they grow larger and better trained. What additional skills will they develop given a few years?</p><h3>See also</h3><ul><li><a href=\"https://www.lesswrong.com/tag/gpt\">GPT</a> - A family of large language models created by <a href=\"https://www.lesswrong.com/tag/openai\">OpenAI</a></li></ul>",
    "description_length": 1414,
    "viewCount": 269,
    "parentTagId": "ai-other"
  },
  {
    "core-tag": "AI",
    "_id": "ocGoDbHKBv46AwXnT",
    "name": "Narrow AI",
    "slug": "narrow-ai",
    "postCount": 20,
    "description_html": "<p>A<strong> Narrow AI</strong> is capable of operating only in a relatively limited domain, such as chess or driving, rather than capable of learning a broad range of tasks like a human or an <a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\">Artificial General Intelligence</a>. Narrow vs General is not a perfectly binary classification: there are degrees of generality with, for example, large language models having a fairly large degree of generality (as the domain of text is large) without being as general as a human, and we may eventually build systems that are significantly more general than humans.</p>",
    "description_length": 636,
    "viewCount": 28,
    "parentTagId": "ai-other"
  },
  {
    "core-tag": "AI",
    "_id": "5f5c37ee1b5cdee568cfb29d",
    "name": "Neuromorphic AI",
    "slug": "neuromorphic-ai",
    "postCount": 36,
    "description_html": "<p>A <strong>Neuromorphic AI</strong> ('neuron-shaped') is a form of AI where most of the functionality has been copied from the human brain. This implies that its inner workings are not necessarily understood by the creators any further than is necessary to simulate them on a computer. It is considered a more <a href=\"https://wiki.lesswrong.com/wiki/Unfriendly_AI\">unsafe</a> form of AI than either <a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\">Whole Brain Emulation</a> or de novo AI because its lacks the high quality replication of human values of the former and the possibility of good theoretical guarantees that the latter may have due to cleaner design.</p><h2>External Links</h2><ul><li>Definition from <a href=\"http://www.theuncertainfuture.com/faq.html#3\">The Uncertain Future</a></li></ul>",
    "description_length": 819,
    "viewCount": 57,
    "parentTagId": "ai-other"
  },
  {
    "core-tag": "World Modeling",
    "_id": "5GYzBE6q89w74dqfk",
    "name": "Abstraction",
    "slug": "abstraction",
    "postCount": 96,
    "description_html": "<p>An <strong>abstraction</strong> is a high-level concept that groups things together while not considering some of their differences.</p><p>(This is a stub, please rewrite if you have a better tag description).</p>",
    "description_length": 216,
    "viewCount": 111,
    "parentTagId": "world-modeling-mathematical-sciences"
  },
  {
    "core-tag": "World Modeling",
    "_id": "PbShukhzpLsWpGXkM",
    "name": "Anthropics",
    "slug": "anthropics",
    "postCount": 257,
    "description_html": "<p><strong>Anthropics</strong> is the study of how the fact that we succeed in making observations of a given kind at all gives us evidence about the world we are living, independently of the content of the observations. As an example, for living beings, making any observations at all is only possible in a universe with physical laws that support life.</p><p><strong>Related Pages: </strong><a href=\"https://www.lesswrong.com/tag/sleeping-beauty-paradox\">Sleeping Beauty Paradox</a>, <a href=\"https://www.lesswrong.com/tag/filtered-evidence\">Filtered Evidence</a>, <a href=\"https://www.lesswrong.com/tag/great-filter\">Great Filter</a></p>",
    "description_length": 640,
    "viewCount": 202,
    "parentTagId": "world-modeling-mathematical-sciences"
  },
  {
    "core-tag": "World Modeling",
    "_id": "NXn3MSft8kzmMJbeg",
    "name": "Category Theory",
    "slug": "category-theory",
    "postCount": 32,
    "description_html": "<p><strong>Category Theory </strong>is a subfield of pure mathematics studying any structure that contains objects and their relations (referred to as <i>morphisms</i>). It emerged in the study of <a href=\"https://www.wolframscience.com/nks/notes-12-9--category-theory/\">algebraic topology</a>, then went on to apply beyond mathematics and into various scientific disciplines, a metamathematical framework comparable to that of type theory and set theory. The notion of <i>compositionality </i>is what differs category theory from graph theory, in which the nodes themselves can be categories.</p><p>Current research on <a href=\"https://www.appliedcategorytheory.org/\">applied category theory</a> and <a href=\"https://cats.for.ai/\">Categories for AI</a> are useful and relevant for topics close to LW, such as <a href=\"https://www.lesswrong.com/tag/rationality\">Rationality</a>, <a href=\"https://www.alignmentforum.org/\">AI Safety</a>, and <a href=\"https://lesswrong.com/tag/game-theory\">Game theory</a>. Due to its abstract nature, category theory is jokingly criticized as being \"abstract nonsense\". A major theorem result is <a href=\"https://www.math3ma.com/blog/the-yoneda-embedding\">Yoneda embedding</a>, which is basically the idea that an object can be defined by its all of its relations.</p>",
    "description_length": 1300,
    "viewCount": 127,
    "parentTagId": "world-modeling-mathematical-sciences"
  },
  {
    "core-tag": "World Modeling",
    "_id": "cq69M9ceLNA35ShTR",
    "name": "Causality",
    "slug": "causality",
    "postCount": 138,
    "description_html": "<p><strong>Causality </strong>is the intuitive notion that some events happening \"result\" in other events happening. What's going on with that? What does it mean for A to cause B? How do we figure out the causal relationship between things?</p><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/free-will\">Free will</a></li><li><a href=\"https://www.lesswrong.com/tag/teleology\">Teleology</a></li><li><a href=\"https://www.lesswrong.com/tag/beliefs-require-observations\">Beliefs require observations</a></li></ul><h2>External links</h2><ul><li><a href=\"http://www.stat.columbia.edu/~cook/movabletype/archives/2009/07/philip_dawids_t.html\">Philip Dawid's explication of Pearl's model, and two ways of thinking about nonrandom sampling</a> by <a href=\"https://en.wikipedia.org/wiki/Philip_Dawid\">Philip Dawid</a> and <a href=\"http://andrewgelman.com/\">Andrew Gelman</a> - Causal inference as \"the task of using data collected under one regime to infer about the properties of another\".</li><li><a href=\"http://www.stat.columbia.edu/~cook/movabletype/archives/2009/07/disputes_about.html\">Resolving disputes between J. Pearl and D. Rubin on causal inference</a> and <a href=\"http://www.stat.columbia.edu/~cook/movabletype/archives/2009/07/more_on_pearlru.html\">More on Pearl's and Rubin's frameworks for causal inference</a> by Andrew Gelman</li><li><a href=\"http://www.michaelnielsen.org/ddi/if-correlation-doesnt-imply-causation-then-what-does/\">If correlation doesn't imply causation, then what does?</a> by <a href=\"https://en.wikipedia.org/wiki/Michael_Nielsen\">Michael Nielsen</a></li><li><a href=\"http://oyhus.no/CorrelationAndCausation.html\">Correlation is Evidence of Causation</a> by Kim Øyhus</li><li>Judea Pearls's works: <a href=\"http://bayes.cs.ucla.edu/BOOK-2K/\"><i><u>Causality: Models, Reasoning, and Inference</u></i></a><i>,</i> <a href=\"https://www.goodreads.com/book/show/36204378-the-book-of-why\">Book of Why, A Primer on Causality</a></li></ul>",
    "description_length": 1975,
    "viewCount": 125,
    "parentTagId": "world-modeling-mathematical-sciences"
  },
  {
    "core-tag": "World Modeling",
    "_id": "GY5kPPpCoyt9fnTMn",
    "name": "Computer Science",
    "slug": "computer-science",
    "postCount": 117,
    "description_html": null,
    "description_length": null,
    "viewCount": 112,
    "parentTagId": "world-modeling-mathematical-sciences"
  },
  {
    "core-tag": "World Modeling",
    "_id": "fp7AHLBpKB3EN4bLu",
    "name": "Free Energy Principle",
    "slug": "free-energy-principle",
    "postCount": 53,
    "description_html": "<p>The <a href=\"https://en.wikipedia.org/wiki/Free_energy_principle\"><strong>Free Energy Principle</strong></a> (FEP) is a principle that suggests that dynamic systems, including the brain and other physical systems, are organized to minimize prediction errors, or the difference between the predictions made about the environment and the actual outcomes experienced. According to the FEP, dynamic systems encode information about their environment in a way to reduce surprisal from its input. The FEP proposes that dynamic systems are motivated to minimize prediction errors in order to maintain stability within the environment. FEP has been influential in <a href=\"https://www.lesswrong.com/tag/neuroscience\">neuroscience</a> and neuropsychology and more recently has been used to describe systems on all spatiotemporal scales, from cells and biological species to AIs and societies.</p><p>FEP gives rise to <i>Active Inference</i><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1wzmjmmjyut\"><sup><a href=\"#fn1wzmjmmjyut\">[1]</a></sup></span>: a process theory of <a href=\"https://www.lesswrong.com/tag/agency\">agency</a>, that can be seen <a href=\"https://www.lesswrong.com/posts/2BPPwboTDrAMFiGHe/the-two-conceptions-of-active-inference-an-intelligence\">both as an explanatory theory and as an agent architecture</a>. In the latter sense, Active Inference rivals <a href=\"https://www.lesswrong.com/tag/reinforcement-learning\">Reinforcement Learning</a>. It has been argued<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefml7x3rfadwa\"><sup><a href=\"#fnml7x3rfadwa\">[2]</a></sup></span>&nbsp;that Active Inference as an agent architecture manages the model complexity (i. e., the bias-variance tradeoff) and the exploration-exploitation tradeoff in a principled way, favours explicit, disentangled, and hence more <a href=\"https://www.lesswrong.com/tag/interpretability-ml-and-ai\">interpretable</a> belief representations, and is amenable for working within hierarchical systems of collective intelligence (which are seen as Active Inference agents themselves<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjjs0mwgmj19\"><sup><a href=\"#fnjjs0mwgmj19\">[3]</a></sup></span>). Building ecosystems of hierarchical collective intelligence can be seen as a proposed solution for and an alternative conceptualisation of the general problem of <i>alignment</i>.</p><p>FEP/Active Inference is an energy-based model of intelligence: a FEP agent minimises an informational quantity called <i>variational free energy</i> (VFE), and Active Inference nuances this picture further, modelling agents as minimising an informational quantity called <i>expected free energy</i> (EFE), which is derived from VFE. This likens FEP/Active Inference to Bengio's <a href=\"https://www.lesswrong.com/tag/gflownets\">GFlowNets</a><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefi1p9vxi0ar\"><sup><a href=\"#fni1p9vxi0ar\">[4]</a></sup></span>&nbsp;and LeCun's Joint Embedding Predictive Architecture (JEPA)<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefp2msricmloe\"><sup><a href=\"#fnp2msricmloe\">[5]</a></sup></span>, which are also energy-based. On the other hand, this distinguishes FEP/Active Inference from Reinforcement Learning, which is a reward-based model of agency, and, more generally, <a href=\"https://www.lesswrong.com/tag/utility\">utility</a>-maximising <a href=\"https://www.lesswrong.com/tag/decision-theory\">decision theories</a>.</p><p>Active Inference is one of the most general theories of agency. It can be seen as a generalisation of the <a href=\"https://www.lesswrong.com/tag/predictive-processing\">predictive coding</a> theory of brain function (or, the <a href=\"https://en.wikipedia.org/wiki/Bayesian_approaches_to_brain_function\">Bayesian Brain hypothesis</a>). Specifically, while predictive coding explains the agent's perception as Bayesian inference, Active Inference models both prediction and action as inference under the single unifying objective: minimisation of the agent's VFE or EFE. Active Inference also recovers Bayes-optimal reinforcement learning, <a href=\"https://en.wikipedia.org/wiki/Optimal_control\">optimal control theory</a>, and <a href=\"https://www.lesswrong.com/tag/bayesian-decision-theory\">Bayesian Decision Theory</a> (aka <a href=\"https://www.lesswrong.com/tag/evidential-decision-theory\">EDT</a>) under different simplifying assumptions<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1wzmjmmjyut\"><sup><a href=\"#fn1wzmjmmjyut\">[1]</a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefer8ec4z97g\"><sup><a href=\"#fner8ec4z97g\">[6]</a></sup></span>.</p><p>The mathematical content of Active Inference is based on <a href=\"https://www.lesswrong.com/posts/MFm3A4ihz9s5j2cCo/variational-bayesian-methods\">Variational Bayesian methods</a>.</p><h3>References</h3><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1wzmjmmjyut\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1wzmjmmjyut\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Parr, Thomas, Giovanni Pezzulo, and Karl J. Friston. <a href=\"https://direct.mit.edu/books/oa-monograph/5299/Active-InferenceThe-Free-Energy-Principle-in-Mind\"><i>Active inference: the free energy principle in mind, brain, and behavior</i></a>. MIT Press, 2022.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnml7x3rfadwa\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefml7x3rfadwa\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Friston, Karl J., Maxwell JD Ramstead, Alex B. Kiefer, Alexander Tschantz, Christopher L. Buckley, Mahault Albarracin, Riddhi J. Pitliya et al. \"<a href=\"https://arxiv.org/abs/2212.01354\">Designing Ecosystems of Intelligence from First Principles</a>.\" <i>arXiv preprint arXiv:2212.01354</i> (2022).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnjjs0mwgmj19\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefjjs0mwgmj19\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Kaufmann, Rafael, Pranav Gupta, and Jacob Taylor. \"<a href=\"https://www.mdpi.com/1099-4300/23/7/830\">An active inference model of collective intelligence</a>.\" <i>Entropy</i> 23, no. 7 (2021): 830.</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fni1p9vxi0ar\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefi1p9vxi0ar\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Bengio, Yoshua. \"<a href=\"https://www.notion.so/GFlowNet-Tutorial-919dcf0a0f0c4e978916a2f509938b00\">GFlowNet Tutorial</a>.\" (2022).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnp2msricmloe\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefp2msricmloe\">^</a></strong></sup></span><div class=\"footnote-content\"><p>LeCun, Yann. \"<a href=\"https://www.lesswrong.com/posts/Y7XkGQXwHWkHHZvbm/yann-lecun-a-path-towards-autonomous-machine-intelligence-1\">A path towards autonomous machine intelligence</a>.\" <i>preprint posted on openreview</i> (2022).</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fner8ec4z97g\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefer8ec4z97g\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Friston, Karl, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas Parr. \"<a href=\"https://www.researchgate.net/profile/Casper-Hesp-2/publication/349697621_Sophisticated_Inference/links/60a2bc13a6fdcc88cc033f24/Sophisticated-Inference.pdf\">Sophisticated inference</a>.\" <i>Neural Computation</i> 33, no. 3 (2021): 713-763.</p></div></li></ol>",
    "description_length": 7652,
    "viewCount": 161,
    "parentTagId": "world-modeling-mathematical-sciences"
  },
  {
    "core-tag": "World Modeling",
    "_id": "b8FHrKqyXuYGWc6vn",
    "name": "Game Theory",
    "slug": "game-theory",
    "postCount": 316,
    "description_html": "<p><strong>Game theory</strong> is the formal study of how rational actors interact to pursue incentives. It investigates situations of conflict and cooperation.</p>\n<p><em>See also:</em> <a href=\"https://www.lesswrong.com/tag/coordination-cooperation?showPostCount=true&amp;useTagName=true\">Coalition/coordination</a>, <a href=\"https://www.lesswrong.com/tag/coalitional-instincts?showPostCount=true&amp;useTagName=true\">Coalitional Instincts</a>, <a href=\"https://www.lesswrong.com/tag/decision-theory\">Decision theory</a>, <a href=\"https://www.lesswrong.com/tag/moloch?showPostCount=true&amp;useTagName=true\">Moloch</a>, <a href=\"https://www.lesswrong.com/tag/utility-functions\">Utility functions</a>, <a href=\"https://lesswrong.com/tag/decision-theory\">Decision Theory</a>, <a href=\"https://lesswrong.com/tag/prisoner-s-dilemma\">Prisoner's Dilemma</a></p>\n<p>Game theory is an extremely powerful and robust tool in analyzing much more complex situations, such as: mergers and acquisitions, political economy, voting systems, war bargaining and biological evolution. Eight game-theorists have won the Nobel Prize in Economic Sciences.</p>\n<h2>References</h2>\n<ul>\n<li><a href=\"http://levine.sscnet.ucla.edu/general/whatis.htm\">Naïve introduction to Game Theory</a></li>\n<li><a href=\"http://plato.stanford.edu/entries/game-theory/\">Stanford Encyclopedia entry on Game Theory</a></li>\n</ul>\n",
    "description_length": 1391,
    "viewCount": 896,
    "parentTagId": "world-modeling-mathematical-sciences"
  },
  {
    "core-tag": "World Modeling",
    "_id": "dPPATLhRmhdJtJM2t",
    "name": "Decision Theory",
    "slug": "decision-theory",
    "postCount": 456,
    "description_html": "<p><strong>Decision theory</strong> is the study of principles and algorithms for making correct decisions—that is, decisions that allow an agent to achieve better outcomes with respect to its goals. Every action at least implicitly represents a decision under uncertainty: in a state of partial knowledge, something has to be done, even if that something turns out to be nothing (call it \"the null action\"). Even if you don't know how you make decisions, decisions do get made, and so there has to be some underlying mechanism. What is it? And how can it be done better? Decision theory has the answers.</p>\n<p><em>Note: this page needs to be updated with content regarding Functional Decision Theory, the latest theory from MIRI.</em></p>\n<p><em>Related:</em> <a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory</a>, <a href=\"https://www.lesswrong.com/tag/robust-agents?showPostCount=true&amp;useTagName=true\">Robust Agents</a>, <a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions</a></p>\n<p>A core idea in decision theory is that of <a href=\"https://lesswrong.com/tag/expected-utility\"><em>expected utility</em></a> <em>maximization</em>, usually intractable to directly calculate in practice, but an invaluable theoretical concept. An agent assigns utility to every possible outcome: a real number representing the goodness or desirability of that outcome. The mapping of outcomes to utilities is called the agent's <em>utility function</em>. (The utility function is said to be invariant under affine transformations: that is, the utilities can be scaled or translated by a constant while resulting in all the same decisions.) For every action that the agent could take, sum over the utilities of the various possible outcomes weighted by their probability: this is the <a href=\"https://lesswrong.com/tag/expected-value\">expected</a> utility of the action, and the action with the highest expected utility is to be chosen.</p>\n<h2>Thought experiments</h2>\n<p>The limitations and pathologies of decision theories can be analyzed by considering the decisions they suggest in the certain idealized situations that stretch the limits of decision theory's applicability. Some of the thought experiments more frequently discussed on <a href=\"https://wiki.lesswrong.com/wiki/LW\">LW</a> include:</p>\n<ul>\n<li><a href=\"https://lesswrong.com/tag/newcomb-s-problem\">Newcomb's problem</a></li>\n<li><a href=\"https://lesswrong.com/tag/counterfactual-mugging\">Counterfactual mugging</a></li>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker\">Parfit's hitchhiker</a></li>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Smoker's_lesion\">Smoker's lesion</a></li>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Absentminded_driver\">Absentminded driver</a></li>\n<li><a href=\"https://lesswrong.com/tag/sleeping-beauty-paradox\">Sleeping Beauty problem</a></li>\n<li><a href=\"https://lesswrong.com/tag/prisoner-s-dilemma\">Prisoner's dilemma</a></li>\n<li><a href=\"https://lesswrong.com/tag/pascal-s-mugging\">Pascal's mugging</a></li>\n</ul>\n<h2>Commonly discussed decision theories</h2>\n<p>Standard theories well-known in academia:</p>\n<ul>\n<li>CDT, <a href=\"http://en.wikipedia.org/wiki/Causal_decision_theory\">Causal Decision Theory</a></li>\n<li>EDT, <a href=\"http://en.wikipedia.org/wiki/Evidential_decision_theory\">Evidential Decision Theory</a></li>\n</ul>\n<p>Theories invented by researchers associated with <a href=\"https://wiki.lesswrong.com/wiki/MIRI\">MIRI</a> and LW:</p>\n<ul>\n<li>FDT: <a href=\"https://intelligence.org/2017/10/22/fdt/\">Functional Decision Theory</a></li>\n<li>TDT, <a href=\"https://lesswrong.com/tag/timeless-decision-theory\">Timeless Decision Theory</a></li>\n<li>UDT, <a href=\"https://lesswrong.com/tag/updateless-decision-theory\">Updateless Decision Theory</a></li>\n<li>ADT: <a href=\"https://lesswrong.com/tag/ambient-decision-theory\">Ambient Decision Theory</a> (a variant of UDT)</li>\n<li>FDT: <a href=\"https://intelligence.org/files/DeathInDamascus.pdf\">Cheating Death in Damascus</a></li>\n</ul>\n<p>Other decision theories are listed in <a href=\"https://casparoesterheld.com/a-comprehensive-list-of-decision-theories/\">A comprehensive list of decision theories</a>.</p>\n<h2>Blog posts</h2>\n<ul>\n<li><a href=\"https://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/\">Terminal Values and Instrumental Values</a></li>\n<li><a href=\"https://lesswrong.com/lw/aq9/decision_theories_a_less_wrong_primer/\">Decision Theories: A Less Wrong Primer</a> by orthonormal</li>\n<li><a href=\"https://lesswrong.com/lw/gu1/decision_theory_faq/\">Decision Theory FAQ</a> by lukeprog and crazy88</li>\n</ul>\n<h2>Sequence by <a href=\"https://wiki.lesswrong.com/wiki/AnnaSalamon\">AnnaSalamon</a></h2>\n<ul>\n<li><a href=\"https://lesswrong.com/lw/16f/decision_theory_an_outline_of_some_upcoming_posts/\">Decision theory: An outline of some upcoming posts</a></li>\n<li><a href=\"https://lesswrong.com/lw/16i/confusion_about_newcomb_is_confusion_about/\">Confusion about Newcomb is confusion about counterfactuals</a></li>\n<li><a href=\"https://lesswrong.com/lw/174/decision_theory_why_we_need_to_reduce_could_would/\">Why we need to reduce “could”, “would”, “should”</a></li>\n<li><a href=\"https://lesswrong.com/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/\">Why Pearl helps reduce “could” and “would”, but still leaves us with at least three alternatives</a></li>\n</ul>\n<h2>Sequence by <a href=\"http://lesswrong.com/user/orthonormal/\">orthonormal</a> (Decision Theories: A Semi-Formal Analysis)</h2>\n<ul>\n<li><a href=\"https://lesswrong.com/lw/aq9/decision_theories_a_less_wrong_primer/\">Part 0: Decision Theories: A Less Wrong Primer</a></li>\n<li><a href=\"https://lesswrong.com/lw/axl/decision_theories_a_semiformal_analysis_part_i/\">Part I: The Problem with Naive Decision Theory</a></li>\n<li><a href=\"https://lesswrong.com/lw/az6/decision_theories_a_semiformal_analysis_part_ii/\">Part II: Causal Decision Theory and Substitution</a></li>\n<li><a href=\"https://lesswrong.com/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/\">Part III: Formalizing Timeless Decision Theory</a></li>\n</ul>\n<h2>See also</h2>\n<ul>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Instrumental_rationality\">Instrumental rationality</a></li>\n<li><a href=\"https://lesswrong.com/tag/causality\">Causality</a></li>\n<li><a href=\"https://lesswrong.com/tag/expected-utility\">Expected utility</a></li>\n<li><a href=\"https://lesswrong.com/tag/evidential-decision-theory\">Evidential Decision Theory</a></li>\n<li><a href=\"https://lesswrong.com/tag/timeless-decision-theory\">Timeless decision theory</a>, <a href=\"https://lesswrong.com/tag/updateless-decision-theory\">Updateless decision theory</a></li>\n<li><a href=\"https://lesswrong.com/tag/aixi\">AIXI</a></li>\n</ul>\n",
    "description_length": 6860,
    "viewCount": 965,
    "parentTagId": "world-modeling-mathematical-sciences"
  },
  {
    "core-tag": "World Modeling",
    "_id": "DCN2zNscbMZp5aatL",
    "name": "Information Theory",
    "slug": "information-theory",
    "postCount": 71,
    "description_html": null,
    "description_length": null,
    "viewCount": 98,
    "parentTagId": "world-modeling-mathematical-sciences"
  },
  {
    "core-tag": "World Modeling",
    "_id": "6nS8oYmSMuFMaiowF",
    "name": "Logic & Mathematics ",
    "slug": "logic-and-mathematics",
    "postCount": 496,
    "description_html": "<p><strong>Logic and Mathematics</strong> are deductive systems, where the conclusion of a successful argument follows necessarily from its premises, given the axioms of the system you’re using: number theory, geometry, predicate logic, etc.</p><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/valid-argument\">Valid argument</a> - An argument is valid when it contains no logical fallacies.</li><li><a href=\"https://www.lesswrong.com/tag/sound-argument\">Sound argument</a> - An argument that is valid and whose premises are all true. In other words, the premises are true and the conclusion necessarily follows from them, making the conclusion true as well.</li><li><a href=\"https://www.lesswrong.com/tag/formal-proof\">Formal proof</a> - A set of steps from axiom(s) and previous proof(s) which follows the rules of induction of a mathematical system.</li><li><a href=\"https://www.lesswrong.com/tag/logical-uncertainty\">Logical Uncertainty</a></li><li><a href=\"https://www.lesswrong.com/tag/logical-induction\">Logical Induction</a></li><li><a href=\"https://www.lesswrong.com/tag/probability-and-statistics\">Probability &amp; Statistics</a></li></ul>",
    "description_length": 1164,
    "viewCount": 267,
    "parentTagId": "world-modeling-mathematical-sciences"
  },
  {
    "core-tag": "World Modeling",
    "_id": "bh7uxTTqmsQ8jZJdB",
    "name": "Probability & Statistics",
    "slug": "probability-and-statistics",
    "postCount": 306,
    "description_html": null,
    "description_length": null,
    "viewCount": 215,
    "parentTagId": "world-modeling-mathematical-sciences"
  },
  {
    "core-tag": "World Modeling",
    "_id": "be2Mh2bddQ6ZaBcti",
    "name": "Prisoner's Dilemma",
    "slug": "prisoner-s-dilemma",
    "postCount": 69,
    "description_html": "<p>The<strong> Prisoner's Dilemma</strong> is a well-studied game in <a href=\"http://lesswrong.com/tag/game-theory\">game theory</a>, where supposedly rational incentive following leads to both players stabbing each other in the back and being worse off than if they had cooperated.</p><p>The original formulation, via <a href=\"https://en.wikipedia.org/wiki/Prisoner%27s_dilemma\">Wikipedia</a>:</p><blockquote><p>Two members of a criminal gang are arrested and imprisoned. Each prisoner is in solitary confinement with no means of communicating with the other. The prosecutors lack sufficient evidence to convict the pair on the principal charge, but they have enough to convict both on a lesser charge. Simultaneously, the prosecutors offer each prisoner a bargain. Each prisoner is given the opportunity either to betray the other by testifying that the other committed the crime, or to cooperate with the other by remaining silent. The possible outcomes are:</p></blockquote><blockquote><p>If A and B each betray the other, each of them serves two years in prison</p></blockquote><blockquote><p>If A betrays B but B remains silent, A will be set free and B will serve three years in prison</p></blockquote><blockquote><p>If A remains silent but B betrays A, A will serve three years in prison and B will be set free</p></blockquote><blockquote><p>If A and B both remain silent, both of them will serve only one year in prison (on the lesser charge).</p></blockquote><p>The \"stay silent\" option is generally called <strong>Cooperate</strong>, and the \"betray\" option is called <strong>Defect</strong>. The only Nash Equilibrium of the Prisoner's Dilemma is both players defecting, even though each would prefer the cooperate/cooperate outcome.</p><p>Notice that it's only if you treat the other player's decision as completely independent from yours, if the other player defects, then you score higher if you defect as well, whereas if the other player cooperates, you do better by defecting. Hence Nash Equilibrium to defect (at least if the game is to be played only once), and indeed, this is what classical causal decision theory says. And yet—and yet, if only somehow both players could agree to cooperate, they would both do better than if they both defected. If the players are <a href=\"https://wiki.lesswrong.com/wiki/timeless_decision_agent\">timeless decision agents</a>, or functional decision theory agents, &nbsp;they can.</p><p>A popular variant is the Iterated Prisoner's Dilemma, where two agents play the Prisoner's Dilemma against each other a number of times in a row. A simple and successful strategy is called Tit for Tat - cooperate on the first round, then on subsequent rounds do whatever your opponent did on the last round.</p><h2>External links</h2><ul><li><a href=\"http://plato.stanford.edu/entries/prisoner-dilemma/\"><u>Prisoner's dilemma</u></a> (Stanford Encyclopedia of Philosophy)</li></ul><h2>See also</h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Game_theory\"><u>Game theory</u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Decision_theory\"><u>Decision theory</u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Newcomb%27s_problem\"><u>Newcomb's problem</u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Counterfactual_mugging\"><u>Counterfactual mugging</u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Parfit%27s_hitchhiker\"><u>Parfit's hitchhiker</u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Smoking_lesion\"><u>Smoking lesion</u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Absentminded_driver\"><u>Absentminded driver</u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Pascal%27s_mugging\"><u>Pascal's mugging</u></a></li><li><a href=\"https://www.lesswrong.com/tag/coordination-cooperation\">Coordination/Cooperation</a></li></ul><h2>References</h2><ul><li>Drescher, Gary (2006). <i>Good and Real</i>. Cambridge: The MIT Press. ISBN 0262042339.</li></ul>",
    "description_length": 3951,
    "viewCount": 120,
    "parentTagId": "world-modeling-mathematical-sciences"
  },
  {
    "core-tag": "World Modeling",
    "_id": "NZB24aR9uHmDc5GcT",
    "name": "Sleeping Beauty Paradox",
    "slug": "sleeping-beauty-paradox",
    "postCount": 78,
    "description_html": "<p>The <strong>Sleeping Beauty Paradox</strong> is a question of how <a href=\"http://lesswrong.com/tag/anthropics\">anthropics</a> affects <a href=\"https://www.lesswrong.com/tag/probability-and-statistics\">probabilities</a>.</p><blockquote><p>Sleeping Beauty volunteers to undergo the following experiment. On Sunday she is given a drug that sends her to sleep. A fair coin is then tossed just once in the course of the experiment to determine which experimental procedure is undertaken. If the coin comes up heads, Beauty is awakened and interviewed on Monday, and then the experiment ends. If the coin comes up tails, she is awakened and interviewed on Monday, given a second dose of the sleeping drug, and awakened and interviewed again on Tuesday. The experiment then ends on Tuesday, without flipping the coin again. The sleeping drug induces a mild amnesia, so that she cannot remember any previous awakenings during the course of the experiment (if any). During the experiment, she has no access to anything that would give a clue as to the day of the week. However, she knows all the details of the experiment.</p></blockquote><blockquote><p>Each interview consists of one question, “What is your credence now for the proposition that our coin landed heads?”</p></blockquote><p>One argument says that since Beauty will see the same thing on waking whether the coin came up heads or not, what she sees on waking provides no evidence one way or the other about the coin, and therefore she should stick with the prior probability of one half.</p><p>Another argument replies that the two awakenings when the coin comes up tails imply that waking up itself should be considered evidence in favor of tails. Out of all possible situations where Beauty is asked the question, only one out of three has the coin showing heads. Therefore, one third.</p><p>A third argument tries to add rigor by considering monetary payoffs. If Beauty's bets about the coin get paid out once per experiment, some argue that she will do best by acting as if the probability is one half (while others argue that probability one third gives the correct result if decision theory is correctly applied). If instead the bets get paid out once per awakening, one can again argue about whether or not acting as if the probability is one third has the best expected value.</p><h2>External Links</h2><ul><li><a href=\"https://www.youtube.com/watch?v=XeSu9fBJ2sI\">Video by Veritasium</a></li><li><a href=\"https://www.youtube.com/watch?v=zL52lG6aNIY\">Video explanation by Julia Galef</a></li><li><a href=\"https://en.wikipedia.org/wiki/Sleeping_Beauty_problem\">Sleeping Beauty Problem on Wikipedia</a></li></ul><h2>See Also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/decision-theory\">Decision Theory</a></li><li><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem\">Newcomb's Problem</a></li><li><a href=\"https://www.lesswrong.com/tag/counterfactual-mugging\">Counterfactual Mugging</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Parfit%27s_hitchhiker\"><u>Parfit's hitchhiker</u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Smoker%27s_lesion\"><u>Smoker's lesion</u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Absentminded_driver\"><u>Absentminded driver</u></a></li><li><a href=\"https://www.lesswrong.com/tag/prisoner-s-dilemma\">Prisoner's Dilemma</a></li><li><a href=\"https://www.lesswrong.com/tag/pascal-s-mugging\">Pascal's Mugging</a></li></ul>",
    "description_length": 3448,
    "viewCount": 213,
    "parentTagId": "world-modeling-mathematical-sciences"
  },
  {
    "core-tag": "World Modeling",
    "_id": "XJjvxWB68GYpts93N",
    "name": "Nanotechnology",
    "slug": "nanotechnology",
    "postCount": 31,
    "description_html": "<p><strong>Nanotechnology</strong> is the field of study concerned with the manipulation of matter at an atomic and molecular scale. Typically, this involves structures with sizes ranging from 1 to 100 nanometres. It is currently one of the most well-funded areas worldwide. The term was first coined in 1974 by Norio Taniguchi.</p><p>The emergence of nanotechnology as a field by itself was the result of the convergence of several lines of work. These include the development of the <a href=\"http://en.wikipedia.org/wiki/Scanning_tunneling_microscope\">scanning tunneling microscope</a> by Gerd Binnig and Einrich Rohrer in 1980s, Richard Feynman's talk \"There's plenty of room at the Bottom\" in 1959 and Eric Drexler suggestions of molecular manipulation in the 70s.</p><p>The field of nanotechonology has led to the development of a huge amount of new technologies and the improvement of old methods. From drug-delivering systems to electronic chips development, there are nowadays hundreds of available functional applications stemming from this area. Besides the size and mobility advantages of such devices and technologies, the fundamental quantum properties that emerge at nano scales continue to defy researchers to speculate of further developments.</p><p>Drexler has proposed, with his <i>molecular nanotechnology</i>, that the field could evolve to exploit more than just this scale properties, this pure nanomaterials research. His suggestions, highly speculative, include research on the ability of developing means of mechanosynthesis - such as having miniature production lines using machines to build structures. This would allow, for example, the precise control of chemical reactions, eliminating the imprecision existing in conventional chemistry.</p><p>When <a href=\"http://intelligence.org/files/AIPosNegFactor.pdf\">discussing</a> the development of <a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a>, Yudkowsky proposes that the unrestricted access to nanotechnology by an <a href=\"https://www.lesswrong.com/tag/unfriendly-artificial-intelligence\">Unfriendly artificial intelligence</a> could have catastrophic results for mankind.</p><h2>Further Reading &amp; References</h2><ul><li>Binnig, G.; Rohrer, H. (1986). \"Scanning tunneling microscopy\". IBM Journal of Research and Development 30: 4.</li><li>\"Nanoscience and nanotechnologies: opportunities and uncertainties\". Royal Society and Royal Academy of Engineering. July 2004. Retrieved 13 May 2011.</li><li>Allhoff, Fritz; Lin, Patrick; Moore, Daniel (2010). What is nanotechnology and why does it matter?: from science to ethics. John Wiley and Sons. pp. 3–5. ISBN 1-4051-7545-1.</li><li><a href=\"http://www.zyvex.com/nanotech/feynman.html\">There's Plenty of Room at the Bottom</a> by Richard Feynman</li></ul><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/exploratory-engineering\">Exploratory engineering</a></li><li><a href=\"https://www.lesswrong.com/tag/rational-evidence\">Rational evidence</a>, <a href=\"https://www.lesswrong.com/tag/science\">Science</a></li></ul>",
    "description_length": 3083,
    "viewCount": 86,
    "parentTagId": "world-modeling-general-science"
  },
  {
    "core-tag": "World Modeling",
    "_id": "csMv9MvvjYJyeHqoo",
    "name": "Physics",
    "slug": "physics",
    "postCount": 236,
    "description_html": null,
    "description_length": null,
    "viewCount": 149,
    "parentTagId": "world-modeling-general-science"
  },
  {
    "core-tag": "World Modeling",
    "_id": "HFou6RHqFagkyrKkW",
    "name": "Programming",
    "slug": "programming",
    "postCount": 169,
    "description_html": null,
    "description_length": null,
    "viewCount": 190,
    "parentTagId": "world-modeling-general-science"
  },
  {
    "core-tag": "World Modeling",
    "_id": "2JdCpTrNgBMNpJiyB",
    "name": "Space Exploration & Colonization",
    "slug": "space-exploration-and-colonization",
    "postCount": 67,
    "description_html": null,
    "description_length": null,
    "viewCount": 57,
    "parentTagId": "world-modeling-general-science"
  },
  {
    "core-tag": "World Modeling",
    "_id": "22z6XpWKqw3bNv4oR",
    "name": "Simulation Hypothesis",
    "slug": "simulation-hypothesis",
    "postCount": 81,
    "description_html": "<p>The <strong>Simulation Hypothesis</strong> proposes that conscious beings could be immersed within an artificial Universe embedded within a higher order of reality. The roots of this argument can be found throughout the history of philosophy in such works as Plato's \"<a href=\"https://en.wikipedia.org/wiki/The_Allegory_of_the_Cave\">Allegory of the Cave</a>\" and Descartes \"<a href=\"https://en.wikipedia.org/wiki/Evil_demon\">evil demon</a>\".</p><p>The important distinction between these and modern <a href=\"https://www.lesswrong.com/tag/simulation-argument\">Simulation Arguments</a> has been the addition of proposed methods of engineering Simulated Reality through the use of computers. The modern <a href=\"https://www.lesswrong.com/tag/simulation-argument\">Simulation Argument</a> makes the case that since a civilization will be able to simulate many more ancient civilizations than there were ancient civilizations, it is more likely that we are in a simulated universe than not. It shows that the belief that there is a significant chance that we will one day become posthumans who run ancestor simulations is false, unless we are currently living in a simulation.</p><p>John Barrow <a href=\"http://www.simulation-argument.com/barrowsim.pdf\">has suggested</a> that if we are living in a computer simulation we may observe \"glitches\" in our programmed environment due to the level of detail being compromised to save computing power. Alternatively, the Simulators may not have a full understanding of the Laws of Nature which would mean over time the simulated environment would drift away from its stable state. These \"glitches\" could be identified by scientists scrutinizing nature using unusual methods of observation. However, <a href=\"https://www.lesswrong.com/tag/nick-bostrom\">Nick Bostrom</a> <a href=\"http://www.simulation-argument.com/simulation.pdf\">argues</a> that it is extremely likely that a civilization will have computational powers far surpassing the ones needed to simulate an ancient civilization in great detail. Moreover, one can argue that due to exponential grow, it's extremely unlikely that the simulators are in the region of progress where they already can simulate an artificial reality but can't simulate it with finer detail. They either can't simulate at all, or have computational powers that far exceed the needed amount.</p><h2>External links</h2><ul><li>Barrow, John (2008) <a href=\"http://www.simulation-argument.com/barrowsim.pdf\">Living in a Simulated Universe</a> Universe or Multiverse? ed. Bernard Carr (Cambridge University Press): pp. 481-486.</li><li><a href=\"http://hplusmagazine.com/2011/01/18/is-god-an-alien-mathematician/\">Is God an Alien Mathematician?</a> — A discussion between Ben Goertzel and Hugo de Garis on Simulated Universes and their Creators</li><li><a href=\"http://www.kurzweilai.net/from-cosmism-to-deism\">From cosmism to deism</a> — Hugo de Garis's essay on Simulated Universes</li><li><a href=\"https://en.wikipedia.org/wiki/Allegory_of_the_Cave\">The Allegory of the Cave</a> on Wikipedia</li><li><a href=\"https://en.wikipedia.org/wiki/Evil_demon\">Evil demon</a> on Wikipedia</li></ul><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/simulation-argument\">Simulation Argument</a></li></ul>",
    "description_length": 3277,
    "viewCount": 187,
    "parentTagId": "world-modeling-general-science"
  },
  {
    "core-tag": "World Modeling",
    "_id": "25oxqHiadqM6Hf7Gn",
    "name": "Great Filter",
    "slug": "great-filter",
    "postCount": 40,
    "description_html": "<p>The <strong>Great Filter</strong> is a proposed reframing of the <a href=\"http://en.wikipedia.org/wiki/Fermi_paradox\">Fermi Paradox</a>, introduced by Robin Hanson in his 1998 essay <a href=\"http://hanson.gmu.edu/greatfilter.html\">The Great Filter - Are We Almost Past It?</a>.</p>\n<p>The development of space-framing intelligent life requires many steps to occur in sequence, such as the emergence of single-celled life and the transition from unicellular to multicellular life forms. Since we have not observed intelligent life beyond our planet, Hanson argues that there se<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span>ems to be a developmental step that is so difficult and unlikely that it \"filters out\" nearl<span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\"><span class=\"mjx-mrow\" aria-hidden=\"true\"></span></span></span></span>y all civilizations before they can reach a space-faring stage — a \"great filter\".</p>\n<p>From Hanson's essay:</p>\n<blockquote>\n<p>Humanity seems to have a bright future, i.e., a non-trivial chance of expanding to fill the universe with lasting life. But the fact that space near us seems dead now tells us that any given piece of dead matter faces an astronomically low chance of begating such a future. There thus exists a great filter between death and expanding lasting life, and humanity faces the ominous question: how far along this filter are we?</p>\n</blockquote>\n<h2>Should we worry?</h2>\n<p>If there is a \"Great Filter\", then this filter might be a step in our evolutionary past, in which case our civilization has already passed it.</p>\n<p>But the hard step might also be ahead of us: <a href=\"https://www.global-catastrophic-risks.com/docs/Chap01.pdf\">surviving</a> the creation of <a href=\"https://wiki.lesswrong.com/wiki/AGI\">AGI</a>, future biotechnology, <a href=\"https://lesswrong.com/tag/nanotechnology\">nanotechnology</a>, or some unknown risk. In that case, we should be worried, as the Great Filter seems to have been successful in stopping the development of every other civilization so far.</p>\n<p>This suggests that estimating the location of the Great Filter may be important for helping estimate the magnitude of <a href=\"https://lesswrong.com/tag/existential-risk\">existential risk</a>. <a href=\"http://hanson.gmu.edu/greatfilter.html\">Many</a> <a href=\"http://hanson.gmu.edu/hardstep.pdf\">efforts</a> <a href=\"http://www.stat.berkeley.edu/~aldous/Papers/GF.pdf\">have</a> <a href=\"http://www.nickbostrom.com/papers/fermi.pdf\">been</a> <a href=\"http://www.global-catastrophic-risks.com/docs/Chap01.pdf\">made</a> <a href=\"http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/\">in</a> that direction, but much remains uncertain.</p>\n<p>If we discovered traces of life on other planets, this would count as <a href=\"https://www.youtube.com/watch?v=_W8zu7lFmhY&amp;themeRefresh=1\">evidence</a> for a later Great Filter. Finding that complex life had evolved independently both on Earth and some other nearby planet, would suggest that getting to such a developmental stage was relatively easy. Thus the Great Filter would almost certainly have to be at a later stage.</p>\n<p>The study of <a href=\"http://en.wikipedia.org/wiki/Extinction_event#Major_extinction_events\">past mass extinctions</a> and astrobiology can provide ideas for estimating the location of a Great Filter. However, there are many difficulties involved. For instance, the time that it takes to pass a step doesn't reveal much about how easy or hard that step was. Robin Hanson gives the following example in his <a href=\"http://hanson.gmu.edu/greatfilter.html\">paper</a>:</p>\n<blockquote>\n<p>… [S]ay you have one hour to pick five locks by trial and error, locks with 1,2,3,4, and 5 dials of ten numbers, so that the expected time to pick each lock is .01,.1, 1, 10, and 100 hours respectively. Then just looking at those rare cases when you do pick all five locks in the hour, the average time to pick the first two locks would be .0096 and .075 hours respectively, close to the usual expected times of .01 and .1 hours. The average time to pick the third lock, however, would be .20 hours, and the average time for the other two locks, and the average time left over at the end, would be .24 hours. That is, conditional on success, all the hard steps, no matter how hard, take about the same time, while easy steps take about their usual time.</p>\n</blockquote>\n<h2>Follow-ups</h2>\n<p><a href=\"http://hanson.gmu.edu/hardstep.pdf\">In a subsequent paper</a>, Hanson constructs a simulation of the distribution of the hard steps, which suggests that there should be about four to seven hard steps, uniformly distributed in our past — a series of lesser filters, rather than a single \"great\" one.</p>\n<p>Hanson's follow-up also suggests that there has been at least one hard step since the evolution of hominids, and that the best extinction model that fits all these requirements is <a href=\"http://www.pnas.org/content/91/15/6735.full.pdf\">William Schopf's model</a>.</p>\n<p>Taking evolutionary arguments for <a href=\"https://wiki.lesswrong.com/wiki/AGI\">AGI</a> and <a href=\"https://lesswrong.com/tag/observation-selection-effect\">observation selection effects</a> together, <a href=\"http://www.nickbostrom.com/aievolution.pdf\">Bostrom and Shulman argue</a> that Hanson’s results can help estimate the difficulty of creating AGI.</p>\n<h2>Blog posts</h2>\n<ul>\n<li><a href=\"http://www.overcomingbias.com/2010/03/very-bad-news.html\">Very Bad News</a> by <a href=\"https://lesswrong.com/tag/robin-hanson\">Robin Hanson</a></li>\n<li><a href=\"https://lesswrong.com/lw/1z8/an_empirical_test_of_anthropic_principle_great/\">An empirical test of anthropic principle / great filter reasoning</a> by James Miller</li>\n<li><a href=\"https://lesswrong.com/lw/1zj/sia_wont_doom_you/\">SIA won't doom you</a> by Stuart Armstrong</li>\n<li><a href=\"https://lesswrong.com/lw/214/late_great_filter_is_not_bad_news/\">Late Great Filter is not bad news</a> by Wei Dai</li>\n<li><a href=\"https://lesswrong.com/lw/7w8/planets_in_the_habitable_zone_the_drake_equation/\">Planets in the habitable zone, the Drake Equation, and the Great Filter</a> by JoshuaZ</li>\n<li><a href=\"http://www.overcomingbias.com/2010/11/beware-future-filters.html\">Beware Future Filters</a> by <a href=\"https://lesswrong.com/tag/robin-hanson\">Robin Hanson</a></li>\n</ul>\n<h2>External links</h2>\n<ul>\n<li>Robin Hanson’s Great Filter original paper: <a href=\"http://hanson.gmu.edu/greatfilter.html\">The Great Filter - Are We Almost Past It?</a></li>\n<li>A simulation of the hard steps distribution: <a href=\"http://hanson.gmu.edu/hardstep.pdf\">Must Early Life Be Easy? The Rhythm of Major Evolutionary Transitions</a> by Robin Hanson</li>\n<li>Strong candidates for present Great Filters: <a href=\"http://www.global-catastrophic-risks.com/docs/Chap01.pdf\">Introduction of the book “Global Catastrophic Risks”, summarizing it</a> by Nick Bostrom</li>\n<li><a href=\"http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/\">SIA Doomsday: The filter is ahead</a> by Katja Grace</li>\n<li>An audio with Bostrom talking about how finding traces of life on mars is terrible bad news: <a href=\"http://www.youtube.com/watch?v=_W8zu7lFmhY\">Nick Bostrom on life on Mars</a></li>\n</ul>\n<h2>See also</h2>\n<ul>\n<li><a href=\"https://lesswrong.com/tag/existential-risk\">Existential risk</a></li>\n<li><a href=\"https://lesswrong.com/tag/doomsday-argument\">Doomsday argument</a></li>\n<li><a href=\"https://lesswrong.com/tag/self-indication-assumption\">Self Indication Assumption</a></li>\n</ul>\n",
    "description_length": 25139,
    "viewCount": 89,
    "parentTagId": "world-modeling-general-science"
  },
  {
    "core-tag": "World Modeling",
    "_id": "PDJ6KqJBRzvKPfuS3",
    "name": "Economics",
    "slug": "economics",
    "postCount": 472,
    "description_html": "<p><strong>Economics</strong> is the social science that studies how humans and other agents interact in a universe with scarce resources. It deals with topics such as trade, specialization of labor, accumulation of capital, technology, and resource consumption. Agents in economics are generally assumed to have utility functions, which they try to maximize under various constraints.</p><p>Economics is usually separated into microeconomics and macroeconomics. Microeconomics concerns the behavior of agents as they interact in a market. More narrowly, it studies the price mechanism, a decentralized system of allocating goods and services based on an evolving system of prices and trade, which all actors in a market economy contribute towards. The price mechanism is closely related to the concept of the <a href=\"https://en.wikipedia.org/wiki/Invisible_hand\">invisible hand</a>, first introduced by <a href=\"https://en.wikipedia.org/wiki/Adam_Smith\">Adam Smith</a>. <a href=\"https://www.lesswrong.com/tag/game-theory\">Game theory</a> is the mathematical study of rational agency, which formalizes many standard results in microeconomics.</p><p>Macroeconomics concerns the aggregate behavior of entire economies. For example, it studies economic growth, inflation, international trade and unemployment. An ongoing debate concerns to what extent the <a href=\"https://www.lesswrong.com/tag/economic-consequences-of-agi\">impacts of artificial intelligence</a> should be viewed through the lens of economics.</p>",
    "description_length": 1513,
    "viewCount": 451,
    "parentTagId": "world-modeling-social-economic"
  },
  {
    "core-tag": "World Modeling",
    "_id": "jgcAJnksReZRuvgzp",
    "name": "Financial Investing",
    "slug": "financial-investing",
    "postCount": 167,
    "description_html": "<p>The <strong>Financial Investing</strong> tag covers concrete personal investment advice, specific investment opportunities (like Bitcoin), and analysis of existing financial investing practices, as well as broad analyses of things like the efficient market hypothesis.</p>",
    "description_length": 275,
    "viewCount": 803,
    "parentTagId": "world-modeling-social-economic"
  },
  {
    "core-tag": "World Modeling",
    "_id": "bY5MaF2EATwDkomvu",
    "name": "History",
    "slug": "history",
    "postCount": 247,
    "description_html": "<p><strong>History</strong>: \"Why should I remember the Wright Brothers’ first flight? I was not there. But as a rationalist, could I dare to not remember, when the event actually happened? Is there so much difference between seeing an event through your eyes—which is actually a causal chain involving reflected photons, not a direct connection—and seeing an event through a history book? Photons and history books both descend by causal chains from the event itself.\" - Eliezer Yudkowsky, <a href=\"https://www.lesswrong.com/posts/TLKPj4GDXetZuPDH5/making-history-available\"><i>Making History Available</i></a>.</p><p><strong>Related Pages:</strong> <a href=\"https://www.lesswrong.com/tag/history-of-rationality\">History of Rationality</a>, <a href=\"https://www.lesswrong.com/tag/history-of-less-wrong\">History of Less Wrong</a></p>",
    "description_length": 833,
    "viewCount": 182,
    "parentTagId": "world-modeling-social-economic"
  },
  {
    "core-tag": "World Modeling",
    "_id": "FkzScn5byCs9PxGsA",
    "name": "Politics",
    "slug": "politics",
    "postCount": 518,
    "description_html": null,
    "description_length": null,
    "viewCount": 279,
    "parentTagId": "world-modeling-social-economic"
  },
  {
    "core-tag": "World Modeling",
    "_id": "sPpZRaxpNNJjw55eu",
    "name": "Progress Studies",
    "slug": "progress-studies",
    "postCount": 318,
    "description_html": "<p><strong>Progress Studies</strong> is the study of the causes of civilizational progress, e.g., the combination of economic, technological, scientific, and cultural advancements that have transformed human life and raised standards of living over the past couple of centuries.</p><blockquote><p><i>The bicycle, as we know it today, was not invented until the late 1800s. Yet it was a simple mechanical invention. It would seem to require no brilliant inventive insight, and certainly no scientific background.&nbsp;</i><br><i>Why, then, wasn’t it invented much earlier? – </i><a href=\"https://www.lesswrong.com/posts/TPytnFcWiD2E4cTrm/why-did-we-wait-so-long-for-the-bicycle\"><i><u>Why did we wait so long for the bicycle?</u></i></a></p></blockquote><h2>See also:</h2><ul><li><a href=\"https://www.lesswrong.com/tag/history\">History</a></li><li><a href=\"https://www.lesswrong.com/tag/industrial-revolution\">Industrial Revolution</a></li><li><a href=\"https://www.lesswrong.com/tag/intellectual-progress-society-level\">Intellectual Progress (society level)</a></li></ul><h2>Origin of the Name</h2><p><i>Progress Studies </i>was proposed as an academic field by Tyler Cowen and Patrick Collison [<a href=\"https://www.theatlantic.com/science/archive/2019/07/we-need-new-science-progress/594946/\"><u>1</u></a>] after they noticed that there’s no intellectual movement focused on understanding the dynamics of progress, or on trying to speed it up.</p><h2>External Resources</h2><ul><li><a href=\"https://rootsofprogress.org/about\"><i>Roots of Progress</i></a><i> </i>is a blog by <a href=\"https://www.lesswrong.com/users/jasoncrawford\">jasoncrawford</a> that explores the history of technology and industry alongside the philosophical questions of human progress. Many of the blogs posts are crossposted to LessWrong.</li><li><a href=\"https://progressforum.org\">The Progress Forum</a> is a forum dedicated to progress studies and the philosophy of progress. It uses the same design as LessWrong. <a href=\"https://www.lesswrong.com/posts/ubs5q4MpsgWBr2Tzw/announcing-the-progress-forum\">Its announcement can be read here</a>.</li></ul>",
    "description_length": 2130,
    "viewCount": 308,
    "parentTagId": "world-modeling-social-economic"
  },
  {
    "core-tag": "World Modeling",
    "_id": "gHCNhqxuJq2bZ2akb",
    "name": "Social & Cultural Dynamics",
    "slug": "social-and-cultural-dynamics",
    "postCount": 361,
    "description_html": null,
    "description_length": null,
    "viewCount": 156,
    "parentTagId": "world-modeling-social-economic"
  },
  {
    "core-tag": "World Modeling",
    "_id": "ogWsaHQKwa6ddidRC",
    "name": "Conflict vs Mistake",
    "slug": "conflict-vs-mistake",
    "postCount": 22,
    "description_html": "<p><strong>Conflict vs Mistake</strong> is a framework for analyzing disagreements about policy.</p><p>Mistake theorists think problems in society are caused by people being bad at achieving common goals. Conflict theorists think problems in society are caused by adversaries with incompatible goals.</p><p>Scott Alexander <a href=\"https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/\">attributed</a> the conflict vs mistake framework to <a href=\"https://www.reddit.com/r/slatestarcodex/comments/74vpwm/socialism_communism_and_marxism_pt_1_on_trust_and/\">a post on reddit by user no_bear_so_low</a>.</p><p>A <strong>conflict theorist</strong> thinks problems are primarily due to the conflicting interests of different players. If someone is suffering, someone else must be making money off of it. Karl Marx was a conflict theorist; he blamed the ills of society on class conflict.</p><p>A <strong>mistake theorist</strong> thinks problems are primarily due to mistakes. If only we knew how to run society better, there would be less problems. Jeremy Bentham was more of a mistake theorist: he thought producing a formula by which we could calculate the quality of social interventions would help improve society.</p><p><a href=\"https://www.lesswrong.com/posts/PBRWb2Em5SNeWYwwB/humans-are-not-automatically-strategic\">Humans are not automatically strategic</a> is a mistake theory of human (ir)rationality. Things are hard. If people are doing something dumb, it's probably because they don't know better.</p><p><a href=\"https://www.lesswrong.com/posts/BgBrXpByCSmCLjpwr/book-review-the-elephant-in-the-brain\">The Elephant in the Brain</a> is more like a conflict theory of human (ir)rationality. Apparent irrationality is attributed mainly to humans not actually wanting what they think they want. </p><p><strong><a href=\"https://en.m.wikipedia.org/wiki/Hanlon%27s_razor\">Hanlon's Razor</a></strong> says: <em>Never attribute to malice what is adequately explained by stupidity.</em> This is a clear bias toward mistake theory.</p><p>On the other hand, economics, evolutionary psychology, and some other fields are based on <em>rational choice theory</em>, IE, an assumption that behavior can be explained by rational decision-making. <em>(Economic rationality assumes that individuals choose rationally to maximize economic value, based on the incentives of the current situation. Evolutionary psychology instead assumes that human and animal behaviors will be optimal solutions to the problems they faced in evolutionary history. Bruce Bueno de Mesquita assumes that politicians act rationally so as to maximize their tenure in positions of power. The ACT-R theory of cognition assumes that individual cognitive mechanisms are designed to optimally perform their individual cognitive tasks, such as retrieving memories which are useful in expectation, even if the whole brain is not perfectly rational.)</em> This assumption of rationality lends itself more naturally to conflict theories.</p><h2>Game-Theoretic Connections</h2><p>In game theory, assuming that people can make mistakes (a so-called <a href=\"https://en.m.wikipedia.org/wiki/Trembling_hand_perfect_equilibrium\">trembling hand</a>) can complicate cooperative strategies. </p><p>For example, in iterated <a href=\"https://www.lesswrong.com/tag/prisoner-s-dilemma\">prisoner's dilemma</a>, <strong>tit for tat </strong>is a cooperative equilibrium (that is to say, it is pareto-optimal, and it is a Nash equilibrium). The tit-for-tat strategy is: cooperate on the first round; then, copy the other person's move from the previous round. This enforces cooperation, because if I defect, I expect my partner to defect on the next round (which is bad for me). This is effectively eye-for-an-eye morality.</p><p>However, if people make mistakes (the trembling-hand assumption), then tit-for-tat only results in cooperation for an initial period before anyone makes a mistake. If both mistakes are equally probable, then in the long run we'll average only 50% cooperation. We can see this as an interminable family feud where both sides see the other as having done more wrong. \"An eye for an eye makes everyone blind.\"</p><p>We need to recognize that people make mistakes sometimes -- we can't punish everything eye-for-an-eye.</p><p>Therefore, some form of <em>forgiving</em> tit-for-tat does better. For example, copy cooperation 100% of the time, but copy defection 90% of the time. This can still work to enforce rational cooperation (depending on the exact payouts and time-discounting of the players), but without everlasting feuds. See also <a href=\"https://www.lesswrong.com/posts/2meuc3kPRkBcRpj3R/contrite-strategies-and-the-need-for-standards\">Contrite Strategies and the Need for Standards</a>.</p><p>In this framing, a conflict theorist thinks people are actually defecting on purpose. They <em>know what they're doing</em>, and therefore, <em>would respond to incentives.</em> Punishing them is prosocial and helps to encourage more cooperation overall.</p><p>A mistake theorist thinks people <em>are defecting accidentally, </em>and therefore, <em>would not respond to incentives</em>. Punishing them is pointless and counterproductive; it could even result in a continuing feud, making things much worse for everyone.</p>",
    "description_length": 5302,
    "viewCount": 199,
    "parentTagId": "world-modeling-social-economic"
  },
  {
    "core-tag": "World Modeling",
    "_id": "8XiMxJaWbjNtWLsEj",
    "name": "Cost Disease",
    "slug": "cost-disease",
    "postCount": 9,
    "description_html": "<p><strong>Cost Disease</strong> or <strong>Baumol's cost disease </strong>is a name for the rise of salaries in jobs that have experienced no or low increase of labor productivity [<a href=\"https://en.wikipedia.org/wiki/Baumol%27s_cost_disease\">1</a>]. Some use the term generally to refer to rising costs in general [<a href=\"https://www.lesswrong.com/posts/BBQ5HEnL3ShefQxEj/considerations-on-cost-disease\">2</a>].</p><p>Often the questions being asked in Cost Disease discussion are why the cost healthcare and education have increased many, many times over.</p><p><strong>External Posts:</strong><br><a href=\"https://slatestarcodex.com/2019/06/10/book-review-the-prices-are-too-dmn-high/\">Book Review: Why Are The Prices So D*mn High? </a>- Scott Alexander<br>&nbsp;</p>",
    "description_length": 775,
    "viewCount": 27,
    "parentTagId": "world-modeling-social-economic"
  },
  {
    "core-tag": "World Modeling",
    "_id": "pnSDArjzAjkvAF5Jo",
    "name": "Efficient Market Hypothesis",
    "slug": "efficient-market-hypothesis",
    "postCount": 50,
    "description_html": null,
    "description_length": null,
    "viewCount": 85,
    "parentTagId": "world-modeling-social-economic"
  },
  {
    "core-tag": "World Modeling",
    "_id": "HkiwLtMRLxpBa6zs5",
    "name": "Industrial Revolution",
    "slug": "industrial-revolution",
    "postCount": 38,
    "description_html": "<html><head></head><body><p>The <strong>Industrial Revolution </strong>was a set of economic and social changes that occurred in Europe and the United States in the 18th and 19th centuries, characterised by a transition from an \"agrarian and handicraft economy to one dominated by industry and machine manufacturing\" [<a href=\"https://www.britannica.com/event/Industrial-Revolution\">1</a>].&nbsp;</p><p>See also: <a href=\"https://www.lesswrong.com/tag/history\">History</a></p></body></html>",
    "description_length": 490,
    "viewCount": 31,
    "parentTagId": "world-modeling-social-economic"
  },
  {
    "core-tag": "World Modeling",
    "_id": "mf8wHrMrJR73uyDLQ",
    "name": "Moral Mazes",
    "slug": "moral-mazes",
    "postCount": 50,
    "description_html": null,
    "description_length": null,
    "viewCount": 186,
    "parentTagId": "world-modeling-social-economic"
  },
  {
    "core-tag": "World Modeling",
    "_id": "Q6P8jLn8hH7kbuXRr",
    "name": "Signaling",
    "slug": "signaling",
    "postCount": 84,
    "description_html": "<p><strong>Signaling</strong> is <a href=\"https://lesswrong.com/lw/did/what_is_signaling_really/\">defined</a> by <a href=\"https://wiki.lesswrong.com/wiki/Yvain\">Yvain</a> as \"a method of conveying information among not-necessarily-trustworthy parties by performing an action which is more likely or less costly if the information is true than if it is not true\". Some signaling is performed exclusively to impress others (to improve your <a href=\"https://lesswrong.com/tag/social-status\">status</a>), and in some cases <a href=\"http://www.overcomingbias.com/2007/01/excess_signalin.html\">isn't even worth that</a>. In other cases, signaling is a side-effect of an otherwise useful activity.</p>\n<p>For example, if doing something is easy for one type of person and hard for another type of person, you might do that thing just to get people to think you're the former type of person, even if the thing isn't in itself worth doing. This could explain many facets of human behavior, and reveal opportunities for reducing waste.</p>\n<p>Not all signaling is about abilities. Signaling can also be about personality, current emotional state, beliefs, loyalty to a particular group, status within a group, etc.</p>\n<p><strong>Countersignaling</strong> is signaling that a naive observer might take to mean that one is the <em>opposite</em> of X, when in fact, one is X, used as a means to signal that one is, in fact, X. For example, aristocrats (\"old money\") may forgo gaudy bling in order to signal that they are not <em>nouveau riche</em> (new money), which may lead some people to incorrectly assume that they are not rich.</p>\n<h2>Blog posts</h2>\n<p>by <a href=\"https://lesswrong.com/tag/robin-hanson\">Robin Hanson</a></p>\n<ul>\n<li><a href=\"http://www.overcomingbias.com/2006/12/do_helping_prof.html\">Do Helping Professions Help More?</a> and <a href=\"http://www.overcomingbias.com/2006/12/gifts_hurt.html\">Gifts Hurt</a></li>\n<li><a href=\"http://www.overcomingbias.com/2007/01/excess_signalin.html\">Excess Signaling Example</a></li>\n<li><a href=\"http://www.overcomingbias.com/2009/01/a-tale-of-two-tradeoffs.html\">A Tale Of Two Tradeoffs</a></li>\n<li><a href=\"http://www.overcomingbias.com/2009/06/why-signals-are-shallow.html\">Why Signals Are Shallow</a> - \"We all want to affiliate with high status people, but since status is about common distant perceptions of quality, we often care more about what distant observers would think about our associates than about how we privately evaluate them.\"</li>\n<li><a href=\"http://www.overcomingbias.com/2009/06/signals-are-forever.html\">Signals Are Forever</a></li>\n<li><a href=\"https://lesswrong.com/lw/g7/least_signaling_activities/\">Least Signaling Activities?</a></li>\n</ul>\n<p>by others</p>\n<ul>\n<li><a href=\"https://lesswrong.com/lw/did/what_is_signaling_really/\">What Is Signaling, Really?</a> by <a href=\"https://wiki.lesswrong.com/wiki/Yvain\">Yvain</a></li>\n<li><a href=\"https://lesswrong.com/lw/1y3/think_before_you_speak_and_signal_it/\">Think Before You Speak (And Signal It)</a> by <a href=\"http://weidai.com/\">Wei Dai</a></li>\n<li><a href=\"https://lesswrong.com/lw/b2/declare_your_signaling_and_hidden_agendas/\">Declare Your Signaling and Hidden Agendas</a> by <a href=\"https://wiki.lesswrong.com/wiki/Kaj_Sotala\">Kaj Sotala</a></li>\n<li><a href=\"https://lesswrong.com/lw/8ev/modularity_signaling_and_belief_in_belief/\">Modularity, Signaling, and Belief in Belief</a> by Kaj Sotala</li>\n</ul>\n<h2>See also</h2>\n<ul>\n<li><a href=\"https://lesswrong.com/tag/social-status\">Status</a></li>\n<li><a href=\"https://lesswrong.com/tag/near-far-thinking\">Near/far thinking</a></li>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Adaptation_executers\">Adaptation executers</a>, <a href=\"https://lesswrong.com/tag/superstimuli\">Superstimulus</a></li>\n<li><a href=\"https://lesswrong.com/tag/goodhart-s-law\">Goodhart's law</a></li>\n</ul>\n<h2>External links</h2>\n<ul>\n<li><a href=\"http://www.econtalk.org/archives/2008/05/hanson_on_signa.html\">Robin Hanson on Signaling (Econtalk Podcast)</a></li>\n</ul>\n",
    "description_length": 4043,
    "viewCount": 145,
    "parentTagId": "world-modeling-social-economic"
  },
  {
    "core-tag": "World Modeling",
    "_id": "EnFKSZYiDHqMJuvJL",
    "name": "Social Reality",
    "slug": "social-reality",
    "postCount": 59,
    "description_html": "<html><head></head><body><p><strong>Social Reality</strong> is \"that which exists, proportional to how much people believe in it\" (contrasted with \"regular reality\", which is \"that which exists, whether you believe in it or not\"). It includes both the rules that govern social interaction, and \"beliefs\" that people adopt as part of tribal membership.</p></body></html>",
    "description_length": 369,
    "viewCount": 57,
    "parentTagId": "world-modeling-social-economic"
  },
  {
    "core-tag": "World Modeling",
    "_id": "2EFq8dJbxKNzforjM",
    "name": "Social Status",
    "slug": "social-status",
    "postCount": 112,
    "description_html": "<p><strong>Social Status </strong>is an abstraction to model how people relate to each other, how social hierarchies are formed, and how people facilitate trade in the absence of financial accounting (as well as a variety of other stuff). I mean, everyone knows what status is, but here is where we break that down into its components and really try to understand what's happening on a mechanistic level.</p><h2>See Also</h2><ul><li><a href=\"https://lessestwrong.com/tag/signaling\">Signaling</a></li></ul><h2>Notable Posts</h2><ul><li><a href=\"https://lessestwrong.com/lw/13s/the_nature_of_offense/\">The Nature of Offense</a> by <a href=\"http://weidai.com/\">Wei Dai</a> - People are <a href=\"https://lessestwrong.com/tag/offense\">offended</a> by grabs for status.</li><li><a href=\"https://lessestwrong.com/lw/154/why_real_men_wear_pink/\">Why Real Men Wear Pink</a> by <a href=\"https://wiki.lesswrong.com/wiki/Yvain\">Yvain</a></li><li><a href=\"http://www.overcomingbias.com/2009/08/actors-see-status.html\">Actors See Status</a> by <a href=\"https://lessestwrong.com/tag/robin-hanson\">Robin Hanson</a>, quoting <a href=\"https://en.wikipedia.org/wiki/Keith_Johnstone\">Keith Johnstone</a></li><li><a href=\"https://lessestwrong.com/lw/1kr/that_other_kind_of_status/\">That Other Kind of Status</a> by Yvain</li></ul><h2>External</h2><ul><li>Melting Asphault (by Kevin Simler) has many great posts on status</li><li>Elephant in the Brain by Simler and Hanson</li><li>Impro (book on improv covering status relations)</li><li>Writings by Venkatesh Rao such as Gervais Principle and something, something Psychopath</li></ul>",
    "description_length": 1613,
    "viewCount": 198,
    "parentTagId": "world-modeling-social-economic"
  },
  {
    "core-tag": "World Modeling",
    "_id": "t7t9nW6BtJhfGNSR6",
    "name": "Aging",
    "slug": "aging",
    "postCount": 68,
    "description_html": null,
    "description_length": null,
    "viewCount": 183,
    "parentTagId": "world-modeling-biological-psychological"
  },
  {
    "core-tag": "World Modeling",
    "_id": "jaf5zfcGgCB2REXGw",
    "name": "Biology",
    "slug": "biology",
    "postCount": 220,
    "description_html": null,
    "description_length": null,
    "viewCount": 162,
    "parentTagId": "world-modeling-biological-psychological"
  },
  {
    "core-tag": "World Modeling",
    "_id": "XSryTypw5Hszpa4TS",
    "name": "Consciousness",
    "slug": "consciousness",
    "postCount": 289,
    "description_html": "<p>The word \"<strong>consciousness</strong>\" is used in a variety of different ways, and there are large disagreements about the reality and nature (and even coherence) of some of the things people profess to mean by \"consciousness.\"</p><p>Colloquially, the word \"conscious\" is used to pick out a few different things:</p><ul><li><strong>Wakefulness </strong>- The property that distinguishes, e.g., a person who is awake from a person who is asleep.<ul><li>We call people \"unconscious\" in this sense based on observed features like \"sharply reduced mobility,\" though we wouldn't normally call someone unconscious if we think they're merely <i>paralyzed</i>. Instead, calling someone \"unconscious\" tends to imply reduced ability to perceive and/or reason about events in one's environment.</li><li>An unconscious person (in this sense) might or might not be dreaming; and if dreaming, they might or might not be lucid.</li></ul></li><li><strong>Having experiences </strong>- The property that distinguishes, e.g., a comatose person who is having experiences from a comatose person who is not having experiences.</li><li><strong>Knowledge, perception, </strong>and/or <strong>attention</strong> - E.g., we might say that someone becomes \"conscious of\" a fact when they first learn that fact. Or we might say that they become \"conscious of\" something whenever they're currently perceiving it, or whenever they're <i>paying attention</i> to it.</li><li><strong>Meta-cognition </strong>or <strong>reflective awareness</strong> - Knowing, perceiving, and/or attending to your own mental states; or knowing, perceiving, and/or attending to <i>the fact that</i> you have certain mental states.<ul><li>E.g., we might say that someone is less \"conscious\" when they're fully immersed in a novel than when they're thinking about their own experiences, directing attention to the fact that they're reading a book, etc.</li></ul></li><li><strong>Self-awareness</strong> - Knowing, perceiving, and/or attending to your own existence or your own central properties.<ul><li>Depending on what exactly is meant by \"self-awareness,\" the \"immersed in a novel\" example might also involve less self-awareness. In some weaker senses of \"self-aware,\" one might instead claim that humans who are experiencing anything are always \"self-aware.\"</li></ul></li></ul><p>Reasonably mainstream academic overviews of \"consciousness\" can be found in the <a href=\"https://plato.stanford.edu/entries/consciousness/\"><i>Stanford Encyclopedia of Philosophy</i></a> and the <a href=\"http://www.mkdavies.net/Martin_Davies/Mind_files/ConsciousnessMITECS.pdf\"><i>MIT Encyclopedia of the Cognitive Sciences</i></a>.</p><p>This tag is <i>tentatively and provisionally</i> about the \"<strong>having experiences</strong>\" meaning(s) of \"consciousness.\" For wakefulness and dreaming, see <a href=\"https://www.lesswrong.com/tag/sleep\">sleep</a>. For knowledge, perception, and attention, see <a href=\"https://www.lesswrong.com/tag/attention\">attention</a> and <a href=\"https://www.lesswrong.com/tag/cognitive-science\">cognitive science</a>. And for reflective awareness and self-awareness, see <a href=\"https://www.lesswrong.com/tag/identity\">identity</a>, <a href=\"https://www.lesswrong.com/tag/personal-identity\">personal identity</a>, and <a href=\"https://www.lesswrong.com/tag/reflective-reasoning\">reflective reasoning</a>.</p><p>This tag's focus is tentative and provisional because it is not altogether clear that \"consciousness in the sense of having experiences\" is a coherent idea, or one that's distinct from the other categories above. This tag is a practical tool for organizing discussion on a family of related topics, and isn't intended as a strong statement \"this is the right way of <a href=\"https://www.lesswrong.com/posts/d5NyJ2Lf6N22AD9PB/where-to-draw-the-boundary\">carving nature at its joints</a>.\"</p><p>Suffice to say that (as of December 8, 2020) <i>enough LessWrongers find consciousness confusing enough</i>, and disagree enough about what's going on here, for it to make sense to use this page to organize discussion of those disagreements, rather than \"picking a winner\" immediately and running with it.</p><p>&nbsp;</p><h2>\"Having experiences\": Practical implications</h2><p>Beyond sheer curiosity about how the mind works, there are several sub-questions that have caused thinkers to take a special interest in the question \"what is 'having an experience'?\":</p><ul><li>1. When should I care about something else's welfare?<ul><li>1.1. <a href=\"https://www.lesswrong.com/tag/animal-welfare\">Animal welfare</a>: Pain, pleasure, desire, etc. are commonly taken to be <i>experiences</i>, and experiences of great moral importance. Knowing which species are capable of \"having experiences,\" then, could matter decisively in assessing the morality of factory farming and the morality of policies affecting wild animals.</li><li>1.2. Machine welfare and <a href=\"https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\">s-risks</a>: Similarly, knowing which kinds of (actual or potential) software have \"<a href=\"https://www.lesswrong.com/posts/wqDRRx9RqwKLzWt7R/nonperson-predicates\">experiences</a>\" could tell us a great deal about which programs are morally important.</li></ul></li><li>2. When should I think of something as \"me\" (or \"relevantly me-like\")?<ul><li>2.1. <a href=\"https://www.lesswrong.com/tag/personal-identity\">Personal identity</a>, <a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\">whole brain emulation</a>, and <a href=\"https://www.lesswrong.com/tag/simulation\">simulations</a>: Normally, people care about their future selves (at least in part) because they anticipate <i>having those selves' experiences</i>. Thus, one might say: \"It doesn't make sense for me to sign up for <a href=\"https://www.lesswrong.com/tag/cryonics\">cryonics</a>, because a cryo-revived copy of me wouldn't be <i>me</i>.\" (Or, replying to 1.2 above, one might say \"It doesn't make sense for me to sign up for cryonics, because a cryo-revived emulation of me would be a mere automaton with no experiences.\")</li><li>2.2. <a href=\"https://www.lesswrong.com/tag/anthropics\">Anthropics</a>: Anthropic questions turn on how many copies of \"you\" exist, or how many copies of \"observers similar to you\" exist. One could speculate that this is related to the question of what makes a copy of you conscious, and what \"consciousness\" is in the first place.</li></ul></li><li>3. Does the existence or nature of subjective experience imply any major updates about the world as a whole, about scientific methodology, etc.?<ul><li>3.1. <a href=\"https://www.lesswrong.com/tag/reductionism\">Reductionism</a>, physicalism, and naturalism: Can experience be a mere matter of, uh, matter? If experience turned out to be irreducibly unphysical (and real), this would falsify some of the most well-established generalizations in science.</li></ul></li></ul><p>LessWrong writers have typically been strongly on board with physicalism (3.1), and on board with the idea that an emulation of me is \"me\" (and conscious) in every sense that matters (2.1). Beyond that, however, views vary. (By comparison, ~74% of Anglophone philosophers of mind endorsed \"physicalism\" as opposed to \"non-physicalism\" <a href=\"https://philpapers.org/surveys/results.pl?affil=Target+faculty&amp;areas0=16&amp;areas_max=1&amp;grain=fine\">in 2009</a>.)</p><p>&nbsp;</p><h2>\"Having experiences\": Pre-LessWrong discussion</h2><p>How does this \"having experiences\" thing work, then? Well, this wiki page's editors haven't agreed on an answer yet. As a cop-out, we instead provide a list of highlights from the history of other people thinking about this.</p><p>For concreteness, we'll list particular years, authors, and texts, even though this makes some choices of what to highlight more arbitrary. Philosophy also shows up much more than psychology or neuroscience proper, not because philosophy is necessarily the right way to make progress here, but because the philosophy highlights are more \"meta\" and therefore choosing what to include relies less on a LessWrong consensus about consciousness itself.</p><p>Highlights:</p><ul><li>A long time ago BC: Someone comes up with the idea that \"minds\" are a pretty basic and fundamental feature of the world. Maybe gods have minds; maybe trees; maybe rivers; and so on. See also <a href=\"https://www.lesswrong.com/s/FqgKAHZAiZn9JAjDo/p/f4RJtHBPvDRJcCTva\">When Anthropomorphism Became Stupid</a> and <a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy\">Mind Projection Fallacy</a>.</li><li>~400 BC: Democritus proposes that all human-scale phenomena, including psychological phenomena, are the result of small physical parts bouncing off each other. From <a href=\"https://www.britannica.com/topic/materialism-philosophy/History-of-materialism\"><i>Encyclopedia Britannica</i></a>: \"Democritus thought that the soul consists of smooth, round atoms and that perceptions consist of motions caused in the soul atoms by the atoms in the perceived thing.\"</li><li>1641: René Descartes, <a href=\"https://yale.learningu.org/download/041e9642-df02-4eed-a895-70e472df2ca4/H2665_Descartes%27%20Meditations.pdf\"><i>Meditations on First Philosophy</i></a>. Descartes argues that mind and matter must be irreducibly distinct (<strong>mind-body dualism</strong>), because (e.g.) material things are spatially extended, while thoughts are not. Descartes speculates that minds interact with the physical world via a specific part of the brain, the pineal gland.<ul><li>Descartes also popularizes the idea that everyone knows their own conscious experiences with certainty: at any given moment, we are infallible about <i>the fact </i>that we are having an experience (the \"cogito\"), and we are also infallible about the <i>contents</i> of that experience.</li></ul></li><li>1651: Thomas Hobbes, <a href=\"https://www.csus.edu/indiv/s/simpsonl/hist162/hobbes.pdf\"><i>Leviathan</i></a>. Hobbes <a href=\"https://plato.stanford.edu/entries/hobbes/#3\">insistently</a> asserts that everything (including the mind) is material, and can be thought of as a mechanism or machine.</li><li>1714: Gottfried Leibniz. <a href=\"https://plato.stanford.edu/entries/leibniz-mind/\"><i>The Monadology</i></a><i>.</i> Leibniz argues that mind can't be reduced to matter:<ul><li>\"One is obliged to admit that <i>perception</i> and what depends upon it is inexplicable on mechanical principles, that is, by figures and motions. In imagining that there is a machine whose construction would enable it to think, to sense, and to have perception, one could conceive it enlarged while retaining the same proportions, so that one could enter into it, just like into a windmill. Supposing this, one should, when visiting within it, find only parts pushing one another, and never anything by which to explain a perception. Thus it is in the simple substance, and not in the composite or in the machine, that one must look for perception.\"</li></ul></li><li>1866: Charles Sanders Peirce, Lowell Lectures. Peirce <a href=\"https://colorysemiotica.files.wordpress.com/2014/08/peirce-collectedpapers.pdf\">introduces</a> the term \"<a href=\"https://plato.stanford.edu/entries/qualia/\"><i><strong>qualia</strong></i></a>\" to refer to what it's like to have a specific experience — e.g., the particular experience of redness. <i>Qualia</i> is the plural of <i>quale</i>, Latin for \"what kind of thing?\" and source of the English word <i>quality</i>.</li><li>1874: Thomas Huxley, \"<a href=\"http://opessoa.fflch.usp.br/sites/opessoa.fflch.usp.br/files/Huxley-English.pdf\">On the Hypothesis that Animals Are Automata, and Its History</a>.\" Huxley argues for <strong>epiphenomenalism</strong>, the view that consciousness is <i>caused</i> by physical processes, but has no effects of its own.<ul><li>\"The consciousness of brutes would appear to be related to the mechanism of their body simply as a collateral product of its working, and to be as completely without any power of modifying that working as the steam-whistle which accompanies the work of a locomotive engine is without influence upon its machinery. Their volition, if they have any, is an emotion indicative of physical changes, not a cause of such changes.\" And: \"to the best of my judgment, the argumentation which applies to brutes holds equally good of men\".</li></ul></li><li>1888: Santiago Ramón y Cajal, \"Estructura de los centros nerviosos de las aves.\" Using Camillo Golgi's staining method, Ramón y Cajal discovers that brains are made of neurons.</li><li>1903: G.E. Moore, \"<a href=\"https://fewd.univie.ac.at/fileadmin/user_upload/inst_ethik_wiss_dialog/Moore__G._1903._The_refutation_of_Idealism._in_MInd.pdf\">The Refutation of Idealism</a>.\" The early 20th century saw sharp moves away from spiritualism and supernaturalism in intellectual circles, beginning with the \"<a href=\"https://en.wikipedia.org/wiki/Bertrand_Russell%27s_philosophical_views#Analytic_philosophy\">Cambridge revolt against idealism</a>.\" Mysticism and metaphysical proclamations about the mind became increasingly unfashionable, as intellectuals grew more skeptical and more inclined to demand testable operationalizations of claims. Extreme manifestations of this attitude included logical positivism in the 1930s-1950s and behaviorism in the 1920s-1960s.</li><li>1943: McCulloch and Pitts, \"<a href=\"https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf\">A Logical Calculus of the Ideas Immanent in Nervous Activity</a>.\" <a href=\"https://plato.stanford.edu/entries/computational-mind/\"><i>SEP</i></a><i> </i>writes that this paper \"first suggested that something resembling the Turing machine might provide a good model for the mind.\" Subsequent developments in this direction include the cognitive revolution and the rise of <a href=\"https://plato.stanford.edu/entries/functionalism/\"><strong>functionalist</strong></a> and <a href=\"https://plato.stanford.edu/entries/computational-mind/\"><strong>computational</strong></a> accounts of the mind, which supplanted behaviorism.</li><li>1968: David Armstrong, <i>A Materialist Theory of the Mind</i>. An early attempt to sketch a theory of consciousness (specifically, a <a href=\"https://plato.stanford.edu/entries/consciousness-higher/\">higher-order</a> theory). For an overview of popular theories or sketches-of-theories in the following decades, see <i>SEP</i>'s review article \"<a href=\"https://plato.stanford.edu/entries/consciousness-neuroscience/\">The Neuroscience of Consciousness</a>.\"</li><li>1974: Thomas Nagel, \"<a href=\"http://www.esalq.usp.br/lepse/imgs/conteudo_thumb/What-Is-It-Like-to-Be-a-Bat-1.pdf\">What Is It Like To Be A Bat?</a>\" Nagel writes that \"fundamentally an organism has conscious mental states if and only if there is something that it is like to <i>be</i> that organism—something it is like for the organism.\" And:<ul><li>\"If physicalism is to be defended, the phenomenological features [i.e., what it's like to have certain experiences] must themselves be given a physical account. But when we examine their subjective character it seems that such a result is impossible. The reason is that every subjective phenomenon is essentially connected with a single point of view, and it seems inevitable that an objective, physical theory will abandon that point of view.\"</li><li>Subsequent authors have tended to use terms like \"<strong>what it's like</strong>,\" \"<strong>phenomenal consciousness</strong>\" (derived from <i>phenomena</i> in the sense of \"appearances\"), and <i>qualia </i>to gesture at this apparent puzzle. These are closely related terms, used in slightly different ways by different authors.</li></ul></li><li>1974: Robert Kirk, \"<a href=\"https://academic.oup.com/aristoteliansupp/article-abstract/48/1/135/1779753?redirectedFrom=fulltext\">Zombies v. Materialists</a>.\" This paper introduces the <strong>philosophical zombie</strong>, or <strong>p-zombie</strong>: a hypothetical being that is physically identical to a conscious person, but lacks consciousness. If the idea of p-zombies has no hidden logical inconsistencies, it is argued, then consciousness is not logically entailed by organisms' physical properties, which would make physicalism false.</li><li>1982: Frank Jackson, \"<a href=\"https://www.sfu.ca/~jillmc/JacksonfromJStore.pdf\">Epiphenomenal Qualia</a>.\" Jackson argues that we can imagine a scientist, Mary, who knows all the physical facts about color but has never seen the color red for herself. If she then sees red, it seems as though she learns a new fact—she learns what it's like to experience redness. Jackson takes this to mean that there are further facts beyond the physical facts, and that physicalism is therefore false. (For subsequent discussion, see <i>SEP</i>'s \"<a href=\"https://plato.stanford.edu/entries/qualia-knowledge/\">Qualia: The Knowledge Argument</a>.\")</li><li>1996: David Chalmers, <a href=\"http://consc.net/books/tcm/intro.html\"><i>The Conscious Mind: In Search of a Fundamental Theory</i></a>. Chalmers argues against physicalism, leaning heavily on the zombie argument and the Mary argument.<ul><li>Chalmers speaks of the \"<strong>hard problem of consciousness</strong>,\" the problem of explaining why we are phenomenally conscious (i.e., why we aren't p-zombies). \"Many books and articles on consciousness have appeared in the last few years, and one might think that we are making progress. But on a closer look, most of this work leaves the hardest problems about consciousness untouched. Often, this work addresses what might be called the 'easy' problems of consciousness: How does the brain process environmental stimulation? How does it integrate information? How do we produce reports on internal states? These are important questions, but to answer them is not to solve the hard problem: why is all this processing accompanied by an experienced inner life?\"</li><li>While Chalmers discussed consciousness earlier (e.g., in <a href=\"http://consc.net/papers/qualia.html\">1993</a>, <a href=\"http://consc.net/papers/facing.pdf\">1994</a>, and <a href=\"http://consc.net/papers/moving.html\">1996</a>), <i>The Conscious Mind</i> is the work that brought dualistic and quasi-dualistic views back into the intellectual almost-mainstream for the first time in a century. In spite of its crazy-sounding conclusions, the book is unusually clear, rigorous, and thorough, anticipating almost all of the obvious objections; and Chalmers attempts to make the irreducibility of consciousness more palatable to scientists by endorsing what he calls \"naturalistic dualism\": the view that consciousness is lawful, predictable, and not specific to humans. Chalmers argues that our consciousness depends on stable (but contingent) \"psychophysical laws\" that would also (for example) make a whole-brain emulation conscious.</li></ul></li><li>2003. Max Tegmark, \"<a href=\"https://space.mit.edu/home/tegmark/multiverse.pdf\">Parallel Universes</a>.\" Although not explicitly concerned with consciousness, Tegmark's picture raises problems for <a href=\"https://www.lesswrong.com/tag/anthropics\">anthropics</a> and our understanding of what makes an observer \"real.\"</li></ul><p>&nbsp;</p><h2>\"Having experiences\": Recent discussion</h2><ul><li>2008. Eliezer Yudkowsky, \"<a href=\"https://www.lesswrong.com/posts/fdEWWr8St59bXLbQr/zombies-zombies\">Zombies! Zombies?</a>\" This and other posts from <a href=\"https://www.lesswrong.com/s/FqgKAHZAiZn9JAjDo\">Physicalism 201</a> argue that we can be confident physicalism is true, even without knowing how to solve (or <a href=\"https://www.lesswrong.com/posts/Mc6QcrsbH5NRXbCRX/dissolving-the-question\">dissolve</a>) the \"hard problem of consciousness\".<ul><li>In particular, Yudkowsky argues that accepting the possibility of p-zombies is tantamount to accepting epiphenomenalism, and that epiphenomenalism is crazy. If our claims about consciousness are <i>true</i> even though consciousness has no causal effect on what we claim (because a p-zombie would move its lips and pen exactly as we do), then our claims would have to be true <i>by coincidence</i>, which is absurd given the Bayesian understanding of evidence and knowledge.</li><li>More generally, LessWrong writers' views on consciousness have been heavily influenced by the intuition pumps and reasoning rules Yudkowsky writes about in the Sequences (2006–2009), such as: <a href=\"https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences\">Making Beliefs Pay Rent</a>; <a href=\"https://www.lesswrong.com/posts/TLKPj4GDXetZuPDH5/making-history-available\">Making History Available</a>; <a href=\"https://www.lesswrong.com/posts/nj8JKFoLSMEmD3RGp/how-much-evidence-does-it-take\">How Much Evidence Does It Take?</a>; <a href=\"https://www.lesswrong.com/posts/QkX2bAkwG2EpGvNug/the-second-law-of-thermodynamics-and-engines-of-cognition\">The Second Law of Thermodynamics and Engines of Cognition</a>; and <a href=\"https://www.lesswrong.com/s/9bvAELWc8y2gYjRav/p/TynBiYt6zg42StRbb\">My Kind of Reflection</a>.</li></ul></li><li>2008. Eliezer Yudkowsky, \"<a href=\"https://www.lesswrong.com/posts/xsZnufn3cQw7tJeQ3/collapse-postulates\">Collapse Postulates</a>.\" This and other posts from the <a href=\"https://www.lesswrong.com/posts/hc9Eg6erp6hk9bWhn/the-quantum-physics-sequence\">Quantum Physics Sequence</a> argue that physicists' belief that observers or consciousness play a privileged role in quantum phenomena is based on a series of confusions and misunderstandings.</li><li>2013. David Chalmers, \"<a href=\"http://consc.net/papers/panpsychism.pdf\">Panpsychism and Panprotopsychism</a>.\" Chalmers argues that everything in the universe (down to the subatomic level) is \"conscious\" or \"proto-conscious.\"</li><li>2014. Benya Fallenstein. \"<a href=\"https://www.lesswrong.com/posts/7nAxgQYGYrEY5ZCAD/l-zombies-l-zombies\">L-zombies! L-zombies?</a>\" Fallenstein asks how we can distinguish between instantiated observers and uninstantiated (\"merely logical\") observers.</li><li>2016: Keith Frankish. \"<a href=\"https://nbviewer.jupyter.org/github/k0711/kf_articles/blob/master/Frankish_Illusionism%20as%20a%20theory%20of%20consciousness_eprint.pdf\">Illusionism as a Theory of Consciousness</a>.\" Frankish argues that \"experiences do not really have qualitative, 'what-it’s-like' properties.\" Instead, subjective experience seems \"unphysical\" or \"irreducible\" because of a sort of introspective illusion.</li><li>2017: Luke Muehlhauser, \"<a href=\"https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood\">2017 Report on Consciousness and Moral Patienthood</a>.\" The single largest work of scholarship on consciousness by the rationality community.</li><li>2018: David Chalmers, \"<a href=\"https://philpapers.org/archive/chatmo-32.pdf\">The Meta-Problem of Consciousness</a>.\" Chalmers discusses \"the problem of explaining why we think consciousness poses a hard problem\".</li></ul><p>&nbsp;</p><h2>Related pages</h2><ul><li>Non-tags: <a href=\"https://www.lesswrong.com/tag/anthropomorphism\">Anthropomorphism</a>, <a href=\"https://www.lesswrong.com/tag/how-an-algorithm-feels\">How an algorithm feels</a>, <a href=\"https://www.lesswrong.com/tag/zombies\">Zombies</a></li><li><a href=\"https://www.lesswrong.com/tag/identity\">Identity</a>, <a href=\"https://www.lesswrong.com/tag/personal-identity\">Personal identity</a>, <a href=\"https://www.lesswrong.com/tag/reflective-reasoning\">Reflective reasoning</a></li><li><a href=\"https://www.lesswrong.com/tag/animal-welfare\">Animal welfare</a>, <a href=\"https://www.lesswrong.com/tag/suffering\">Suffering</a>, <a href=\"https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\">Risks of astronomical suffering (s-risks)</a></li><li><a href=\"https://www.lesswrong.com/tag/reductionism\">Reductionism</a>, <a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy\">Mind Projection Fallacy</a></li><li><a href=\"https://www.lesswrong.com/tag/quantum-mechanics\">Quantum mechanics</a></li></ul>",
    "description_length": 24058,
    "viewCount": 419,
    "parentTagId": "world-modeling-biological-psychological"
  },
  {
    "core-tag": "World Modeling",
    "_id": "nZCb9BSnmXZXSNA2u",
    "name": "Evolution",
    "slug": "evolution",
    "postCount": 190,
    "description_html": "<p><strong>Evolution</strong> is \"<i>change in the heritable characteristics of biological populations over successive generations</i>\" (<a href=\"https://en.wikipedia.org/wiki/Evolution\">Wikipedia</a>). For posts about machine learning look <a href=\"https://www.lesswrong.com/tag/machine-learning?showPostCount=false&amp;useTagName=false\">here</a>.</p><p><i>Related: </i><a href=\"https://www.lesswrong.com/tag/biology?showPostCount=true&amp;useTagName=true\">Biology</a>, <a href=\"https://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&amp;useTagName=true\">Evolutionary Psychology</a>,</p><p>The sequence, <a href=\"https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8\">The Simple Math of Evolution</a> provides a good introduction to LessWrong thinking about evolution.</p><h1>Why be interested in evolution?</h1><p>Firstly, evolution is a useful case study of humans' ability (or inability) to model the real world. This is because it has a single clear criterion (\"relative reproductive fitness\") which is selected (optimized) for:</p><blockquote><p><i>\"If we can't see clearly the result of a single monotone optimization criterion—if we can't even train ourselves to hear a single pure note—then how will we listen to an orchestra? How will we see that \"Always be selfish\" or \"Always obey the government\" are poor guiding principles for human beings to adopt—if we think that even optimizing genes for inclusive fitness will yield organisms which sacrifice reproductive opportunities in the name of social resource conservation?</i></p></blockquote><blockquote><p><i>To train ourselves to see clearly, we need simple practice cases\" -- </i>Eliezer Yudkowsky<i>, </i><a href=\"https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/i6fKszWY6gLZSX2Ey\"><i>Fake Optimisation Criteria</i></a></p></blockquote><p>Secondly, much of rationality necessarily revolves around the human brain (<a href=\"https://www.lesswrong.com/tag/transhumanism?usePostCount=false&amp;useTagName=false\">for</a> <a href=\"https://www.lesswrong.com/tag/mind-uploading?showPostCount=false&amp;useTagName=false\">now</a>). An understanding of how it came into being can be very helpful both for understanding 'bugs' in the system (like superstimuli), and for explaining <a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value</a>, among others.</p><blockquote><p><i>A candy bar is a superstimulus: it contains more concentrated sugar, salt, and fat than anything that exists in the ancestral environment.&nbsp; &nbsp;A candy bar matches taste buds that evolved in a hunter-gatherer environment, but it matches those taste buds much more strongly than anything that actually existed in the hunter-gatherer environment.&nbsp; The signal that once reliably correlated to healthy food has been hijacked, blotted out with a point in tastespace that wasn't in the training dataset - an impossibly distant outlier on the old ancestral graphs.&nbsp;</i><br><i>-- </i>Eliezer Yudkowsky, <a href=\"https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/Jq73GozjsuhdwMLEG\">Superstimuli and the Collapse of Western Civilisation</a></p></blockquote><h2>See also</h2><ul><li><a href=\"https://lessestwrong.com/tag/evolution-as-alien-god\">Evolution as alien god</a></li><li><a href=\"https://lessestwrong.com/tag/slowness-of-evolution\">Slowness of evolution</a></li><li><a href=\"https://lessestwrong.com/tag/stupidity-of-evolution\">Stupidity of evolution</a></li><li><a href=\"https://lessestwrong.com/tag/evolutionary-psychology\">Evolutionary psychology</a></li></ul><h2>External links</h2><ul><li><a href=\"http://dl.dropbox.com/u/33627365/Scholarship/Selfish%20Gene%20-%20Dawkins.pdf\">Richard Dawkins - The Selfish Gene</a> (PDF)</li></ul><h2>Summaries of Sequence's Posts on Evolution</h2><p><i>The following are summaries of posts concerning evolution in the Eliezer's sequences:</i></p><ul><li><a href=\"https://lessestwrong.com/lw/kr/an_alien_god/\">An Alien God</a> - Evolution is awesomely powerful, unbelievably stupid, incredibly slow, monomaniacally singleminded, irrevocably splintered in focus, blindly shortsighted, and itself a completely accidental process. If evolution were a god, it would not be Jehovah, but H. P. Lovecraft's Azathoth, the blind idiot God burbling chaotically at the center of everything.</li><li><a href=\"https://lessestwrong.com/lw/ks/the_wonder_of_evolution/\">The Wonder of Evolution</a> - The wonder of the first replicator was not how amazingly well it replicated, but that a first replicator could arise, at all, by pure accident, in the primordial seas of Earth. That first replicator would undoubtedly be devoured in an instant by a sophisticated modern bacterium. Likewise, the wonder of evolution itself is not how <i>well</i> it works, but that a <i>brainless, accidentally occurring</i> <a href=\"https://lessestwrong.com/tag/optimization\">optimization process</a> can work <i>at all</i>. If you praise evolution for being such a wonderfully intelligent Creator, you're entirely missing the wonderful thing about it.</li><li><a href=\"https://lessestwrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/\">Evolutions Are Stupid (But Work Anyway)</a> - Modern evolutionary theory gives us a definite picture of evolution's capabilities. If you praise evolution one millimeter higher than this, you are not scoring points against creationists, you are just being factually inaccurate. In particular we can calculate the probability and time for advantageous genes to rise to fixation. For example, a mutation conferring a 3% advantage would have only a 6% probability of surviving, and if it did so, would take 875 generations to rise to fixation in a population of 500,000 (on average).</li><li><a href=\"https://lessestwrong.com/tag/speed-limit-and-complexity-bound-for-evolution\">Speed limit and complexity bound for evolution</a> - It is widely understood that there is a limit on how fast evolution can accumulate information in a gene pool, and an upper bound on how much genetic information can be sustained against the degenerative pressure of copying errors. (But Yudkowsky's attempt to calculate an actual bound failed mathematically, so see the referenced summary of the discussion instead of the original blog post.)</li><li><a href=\"https://lessestwrong.com/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">Adaptation-Executers, not Fitness-Maximizers</a> - A central principle of evolutionary biology in general, and <a href=\"https://lessestwrong.com/tag/evolutionary-psychology\">evolutionary psychology</a> in particular. If we regarded human taste buds as trying to <i>maximize fitness</i>, we might expect that, say, humans fed a diet too high in calories and too low in micronutrients, would begin to find lettuce delicious, and cheeseburgers distasteful. But it is better to regard taste buds as an <i>executing adaptation</i> - they are adapted to an ancestral environment in which calories, not micronutrients, were the limiting factor.</li><li><a href=\"https://lessestwrong.com/lw/l6/no_evolutions_for_corporations_or_nanodevices/\">No Evolutions for Corporations or Nanodevices</a> - Price's Equation describes quantitatively how the change in a average trait, in each generation, is equal to the covariance between that trait and fitness. Such covariance requires substantial variation in traits, substantial variation in fitness, and substantial correlation between the two - and then, to get large <i>cumulative</i> selection pressures, the correlation must have persisted over <i>many</i> generations with <i>high-fidelity</i> inheritance, continuing sources of new variation, and frequent birth of a significant fraction of the population. People think of \"evolution\" as something that automatically gets invoked where \"reproduction\" exists, but these other conditions may not be fulfilled - which is why corporations haven't evolved, and nanodevices probably won't.</li><li><a href=\"https://lessestwrong.com/lw/l5/evolving_to_extinction/\">Evolving to Extinction</a> - Contrary to a naive view that evolution works for the good of a species, evolution says that genes which outreproduce their alternative alleles increase in frequency within a gene pool. It is entirely possible for genes which \"harm\" the species to outcompete their alternatives in this way - indeed, it is entirely possible for a species to <i>evolve to extinction</i>.</li><li><a href=\"https://lessestwrong.com/lw/kw/the_tragedy_of_group_selectionism/\">The Tragedy of Group Selectionism</a> - Describes a key case where some pre-1960s evolutionary biologists went wrong by <a href=\"https://wiki.lesswrong.com/wiki/anthropomorphizing\">anthropomorphizing</a> evolution - in particular, Wynne-Edwards, Allee, and Brereton among others believed that predators would voluntarily restrain their breeding to avoid overpopulating their habitat. Since evolution does not usually do this sort of thing, their rationale was <a href=\"https://lessestwrong.com/tag/group-selection\">group selection</a> - populations that did this would survive better. But group selection is extremely difficult to make work mathematically, and an experiment under sufficiently extreme conditions to permit group selection, had rather different results.</li><li><a href=\"https://lessestwrong.com/lw/kz/fake_optimization_criteria/\">Fake Optimization Criteria</a> - Why study evolution? For one thing - it lets us see an alien <a href=\"https://lessestwrong.com/tag/optimization\">optimization process</a> up close - lets us see the <i>real</i> consequence of optimizing <i>strictly</i> for an alien optimization criterion like inclusive genetic fitness. Humans, who try to persuade other humans to do things their way, think that this policy criterion ought to require predators to <a href=\"https://lessestwrong.com/tag/group-selection\">restrain their breeding</a> to live in harmony with prey; the true result is something that humans find less aesthetic.</li><li><a href=\"https://lessestwrong.com/lw/kv/beware_of_stephen_j_gould/\">Beware of Stephen J. Gould</a> - A lot of people have gotten their grasp of evolutionary theory from Stephen J. Gould, a man who committed the moral equivalent of fraud in a way that is difficult to explain. At any rate, he severely misrepresented what evolutionary biologists believe, in the course of pretending to attack certain beliefs. One needs to clear from memory, as much as possible, not just everything that Gould positively stated but everything he seemed to imply the mainstream theory believed.</li><li><a href=\"https://lessestwrong.com/lw/l8/conjuring_an_evolution_to_serve_you/\">Conjuring An Evolution To Serve You</a> - If you take the hens who lay the most eggs in each generation, and breed from them, you should get hens who lay more and more eggs. Sounds logical, right? But this selection may actually favor the most <i>dominant</i> hen, that pecked its way to the top of the pecking order at the expense of other hens. Such breeding programs produce hens that must be housed in individual cages, or they will peck each other to death. Jeff Skilling of Enron fancied himself an evolution-conjurer - summoning <i>the awesome power of evolution</i> to work for him - and so, every year, every Enron employee's performance would be evaluated, and the bottom 10% would get fired, and the top performers would get huge raises and bonuses.</li></ul>",
    "description_length": 11414,
    "viewCount": 118,
    "parentTagId": "world-modeling-biological-psychological"
  },
  {
    "core-tag": "World Modeling",
    "_id": "exZi6Bing5AiM4ZQB",
    "name": "Evolutionary Psychology",
    "slug": "evolutionary-psychology",
    "postCount": 84,
    "description_html": "<p><a href=\"https://wiki.lesswrong.com/wiki/Evolution\"><u>Evolution</u></a>, the cause of the diversity of biological life on Earth, <i>does not work like humans do</i>, and does not design things the way a human engineer would. This <a href=\"https://wiki.lesswrong.com/wiki/Alienness_of_evolution\"><u>blind idiot god</u></a> is also the source and patterner of human beings. \"Nothing in biology makes sense except in the light of evolution,\" said Theodosius Dobzhansky. Humans brains are also biology, and nothing about our thinking makes sense except in the light of evolution.</p><p>Consider, for example, the following tale:</p><blockquote><p>A man and a woman meet in a bar. The man is attracted to her form and clear complexion, which would have been fertility cues in the ancestral environment, but which in this case result from makeup and a bra. This does not bother the man; he just likes the way she looks. His clear-complexion-detecting neural circuitry does not know that its purpose is to detect fertility, any more than the atoms in his hand contain tiny little XML tags reading \"&lt;purpose&gt;pick things up&lt;/purpose&gt;\". The woman is attracted to his confident smile and firm manner, cues to high status, which in the ancestral environment would have signified the ability to provide resources for children. She plans to use birth control, but her confident-smile-detectors don't know this any more than a toaster knows its designer intended it to make toast. She's not concerned philosophically with the meaning of this rebellion, because her brain is a creationist and denies vehemently that evolution exists. He's not concerned philosophically with the meaning of this rebellion, because he just wants to get laid. They go to a hotel, and undress. He puts on a condom, because he doesn't want kids, just the dopamine-noradrenaline rush of sex, which reliably produced offspring 50,000 years ago when it was an invariant feature of the ancestral environment that condoms did not exist. They have sex, and shower, and go their separate ways. The main objective consequence is to keep the bar and the hotel and condom-manufacturer in business; which was not the cognitive purpose in their minds, and has virtually nothing to do with the key statistical regularities of reproduction 50,000 years ago which explain how they got the genes that built their brains that executed all this behavior.</p></blockquote><p>This only makes sense in the light of evolution as a designer - that we are <i>poorly</i> optimized to reproduce by a blind and unforesightful god.</p><p>The idea of evolution as the idiot designer of humans - that our brains are <i>not</i> consistently well-designed - is a key element of many of the <i>explanations of human errors</i> that appear on this website.</p><p>Some of the key ideas of evolutionary psychology are these:</p><ul><li>People's brains do not explicitly represent evolutionary reasons, <i>consciously or unconsciously</i>.</li><li>We are optimized for an \"ancestral environment\" (often referred to as EEA, for \"environment of evolutionary adaptedness\") that differs significantly from the environments in which most of us live. In the ancestral environment, calories were the limiting resource, so our tastebuds are built to like sugar and fat.</li><li>The brain is not built the way a human engineer would build it. A human engineer would have built our bodies to measure what it needed, so that if you already had enough calories but were lacking micronutrients, your taste buds would start liking lettuce instead of cheeseburgers.</li><li>The brain is a giant hack that starts to break down when you try to do things with it that hunter-gatherers weren't doing. Like computer programming, say.</li><li>Evolution's purposes also differ from our own purposes. We are built to deceive ourselves because self-deceivers were more effective liars in ancestral political disputes; and this fact about our underlying brain design doesn't change when we try to make a moral commitment to truth and rationality.</li><li>Although human beings do absorb significant additional complexity in the form of culture, we don't absorb it in a fully general way, but rather, in the way that we <a href=\"https://wiki.lesswrong.com/wiki/Detached_lever_fallacy\"><u>evolved to absorb it</u></a>. That's why the Soviets couldn't raise perfect communist children. Children are programmed to absorb their parents' language, say, but there is no environment which evokes the response of perfect altruism in human children.</li></ul><h2>External links</h2><ul><li><a href=\"http://www.cep.ucsb.edu/papers/pfc92.pdf\"><u>The Psychological Foundations of Culture</u></a> by Leda Cosmides and John Tooby.</li><li><a href=\"http://www.cep.ucsb.edu/primer.html\"><u>Evolutionary Psychology: A Primer</u></a> by Leda Cosmides and John Tooby.</li><li><a href=\"http://hplusmagazine.com/2009/11/23/darwinian-psychologist-straw-mans-ass-kicked/\"><u>“Darwinian Psychologist” Straw Man’s Ass Kicked</u></a></li><li><a href=\"http://www.amazon.com/Moral-Animal-Science-Evolutionary-Psychology/dp/0679763996\"><u>The Moral Animal: Why We Are the Way We Are: The New Science of Evolutionary Psychology</u></a> - popular book by Robert Wright.</li></ul><h2>See also</h2><ul><li><a href=\"lesswrong.com/tag/evolution\"><u>Evolution</u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Stupidity_of_evolution\"><u>Stupidity of evolution</u></a>, <a href=\"https://wiki.lesswrong.com/wiki/Evolution_as_alien_god\"><u>evolution as alien god</u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Human_universal\"><u>Human universal</u></a></li><li><a href=\"https://www.lesswrong.com/tag/adaptation-executors\"><u>Adaptation executers</u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Corrupted_hardware\"><u>Corrupted hardware</u></a></li><li><a href=\"https://www.lesswrong.com/tag/psychology\">Psychology</a></li></ul>",
    "description_length": 5930,
    "viewCount": 154,
    "parentTagId": "world-modeling-biological-psychological"
  },
  {
    "core-tag": "World Modeling",
    "_id": "Wi3EopKJ2aNdtxSWg",
    "name": "Neuroscience",
    "slug": "neuroscience",
    "postCount": 213,
    "description_html": "<p><strong>Neuroscience</strong> is a field of study dealing with the structure or function of the brain. It is of particular interest to LessWrong both because it can shed light on <a href=\"https://www.lesswrong.com/tag/ai?showPostCount=true\">AI</a>, and because it is often helpful for <a href=\"https://www.lesswrong.com/tag/rationality?showPostCount=true\">rationality</a>. For example, understanding how <a href=\"https://www.lesswrong.com/posts/rD57ysqawarsbry6v?lw_source=posts_sheet\">attentional control</a> works can inform possible solutions.</p><p>Some specific theories of neuroscience:</p><ul><li><a href=\"https://www.lesswrong.com/tag/predictive-processing\">Predictive processing</a></li><li><a href=\"https://www.lesswrong.com/tag/free-energy-principle\">FEP/Active Inference</a></li><li><a href=\"https://www.lesswrong.com/posts/ixZLTmFfnKRbaStA5/book-review-a-thousand-brains-by-jeff-hawkins\">A Thousand Brains theory</a></li></ul><p>See also:</p><ul><li><a href=\"https://www.lesswrong.com/tag/neuromorphic-ai\">Neuromorphic AI</a></li></ul>",
    "description_length": 1051,
    "viewCount": 157,
    "parentTagId": "world-modeling-biological-psychological"
  },
  {
    "core-tag": "World Modeling",
    "_id": "8e9e8fzXuW5gGBS3F",
    "name": "Qualia",
    "slug": "qualia",
    "postCount": 61,
    "description_html": "<p><strong>Qualia</strong> are aspects of subjective conscious experience. The discussion of qualia tends to come up on LessWrong in two contexts: as an argument against reductionism (with a claim that qualia cannot be a mere matter of, uh, matter), and as a key factor in how seriously we should weight the suffering of animals. There is also a third context for Qualia arguments: as an argument against Whole Brain Emulation being you, or of emulations of minds not being conscious and instead P-zonbies.</p><p>There are several philosophical discussions of how to reduce qualia into functions of the brain (most famously by Daniel Dennett), but the discussion continues among philosophers.</p>",
    "description_length": 696,
    "viewCount": 85,
    "parentTagId": "world-modeling-biological-psychological"
  },
  {
    "core-tag": "World Modeling",
    "_id": "ac84EpK6mZbPLzmqj",
    "name": "General Intelligence",
    "slug": "general-intelligence",
    "postCount": 145,
    "description_html": "<p><strong>General Intelligence</strong> or <strong>Universal Intelligence</strong> is the ability to efficiently achieve goals in a wide range of domains.&nbsp;</p><p>This tag is specifically for discussing intelligence in the broad sense: for discussion of IQ testing and psychometric intelligence, see <a href=\"https://www.lesswrong.com/tag/iq-g-factor\">IQ / g-factor</a>; for discussion about e.g. specific results in artificial intelligence, see <a href=\"https://www.lesswrong.com/tag/ai\">AI</a>. These tags may overlap with this one to the extent that they discuss the nature of general intelligence.</p><p>Examples of posts that fall under this tag include <a href=\"https://www.lesswrong.com/posts/aiQabnugDhcrFtr9n/the-power-of-intelligence\">The Power of Intelligence</a>, <a href=\"https://www.lesswrong.com/posts/Q4hLMDrFd8fbteeZ8/measuring-optimization-power\">Measuring Optimization Power</a>, <a href=\"https://www.lesswrong.com/posts/XPErvb8m9FapXCjhA/adaptation-executers-not-fitness-maximizers\">Adaption-Executers not Fitness Maximizers</a>, <a href=\"https://www.lesswrong.com/posts/FbQ9Y9pBif5xZ7w2f/distinctions-in-types-of-thought\">Distinctions in Types of Thought</a>, <a href=\"https://www.lesswrong.com/posts/GMqZ2ofMnxwhoa7fD/the-octopus-the-dolphin-and-us-a-great-filter-tale\">The Octopus, the Dolphin and Us: a Great Filter tale</a>.</p><p>On the difference between psychometric intelligence (IQ) and general intelligence:</p><blockquote><p>But the word “intelligence” commonly evokes pictures of the starving professor with an IQ of 160 and the billionaire CEO with an IQ of merely 120. Indeed there are differences of individual ability apart from “book smarts” which contribute to relative success in the human world: enthusiasm, social skills, education, musical talent, rationality. Note that each factor I listed is cognitive. Social skills reside in the brain, not the liver. And jokes aside, you will not find many CEOs, nor yet professors of academia, who are chimpanzees. You will not find many acclaimed rationalists, nor artists, nor poets, nor leaders, nor engineers, nor skilled networkers, nor martial artists, nor musical composers who are mice. Intelligence is the foundation of human power, the strength that fuels our other arts.</p></blockquote><blockquote><p>-- Eliezer Yudkowsky, <a href=\"https://intelligence.org/files/AIPosNegFactor.pdf\">Artificial Intelligence as a Positive and Negative Factor in Global Risk</a></p></blockquote><h2>Definitions of General Intelligence</h2><p>After reviewing extensive literature on the subject, Legg and Hutter<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefosnb04qur8\"><sup><a href=\"#fnosnb04qur8\">[1]</a></sup></span>&nbsp;summarizes the many possible valuable definitions in the informal statement “Intelligence measures an agent’s ability to achieve goals in a wide range of environments.” They then show this definition can be mathematically formalized given reasonable mathematical definitions of its terms. They use <a href=\"https://lessestwrong.com/tag/solomonoff-induction\">Solomonoff induction</a> - a formalization of <a href=\"https://lessestwrong.com/tag/occam-s-razor\">Occam's razor</a> - to construct an <a href=\"https://lessestwrong.com/tag/aixi\">universal artificial intelligence</a> with a embedded <a href=\"https://lessestwrong.com/tag/utility-functions\">utility function</a> which assigns less <a href=\"https://lessestwrong.com/tag/expected-utility\">utility</a> to those actions based on theories with higher <a href=\"https://wiki.lesswrong.com/wiki/Kolmogorov_complexity\">complexity</a>. They argue this final formalization is a valid, meaningful, informative, general, unbiased, fundamental, objective, universal and practical definition of intelligence.</p><p>We can relate Legg and Hutter's definition with the concept of <a href=\"https://lessestwrong.com/tag/optimization\">optimization</a>. According to <a href=\"https://lessestwrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky</a> intelligence is <a href=\"https://lessestwrong.com/lw/vb/efficient_crossdomain_optimization/\">efficient cross-domain optimization</a>. It measures an agent's capacity for efficient cross-domain optimization of the world according to the agent’s preferences.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7hbpdfpe6x3\"><sup><a href=\"#fn7hbpdfpe6x3\">[2]</a></sup></span>&nbsp;Optimization measures not only the capacity to achieve the desired goal but also is inversely proportional to the amount of resources used. It’s the ability to steer the future so it hits that small target of desired outcomes in the large space of all possible outcomes, using fewer resources as possible. For example, when Deep Blue defeated Kasparov, it was able to hit that small possible outcome where it made the right order of moves given Kasparov’s moves from the very large set of all possible moves. In that domain, it was more optimal than Kasparov. However, Kasparov would have defeated Deep Blue in almost any other relevant domain, and hence, he is considered more intelligent.</p><p>One could cast this definition in a possible world vocabulary, intelligence is:</p><ol><li>the ability to precisely realize one of the members of a small set of possible future worlds that have a higher preference over the vast set of all other possible worlds with lower preference; while</li><li>using fewer resources than the other alternatives paths for getting there; and in the</li><li>most diverse domains as possible.</li></ol><p>How many more worlds have a higher preference then the one realized by the agent, less intelligent he is. How many more worlds have a lower preference than the one realized by the agent, more intelligent he is. (Or: How much smaller is the set of worlds at least as preferable as the one realized, more intelligent the agent is). How much less paths for realizing the desired world using fewer resources than those spent by the agent, more intelligent he is. And finally, in how many more domains the agent can be more efficiently optimal, more intelligent he is. Restating it, the intelligence of an agent is directly proportional to:</p><ul><li>(a) the numbers of worlds with lower preference than the one realized,</li><li>(b) how much smaller is the set of paths more efficient than the one taken by the agent and</li><li>(c) how more wider are the domains where the agent can effectively realize his preferences;</li></ul><p>and it is, accordingly, inversely proportional to:</p><ul><li>(d) the numbers of world with higher preference than the one realized,</li><li>(e) how much bigger is the set of paths more efficient than the one taken by the agent and</li><li>(f) how much more narrow are the domains where the agent can efficiently realize his preferences.</li></ul><p>This definition avoids several problems common in many others definitions, especially it avoids <a href=\"https://lessestwrong.com/tag/anthropomorphism\">anthropomorphizing</a> intelligence.</p><h2>See Also</h2><ul><li><a href=\"https://lessestwrong.com/tag/optimization\">Optimization process</a></li><li><a href=\"https://lessestwrong.com/tag/decision-theory\">Decision theory</a></li><li><a href=\"https://lessestwrong.com/tag/rationality\">Rationality</a></li><li><a href=\"http://arxiv.org/pdf/0712.3329.pdf\">Legg and Hutter paper “Universal Intelligence: A Deﬁnition of Machine Intelligence”</a></li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnosnb04qur8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefosnb04qur8\">^</a></strong></sup></span><div class=\"footnote-content\"><p>http://arxiv.org/pdf/0712.3329.pdf</p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7hbpdfpe6x3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7hbpdfpe6x3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>http://intelligence.org/files/IE-EI.pdf</p></div></li></ol>",
    "description_length": 7948,
    "viewCount": 270,
    "parentTagId": "world-modeling-biological-psychological"
  },
  {
    "core-tag": "World Modeling",
    "_id": "8JMCse6n4eXwRtjET",
    "name": "Neocortex",
    "slug": "neocortex",
    "postCount": 13,
    "description_html": null,
    "description_length": null,
    "viewCount": 24,
    "parentTagId": "world-modeling-biological-psychological"
  },
  {
    "core-tag": "World Modeling",
    "_id": "yt9Z7xdQrofW7fCN8",
    "name": "Epistemic Review",
    "slug": "epistemic-review",
    "postCount": 34,
    "description_html": "<p><strong>Epistemic Reviews</strong> take a closer look at an existing publication – such as a book, paper or blogpost – and evaluate whether its claims are true.&nbsp;</p><p>Alt search term: fact check.</p>",
    "description_length": 208,
    "viewCount": 48,
    "parentTagId": "world-modeling-practice"
  },
  {
    "core-tag": "World Modeling",
    "_id": "Q6hq54EXkrw8LQQE7",
    "name": "Gears-Level",
    "slug": "gears-level",
    "postCount": 64,
    "description_html": "<p>A <strong>gears-level </strong>model is 'well-constrained' in the sense that there is a strong connection between each of the things you observe-- it would be hard for you to imagine that one of the variables could be different while all of the others remained the same. </p><p><em>Related Tags: <a href=\"https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&amp;useTagName=true\">Anticipated Experiences</a>, <a href=\"https://www.lesswrong.com/tag/double-crux\">Double-Crux</a>, <a href=\"https://www.lesswrong.com/tag/empiricism?showPostCount=true&amp;useTagName=true\">Empiricism</a>, <a href=\"https://www.lesswrong.com/tag/falsifiability?showPostCount=true&amp;useTagName=true\">Falsifiability</a>, <a href=\"https://www.lesswrong.com/tag/map-and-territory?showPostCount=true&amp;useTagName=true\">Map and Territory</a></em></p><br><p>The term <strong>gears-level</strong> was first described on LW in the post <a href=\"https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding\">\"Gears in Understanding\"</a>:</p><blockquote>This property is <em>how deterministically interconnected the variables of the model are</em>. There are a few <a href=\"https://en.wikipedia.org/wiki/Goodhart%27s_law\">tests</a> I know of to see to what extent a model has this property, though I don't know if this list is exhaustive and would be a little surprised if it were:</blockquote><blockquote> 1. Does the model p<a href=\"https://www.lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">ay rent?</a> If it does, and if it were falsified, how much (and how precisely) could you infer other things from the falsification?</blockquote><blockquote> 2. How incoherent is it to imagine that the model is accurate but that a given variable <a href=\"https://www.lesswrong.com/lw/if/your_strength_as_a_rationalist/\">could be different</a>?</blockquote><blockquote> 3. If you knew the model were accurate but you were to forget the value of one variable, <a href=\"https://www.lesswrong.com/lw/la/truly_part_of_you/\">could you rederive it</a>?</blockquote><p>An example from <a href=\"https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding\">Gears in Understanding</a> of a gears-level model is (surprise) a box of gears. If you can see a series of  interlocked gears, alternately turning clockwise, then counterclockwise, and so on, then you're able to anticipate the direction of any given, even if you cannot see it. It would be very difficult to imagine all of the gears turning as they are but only one of them changing direction whilst remaining interlocked. And finally, you would be able to rederive the direction of any given gear if you forgot it. </p><br><p>Note that the author of <a href=\"https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding\">Gears in Understanding</a>, <a href=\"https://www.lesswrong.com/users/valentine\">Valentine</a>, was careful to point out that these tests do not fully <em>define</em> the property 'gears-level', and that  \"Gears-ness is not the same as goodness\"-- there are other things that are valuable in a model, and many things cannot practically be modelled in this fashion. If you intend to use the term it is highly recommended you read the post beforehand, as the concept is not easily defined.</p>",
    "description_length": 3308,
    "viewCount": 243,
    "parentTagId": "world-modeling-practice"
  },
  {
    "core-tag": "World Modeling",
    "_id": "SbsZrDB844cENk4DQ",
    "name": "Falsifiability",
    "slug": "falsifiability",
    "postCount": 14,
    "description_html": "<p><strong>Falsifiability</strong> or <strong>Refutability</strong> is the capacity for a statement, theory or hypothesis to be contradicted by evidence. For example, the statement \"All swans are white\" is falsifiable because one can observe that black swans exist. <a href=\"https://en.wikipedia.org/wiki/Falsifiability\">(From Wikipedia)</a></p>",
    "description_length": 345,
    "viewCount": 29,
    "parentTagId": "world-modeling-practice"
  },
  {
    "core-tag": "World Modeling",
    "_id": "AeqCtS3BaY3cwzKAs",
    "name": "Fermi Estimation",
    "slug": "fermi-estimation",
    "postCount": 37,
    "description_html": "<p>A <strong>Fermi Estimation</strong> is a rough calculation which aims to be right within ~an order of magnitude, prioritizing getting a good enough to be useful answer without putting large amounts of thought and research in rather than being extremely accurate.</p><p><strong>Related Pages:</strong> <a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\">Forecasting &amp; Prediction</a></p>",
    "description_length": 407,
    "viewCount": 194,
    "parentTagId": "world-modeling-practice"
  },
  {
    "core-tag": "World Modeling",
    "_id": "8daMDi9NEShyLqxth",
    "name": "Forecasting & Prediction",
    "slug": "forecasting-and-prediction",
    "postCount": 427,
    "description_html": "<p><strong>Forecasting&nbsp;</strong>or&nbsp;<strong>Predicting</strong> is the act of making statements about what will happen in the future (and in some cases, the past) and then scoring the predictions. Posts marked with this tag are for discussion of the practice, skill, and methodology of forecasting. Posts exclusively containing object-level lists of forecasts and predictions are in&nbsp;<a href=\"https://www.lesswrong.com/tag/forecasts\"><u>Forecasts</u></a>.</p><blockquote><p><i>Above all, don’t ask what to believe—ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the center of the inquiry.</i></p><p>—<a href=\"https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences\"><u>Making Beliefs Pay Rent</u></a></p></blockquote><p>Forecasting allows individuals and institutions to test their internal models of reality. &nbsp;A forecaster with a good track record can have more confidence in future predictions and hence actions in the same area as they have a good track record in. Organisations with decision-makers with good track records can likewise be more confident in their choices.</p><p>Crucially, forecasting is a tool to test decision making, rather than a tool for good decision making. If your decision makers are found to be poor forecasters, that is a bad sign, but if your decision making process doesn't involve forecasting, it's not a bad sign. It's not clear that it should.</p><h1>Where to start</h1><p>Some common recommendations for getting into forecasting are as follows:</p><ul><li>Track your personal forecasts. Get a notebook, spreadsheet or <a href=\"https://fatebook.io\">fatebook.io</a> and write what you think will happen and a % or odds chance. Follow up on it later.</li><li>Bet fake money on <a href=\"https://manifold.markets/\">manifold.markets</a>. It's still pretty addictive, so if you have a gambling problem, please avoid.</li><li>Take part in the monthly <a href=\"https://www.quantifiedintuitions.org/estimation-game\">estimation game</a>. Test your ability to estimate quantities. This is correlated with your ability to navigate the world well</li><li>Forecast on <a href=\"https://metaculus.com\">metaculus.com</a>. Questions are often pretty focused on geopolitics</li><li>Read <a href=\"https://www.amazon.co.uk/Superforecasting-Science-Prediction-Philip-Tetlock/dp/1847947158\">Superforecasting</a> by Philip Tetlock. This is if books are a way you learn well.&nbsp;</li></ul><h1>Forecasting Techniques</h1><p>Forecasting is hard but many top forecasters use common techniques. This suggests that forecasting is a skill that can be learnt and practised.</p><h2>Base rates</h2><p><a href=\"https://en.wikipedia.org/wiki/Reference_class_forecasting\"><i><u>Reference Class Forecasting on Wikipedia</u></i></a></p><p>Suppose we are trying to find the probability that an event will occur within the next 5 years. One good place to start is by asking \"of all similar time periods, what fraction of the time does this event occur?\". This is the base rate.</p><p>If we want to know the probability that Joe Biden is President of the United States on Nov. 1st, 2024, we could ask</p><ul><li>What fraction of presidential terms are fully completed (last all 4 years)? The answer to this is 49 out of the 58 total terms, or around <strong>84%</strong>.</li><li>On the other hand, we know that Biden has already made it through 288 days of his term. If we remove the 5 presidents who left office before that, there are 49 out of 53 or around <strong>92%</strong>.</li><li>But alternately, Joe Biden is pretty old (78 to be exact). If we look up <a href=\"https://www.ssa.gov/oact/STATS/table4c6.html\">death rate per year in actuarial tables</a>, it's around 5.1% per year, so this leaves him with a ~15% chance of death or a <strong>85%</strong> chance of surviving his term.</li></ul><p>These are all examples of using base rates. [These examples are taken from&nbsp;<a href=\"https://www.lesswrong.com/posts/ahWnHGZCWqzTnXs4i/base-rates-and-reference-classes\"><u>Base Rates and Reference Classes</u></a> by jsteinhardt.]</p><p>Base rates represent the outside view for a given question. They are a good place to start but can often be improved on by updating the probability according to an inside view.</p><p>Note that there are often several reference classes we could use, each implying a different base rate. The problem of deciding which class to use is known as the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Reference_class_problem\"><u>reference class problem</u></a>.</p><h2>Calibration training</h2><p>A forecaster is said to be calibrated if the events they say have a X% chance of happening, happen X% of the time.</p><p>Most people are overconfident. When they say an event has a 99% chance of happening, often the events happen much less frequently than that.</p><p>This natural overconfidence can be corrected with calibration training. In calibration training, you are asked to answer a set of factual questions, assigning a probability to each of your answers.</p><p>A calibration exercise can be found here: <a href=\"https://www.quantifiedintuitions.org/calibration\">https://www.quantifiedintuitions.org/calibration</a>&nbsp;</p><h2>Question decomposition</h2><p>Much like Fermi estimation, questions about future events can often be decomposed into many different questions, these questions can be answered, and the answers to these questions can be used to reconstruct an answer to the original question.</p><p>Suppose you are interested in whether AI will cause a catastrophe by 2100. For AI to cause such an event, several things need to be true: (1) it needs to be possible to build advanced AI with agentic planning and strategic awareness by 2100, (2) there need to be strong incentives to apply such a system, (3) it needs to be difficult to align such a system should it be deployed, (4) a deployed and unaligned AI would act in unintended and high-impact power seeking ways causing trillions of dollars in damage, (5) of these consequences will result in the permanent disempowerment of all humanity and (6) this disempowerment will constitute an existential catastrophe. Taking the probabilities that Eli Lifland assigned to each question gives a 80%, 85%, 75%, 90%, 80% and 95% chance of events 1 through 6 respectively. Since each event is conditional on the ones before it, we can find the probability of the original question by multiplying all the probabilities together. This gives Eli Lifland a probability of existential risk from misaligned AI before 2100 to be approximately 35%. For more detail see Eli's original post <a href=\"https://www.foxy-scout.com/wwotf-review/\">here</a>.</p><p>Decomposing questions into their constituent parts, assigning probabilities to these sub-questions, and combining these probabilities to answer the original questions is believed to improve forecasts. This is because, while each forecast is noisy, combining the estimates from many questions cancels the noise and leaves us with the signal.</p><p>Question decomposition is also good at increasing epistemic legibility. It helps forecasters to communicate to others why they've made the forecast that they did and it allows them to identify their specific points of disagreement.</p><h2>Premortems</h2><p><a href=\"https://en.wikipedia.org/wiki/Pre-mortem\"><u>Premortems on Wikipedia</u></a></p><p>A premortem is a strategy used once you've assigned a probability to an event. You ask yourself to imagine that the forecast was wrong and you then work backwards to determine what could potentially have caused this.</p><p>It is simply a way to reframe the question \"in what ways might I be wrong?\" but in a way that reduces motivated reasoning caused by attachment to the bottom line.&nbsp;</p><h2>Practice</h2><p><a href=\"https://forecasting.wiki/wiki/How_to_Start_Forecasting\"><i><u>Getting Started on the Forecasting Wiki</u></i></a></p><p>While the above techniques are useful, they are no substitute for actually making predictions. Get out there and make predictions! Use the above techniques. Keep track of your predictions. Periodically evaluate questions that have been resolved and review your performance. Assess the degree to which you are calibrated. Look out for systematic mistakes that you might be making. Make more predictions! Over time, like with any skill, your ability can and should improve.</p><h2>Other Resources</h2><p>Other resources include:</p><ul><li>Superforcasting by Philip Tetlock and Dan Gardener</li><li>Intro to Forecasting by Alex Lawson</li><li><a href=\"https://forecasting.substack.com/\"><u>Forecasting Newsletter</u></a> by Nuño Sempere</li></ul><h2>Forecasting Research</h2><p>Forecasting beyond 3 years is not good. Anything above .25 is worse than random. Many questions are too specific and too far away for forecasting to be useful to them (<a href=\"https://forum.effectivealtruism.org/posts/hqkyaHLQhzuREcXSX/data-on-forecasting-accuracy-across-different-time-horizons \">Dillon 2020</a>).<img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/lpmcuvclu9akc6ytdjvb\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/nkitkelpgq1otpiure0i 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/kc2luqtf0ugy8h6li17k 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/qxguotvyvchc3wrjx87f 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/ogxcnvsd9jtza50txxtz 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/uejg0ctlm5vpyq0kgxvx 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/hmze1uwmirvsoyxgq3fo 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/bznkoxebe3nzzzmrtglz 980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/ijpmqbdjflcx7nl9ztek 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/ifjc9bcavcswykrhhfjv 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8daMDi9NEShyLqxth/pmbpmgv6i12aedfmnzbd 1314w\"></p><h3>Difficulties in Applying Forecasting</h3><p>Decision makers largely don't trust forecasts. Even if you had the perfect set of 1000 forecast that gave policy recommendations (which is usually not the case), decision makers would need to want to act on them. That they don't is a significant bottleneck to successfully use forecasting.</p><p>It is difficult to forecast things policy makers actually care about. Forecasting sites forecast things like \"will Putin leave power\" rather than \"If Putin leaves power between July18th and the end of Aug how will that affect the likelihood of a rogue nuclear warhead\". &nbsp;This question probably still isn't specific enough to be useful - it doesn't forecast specific policy outcomes. And if it <i>were, </i>decision makers would have to trust the results, which they currently largely don't. This is related to the problem of specifying good forecasting questions, especially for nebulous domains.</p><h1>State of the Art</h1><p>For many years there have been calls to apply forecasting techniques to non-academic domains including journalism, policy, investing and business strategy. Several organisations now exist within these niche.</p><h2>Metaculus</h2><p><a href=\"https://www.metaculus.com/\"><u>Metaculus</u></a> is a popular and established web platform for forecasting. Their questions mainly focus on geopolitics, the coronavirus pandemic and topics of interest to Effective Altruism.</p><p>They host prediction competitions with real money prizes and collect and track public predictions made by various figures.</p><h2>Cultivate Labs</h2><p><a href=\"https://www.cultivatelabs.com/\"><u>Cultivate Labs</u></a> build tools that companies can use to crowdsource information from among their employees. This helps leadership to understand the consensus of people working on the ground and use this to improve the decisions they make.</p><h2>Kalshi</h2><p><a href=\"https://kalshi.com/\"><u>Kalshi</u></a> provide real money prediction markets on geopolitical events. The financial options they provide are intended to be used as hedges for political risk.</p><h2>Manifold.Markets</h2><p><a href=\"https://manifold.markets/\"><u>Manifold.Markets</u></a> is a prediction market platform that uses play money. It is noteworthy for its ease of use, great UI and the fact that the market creator decides how the market resolves.</p><h2>QURI</h2><p><a href=\"https://quantifieduncertainty.org/\"><u>QURI</u></a> is a research organisation that builds tools that make it easier to make good forecasts. Their most notable tool is Squiggle - a programming language designed to be used to make legible forecasts in a wide range of contexts.</p><h1>Forecasters on twitter</h1><ul><li><strong>Top forecasters on </strong><a href=\"https://www.lesswrong.com/users/metaculus?mention=user\"><strong>@Metaculus</strong></a><strong>:</strong><ul><li><a href=\"https://twitter.com/SmoLurks\">@SmoLurks</a> (ranked #1 in 2021)</li><li><a href=\"https://twitter.com/Jotto999\">@Jotto999</a> (#11 in 2019)&nbsp;</li><li><a href=\"https://twitter.com/ryanbeck111\">@ryanbeck111</a> (#12 in 2021)</li><li><a href=\"https://twitter.com/SchoeneggerPhil\">@SchoeneggerPhil</a> (#15 in 2023)</li><li><a href=\"https://twitter.com/lxrjl\">@lxrjl</a> (#15 in 2021)</li><li><a href=\"https://twitter.com/JgaltTweets\">@JgaltTweets</a> (#16 in 2023)</li><li><a href=\"https://twitter.com/peterwildeford\">@peterwildeford</a> (#19 in 2023)</li><li><a href=\"https://twitter.com/nextbigfuture\">@nextbigfuture</a> (#25 in 2023)</li><li><a href=\"https://twitter.com/draaglom\">@draaglom</a> (#26 in 2023)</li><li><a href=\"https://twitter.com/EgeErdil2\">@EgeErdil2</a> (#27 in 2021)&nbsp;</li><li><a href=\"https://twitter.com/LinchZhang\">@LinchZhang</a> (#29 in 2020)&nbsp;</li><li><a href=\"https://twitter.com/beala\">@beala</a> (#57 in 2021)&nbsp;</li><li><a href=\"https://twitter.com/NathanpmYoung\">@NathanpmYoung</a> (#72 in 2023)</li><li><a href=\"https://twitter.com/MatthewJBar\">@MatthewJBar</a> (#79 in 2021)&nbsp;</li></ul></li><li>&nbsp;<strong>Top forecasters on </strong><a href=\"https://twitter.com/INFERpub\">@INFERpub</a><strong>:</strong><ul><li><a href=\"https://twitter.com/celloMolly\">@celloMolly</a> (#6 in 2022)</li><li><a href=\"https://twitter.com/NunoSempere\">@NunoSempere</a> (#7 in 2022)&nbsp;</li><li><a href=\"https://twitter.com/cmeinel\">@cmeinel</a> (#12 in 2023) &nbsp;</li></ul></li><li><strong>Top forecasters on </strong><a href=\"https://twitter.com/GJ_Open\">@GJ_Open:</a> &nbsp;<ul><li><a href=\"https://twitter.com/leonardbarrett\">@leonardbarrett</a> (#1 in News 2022 tournament)</li><li><a href=\"https://twitter.com/juan_cambeiro\">@juan_cambeiro</a> (#3 in Coronavirus 2022)</li></ul></li><li><strong>Top bettors on</strong> <a href=\"https://twitter.com/Polymarket\">@Polymarket</a><strong>:</strong><ul><li><a href=\"https://twitter.com/Domahhhh\">@Domahhhh</a></li></ul></li><li><strong>Top bettors on</strong> <a href=\"https://twitter.com/ManifoldMarkets\">@ManifoldMarkets</a><strong>:&nbsp;</strong><ul><li><a href=\"https://twitter.com/firstuserhere\">@firstuserhere</a> ($1m play money lifetime profit)</li><li><a href=\"https://twitter.com/ReplyHobbes\">@ReplyHobbes</a> ($600k)</li><li><a href=\"https://twitter.com/Chrisbilbo\">@Chrisbilbo</a> ($450k) <strong>Top forecasting teams</strong></li></ul></li><li><a href=\"https://twitter.com/swift_centre\">@swift_centre</a> <strong>&amp; </strong><a href=\"https://twitter.com/SamotsvetyF\">@SamotsvetyF</a><strong>:</strong><ul><li><a href=\"https://twitter.com/MWStory\">@MWStory</a></li><li><a href=\"https://twitter.com/TolgaBilge_\">@TolgaBilge_</a></li></ul></li><li><strong>Official</strong> <a href=\"https://twitter.com/superforecaster\">@superforecaster</a><strong>:&nbsp;</strong><ul><li><a href=\"https://www.lesswrong.com/users/sam_atis?mention=user\">@sam_atis</a>&nbsp;</li><li><a href=\"https://twitter.com/davidmanheim\">@davidmanheim</a></li><li><a href=\"https://twitter.com/BalkanDevlen\">@BalkanDevlen</a></li><li><a href=\"https://twitter.com/KitsonJ1\">@KitsonJ1</a></li><li><a href=\"https://twitter.com/thatMikeBishop\">@thatMikeBishop</a></li><li><a href=\"https://twitter.com/EmilDimanchev\">@EmilDimanchev</a></li><li><a href=\"https://twitter.com/foxyforecaster\">@foxyforecaster</a></li><li><a href=\"https://twitter.com/rdeneufville\">@rdeneufville</a></li><li><a href=\"https://twitter.com/Allmyalibis\">@Allmyalibis</a></li><li><a href=\"https://twitter.com/brettabroad\">@brettabroad</a></li><li><a href=\"https://twitter.com/JoeGillis23\">@JoeGillis23</a></li><li><a href=\"https://twitter.com/HartmanMath\">@HartmanMath</a></li><li><a href=\"https://twitter.com/scholarandcat\">@scholarandcat</a></li><li><a href=\"https://twitter.com/dkcoutant\">@dkcoutant</a></li><li><a href=\"https://twitter.com/malcmur\">@malcmur</a></li></ul></li></ul><h1>See also</h1><ul><li><a href=\"https://www.lesswrong.com/tag/antiprediction\"><u>Antiprediction</u></a></li><li><a href=\"https://www.lesswrong.com/tag/making-beliefs-pay-rent\"><u>Making beliefs pay rent</u></a></li><li><a href=\"https://www.lesswrong.com/tag/prediction-markets\"><u>Prediction market</u></a></li><li><a href=\"https://www.lesswrong.com/tag/calibration\"><u>Calibration</u></a></li><li><a href=\"https://www.lesswrong.com/tag/black-swans\"><u>Black swan</u></a></li><li><a href=\"https://www.lesswrong.com/tag/betting\"><u>Betting</u></a></li></ul><p><br>&nbsp;</p>",
    "description_length": 17971,
    "viewCount": 391,
    "parentTagId": "world-modeling-practice"
  },
  {
    "core-tag": "World Modeling",
    "_id": "rWzGNdjuep56W5u2d",
    "name": "Inside/Outside View",
    "slug": "inside-outside-view",
    "postCount": 58,
    "description_html": "<p>An <strong>Inside View </strong>on a topic involves making predictions based on your understanding of the details of the process. An <strong>Outside View </strong>involves ignoring these details and using an estimate based on a class of roughly similar previous cases (alternatively, this is called <a href=\"http://en.wikipedia.org/wiki/Reference_class_forecasting\">reference class forecasting</a>), though it has been <a href=\"https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view\">pointed out</a> that the possible meaning has expanded beyond that.</p><p>For example, someone working on a project may estimate that they can reasonably get 20% of it done per day, so they will get it done in five days (inside view). Or they might consider that all of their previous projects were completed just before the deadline, so since the deadline for this project is in 30 days, that's when it will get done (outside view).</p><p>The terms were originally developed by Daniel Kahneman and Amos Tversky. An early use is in <a href=\"http://doi.org/10.1287/mnsc.39.1.17\">Timid Choices and Bold Forecasts: A Cognitive Perspective on Risk Taking (Kahneman &amp; Lovallo, 1993)</a> and the terms were popularised in <i>Thinking, Fast and Slow</i> (Kahneman, 2011; <a href=\"https://www.mckinsey.com/business-functions/strategy-and-corporate-finance/our-insights/daniel-kahneman-beware-the-inside-view\">relevant excerpt</a>). The planning example is discussed in <a href=\"https://www.lesswrong.com/posts/CPm5LTwHrvBJCa9h5/planning-fallacy\">The Planning Fallacy</a>.&nbsp;</p><h3>Examples of outside view</h3><p><strong>1.</strong> From <a href=\"https://www.overcomingbias.com/2007/07/beware-the-insi.html\">Beware the Inside View</a>, by Robin Hanson:</p><blockquote><p>I did 1500 piece jigsaw puzzle of fireworks, my first jigsaw in at least ten years.&nbsp; Several times I had the strong impression that I had carefully eliminated every possible place a piece could go, or every possible piece that could go in a place.&nbsp; I was very tempted to conclude that many pieces were missing, or that the box had extra pieces from another puzzle.&nbsp; This wasn’t impossible – the puzzle was an open box a relative had done before.&nbsp; And the alternative seemed humiliating.&nbsp;</p></blockquote><blockquote><p>But I allowed a very different part of my mind, using different considerations, to overrule this judgment; so many extra or missing pieces seemed unlikely.&nbsp; And in the end there was only one missing and no extra pieces.&nbsp; I recall a similar experience when I was learning to program. I would carefully check my program and find no errors, and then when my program wouldn’t run I was tempted to suspect compiler or hardware errors.&nbsp; Of course the problem was almost always my fault.&nbsp; &nbsp;</p></blockquote><p><strong>2.</strong> Japanese students expected to finish their essays an average of 10 days before deadline. The average completion time was actually 1 day before deadline. When asked when they'd completed similar, previous tasks, the average reply was 1 day before deadline[1].</p><p><strong>3.</strong> Students instructed to visualize how, where, and when they would perform their Christmas shopping, expected to finish shopping more than a week before Christmas. A control group asked when they expected their Christmas shopping to be finished, expected it to be done 4 days before Christmas. Both groups finished 3 days before Christmas[2].</p><h3>Problems with the outside view</h3><p>It is controversial how far the lesson of these experiments can be extended. Robin Hanson argues that this implies that, in futurism, forecasts should be made by trying to find a reference class of similar cases, rather than by trying to visualize outcomes. Eliezer Yudkowsky responds that this leads to \"reference class tennis\" wherein people feel that the same event 'obviously' belongs to two different reference classes, and that the above experiments were performed in cases where the new example was highly similar to past examples. I.e., this year's Christmas shopping optimism and last year's Christmas shopping optimism are much more similar to one another, than the invention of the Internet is to the invention of agriculture. If someone else then feels that the invention of the Internet is more like the category 'recent communications innovations' and should be forecast by reference to television instead of agriculture, both sides pleading the outside view has no resolution except \"I'm taking my reference class and going home!\"</p><p>More possible limitations and problems with using the outside view are discussed in <a href=\"https://www.lesswrong.com/posts/pqoxE3AGMbse68dvb/the-outside-view-s-domain\">The Outside View's Domain</a> and <a href=\"https://www.lesswrong.com/posts/FsfnDfADftGDYeG4c/outside-view-as-conversation-halter\">\"Outside View\" as Conversation-Halter</a>. <a href=\"https://www.lesswrong.com/posts/iyRpsScBa6y4rduEt/model-combination-and-adjustment\">Model Combination and Adjustment</a> discusses the implications of there usually existing multiple <i>different</i> outside views. <a href=\"https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view\">Taboo \"Outside View\"</a> argues that the meaning of \"Outside View\" have expanded too much, and that it should be <a href=\"https://www.lesswrong.com/tag/rationalist-taboo\">tabooed</a> and replaced with more precise terminology. An alternative to \"inside/outside view\" has been proposed in <a href=\"https://www.lesswrong.com/s/uLEjM2ij5y3CXXW6c/p/vKbAWFZRDBhyD6K6A\">Gears Level &amp; Policy Level</a>.</p><h2>External Posts</h2><ul><li><a href=\"http://www.overcomingbias.com/2007/07/beware-the-insi.html\">Beware the Inside View</a> by <a href=\"https://lessestwrong.com/tag/robin-hanson\">Robin Hanson</a></li></ul><h2>See Also</h2><ul><li><a href=\"https://lessestwrong.com/tag/planning-fallacy\">Planning fallacy</a></li><li><a href=\"https://www.lesswrong.com/tag/modest-epistemology\">Modest Epistemology</a></li><li><a href=\"https://lessestwrong.com/tag/near-far-thinking\">Near/far thinking</a></li><li><a href=\"https://lessestwrong.com/tag/connotation\">Connotation</a>, <a href=\"https://lessestwrong.com/tag/absurdity-heuristic\">Absurdity heuristic</a></li><li><a href=\"https://lessestwrong.com/tag/arguing-by-analogy\">Arguing by analogy</a></li><li><a href=\"https://lessestwrong.com/tag/intelligence-explosion\">Intelligence explosion</a>, <a href=\"https://lessestwrong.com/tag/the-hanson-yudkowsky-ai-foom-debate\">The Hanson-Yudkowsky AI-Foom Debate</a></li></ul><p>[1] Buehler, R., Griffin, D., &amp; Ross, M. 2002. Inside the planning fallacy: The causes and consequences of optimistic time predictions. Heuristics and biases: The psychology of intuitive judgment, 250-270. Cambridge, UK: Cambridge University Press.</p><p>[2] Buehler, R., Griffin, D. and Ross, M. 1995. It's about time: Optimistic predictions in work and love. European Review of Social Psychology, Volume 6, eds. W. Stroebe and M. Hewstone. Chichester: John Wiley &amp; Sons.</p>",
    "description_length": 7082,
    "viewCount": 793,
    "parentTagId": "world-modeling-practice"
  },
  {
    "core-tag": "World Modeling",
    "_id": "iTe27Ced8s8bGuvMK",
    "name": "Intellectual Progress (Individual-Level)",
    "slug": "intellectual-progress-individual-level",
    "postCount": 50,
    "description_html": null,
    "description_length": null,
    "viewCount": 243,
    "parentTagId": "world-modeling-practice"
  },
  {
    "core-tag": "World Modeling",
    "_id": "ZpG9rheyAkgCoEQea",
    "name": "Practice & Philosophy of Science",
    "slug": "practice-and-philosophy-of-science",
    "postCount": 242,
    "description_html": "<p><strong>Practice and Philosophy of Science</strong> is for posts that discuss how science is done or should be done; examples include <a href=\"https://www.lesswrong.com/posts/tSemJckYr29Gnxod2/building-intuitions-on-non-empirical-arguments-in-science\">Building Intuitions on Non-Empirical Arguments in Science</a> and the <a href=\"https://www.lesswrong.com/s/fxynfGCSHpY4FmBZy\">Science and Rationality sequence</a>. (It is not for posts that simply report on a new scientific result.)</p>",
    "description_length": 491,
    "viewCount": 212,
    "parentTagId": "world-modeling-practice"
  },
  {
    "core-tag": "World Modeling",
    "_id": "R6dqPii4cyNpuecLt",
    "name": "Prediction Markets",
    "slug": "prediction-markets",
    "postCount": 154,
    "description_html": "<p><strong>Prediction markets</strong> are speculative markets created for the purpose of making predictions. Assets are created whose final cash value is tied to a particular event or parameter. The current market prices can then be interpreted as predictions of the probability of the event or the expected value of the parameter. Prediction markets are thus structured as betting exchanges, without any risk for the bookmaker. <a href=\"https://lessestwrong.com/tag/robin-hanson\">Robin Hanson</a> was the first to run a corporate prediction market - at Project Xanadu -, and has made several contributions to the field such as: conditional predictions, accuracy issues and market and media manipulation.</p><p>People who buy low and sell high are rewarded for improving the market prediction, while those who buy high and sell low are punished for degrading the market prediction. Evidence so far suggests that prediction markets are at least as accurate as other institutions predicting the same events with a similar pool of participants.</p><p>Predictions markets have been used by organizations such as Google, General Electric, and Microsoft; several online and commercial prediction markets are also in operation. Historically, prediction markets have often been used to predict election outcomes.</p><h2>See Also</h2><ul><li><a href=\"https://lessestwrong.com/tag/forecasting-and-prediction\">Prediction</a></li><li><a href=\"https://lessestwrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation\">Economic consequences of AI and whole brain emulation</a></li><li><a href=\"https://lessestwrong.com/tag/group-rationality\">Group rationality</a></li><li><a href=\"https://lessestwrong.com/tag/making-beliefs-pay-rent\">Making beliefs pay rent</a></li><li><a href=\"https://www.lesswrong.com/tag/quri\">QURI</a></li></ul><h2>External Posts</h2><ul><li><a href=\"https://www.astralcodexten.com/p/prediction-market-faq\">Prediction Market FAQ</a> by <a href=\"https://www.lesswrong.com/users/scottalexander?mention=user\">@Scott Alexander</a>&nbsp;</li><li><a href=\"http://www.overcomingbias.com/2006/11/first_known_bus.html\">A 1990 Corporate Prediction Market</a> by <a href=\"https://lessestwrong.com/tag/robin-hanson\">Robin Hanson</a></li><li><a href=\"http://www.overcomingbias.com/2006/12/leamers_1986_id.html\">Leamer's 1986 Idea Futures Proposal</a> by Robin Hanson</li><li><a href=\"http://www.overcomingbias.com/2006/12/should_predicti.html\">Should Prediction Markets be Charities?</a> by Peter McCluskey</li><li><a href=\"http://www.overcomingbias.com/2006/12/the_future_of_o_1.html\">The Future of Oil Prices 2: Option Probabilities</a> by <a href=\"https://en.wikipedia.org/wiki/Hal_Finney_(cipherpunk)\">Hal Finney</a></li><li><a href=\"http://www.overcomingbias.com/2009/09/prediction-markets-as-collective-inteligence.html\">Prediction Markets As Collective Intelligence</a> by Robin Hanson</li><li><a href=\"http://www.overcomingbias.com/2011/11/conditional-close-election-markets.html\">Fixing Election Markets</a> by Robin Hanson</li><li><a href=\"http://www.gwern.net/Prediction%20markets\">Prediction Markets</a> at gwern.net</li><li><a href=\"http://hanson.gmu.edu/ideafutures.html\">Idea Futures (a.k.a. Prediction Markets)</a> by Robin Hanson</li></ul><h3>External Links</h3><ul><li><a href=\"http://dl.dropbox.com/u/5317066/2011-graefe.pdf\">Comparing face-to-face meetings, nominal groups, Delphi and prediction markets on an estimation task</a></li><li><a href=\"http://videolectures.net/uai08_hanson_cpm/\">Video of Robin Hanson's Combinatorial Prediction Markets lecture at the Uncertainty in Artificial Intelligence conference in Helsinki, 2008</a></li></ul>",
    "description_length": 3671,
    "viewCount": 488,
    "parentTagId": "world-modeling-practice"
  },
  {
    "core-tag": "World Modeling",
    "_id": "bmfs4jiLaF6HiiYkC",
    "name": "Reductionism",
    "slug": "reductionism",
    "postCount": 50,
    "description_html": "<p><strong>Reductionism</strong> is a disbelief that the higher levels of simplified multilevel models are out there in the <a href=\"https://wiki.lesswrong.com/wiki/territory\">territory</a>, that concepts constructed by mind in themselves play a role in the behavior of reality. This doesn't contradict the notion that the concepts used in simplified multilevel models refer to the actual clusters of configurations of reality.</p><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/reductionism-sequence\">Reductionism (sequence)</a></li><li><a href=\"https://www.lesswrong.com/tag/universal-law\">Universal law</a>, <a href=\"https://www.lesswrong.com/tag/magic\">Magic</a></li><li><a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy\">Mind projection fallacy</a></li><li><a href=\"https://www.lesswrong.com/tag/how-an-algorithm-feels\">How an algorithm feels</a></li><li><a href=\"https://www.lesswrong.com/tag/free-will\">Free will</a></li></ul>",
    "description_length": 962,
    "viewCount": 67,
    "parentTagId": "world-modeling-practice"
  },
  {
    "core-tag": "World Optimization",
    "_id": "JsJPrdgRGRqnci8cZ",
    "name": "Altruism",
    "slug": "altruism",
    "postCount": 90,
    "description_html": "<p><strong>Altruism</strong> refers to actions undertaken for the concern and benefit of others at ones own expense.</p>\n<blockquote>\n<p>\"an \"altruist\" is someone who chooses between actions according to the criterion of others' welfare\" - <a href=\"https://www.lesswrong.com/posts/EA39yRbhBbrccXnHi/inner-goodness\">Eliezer Yudkowsky</a></p>\n</blockquote>\n<p><em>Related tags and wikis:</em> <a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\">Shut up and multiply</a>, <a href=\"https://www.lesswrong.com/tag/fuzzies\">Fuzzies</a>, <a href=\"https://www.lesswrong.com/tag/world-optimization\">World Optimization</a>, <a href=\"https://www.lesswrong.com/tag/effective-altruism\">Effective Altruism</a>, <a href=\"https://www.lesswrong.com/tag/cause-prioritization\">Cause Prioritization</a>, <a href=\"https://www.lesswrong.com/tag/motivations\">Motivations</a>, <a href=\"https://www.lesswrong.com/tag/psychology-of-altruism\">Psychology of Altruism</a>, <a href=\"https://www.lesswrong.com/tag/ethics-and-morality\">Ethics and Morality</a></p>\n",
    "description_length": 1040,
    "viewCount": 101,
    "parentTagId": "world-optimization-moral-theory"
  },
  {
    "core-tag": "World Optimization",
    "_id": "ZTRNmvQGgoYiymYnq",
    "name": "Consequentialism",
    "slug": "consequentialism",
    "postCount": 89,
    "description_html": "<p><strong>Consequentialism</strong> is the ethical theory that people should choose their actions based on the outcomes they expect will result. Particular frameworks of consequentialism specify how outcomes should be judged. For example, <a href=\"https://www.lesswrong.com/tag/utilitarianism\">utilitarianism</a> holds that the best outcome is that which maximizes the total welfare of all people, and ethical egoism holds that the best outcome is that which maximizes their own personal interests. Consequentialism is one of three main strands of ethical thought, along with deontology, which holds that people should choose actions based on the merit of the act itself, and virtue ethics, which holds that people should be judged by how virtuous they are, as an assessment of their entire history of actions.</p><p>Related: <a href=\"https://www.lesswrong.com/tag/ethics-and-morality\">Ethics &amp; Morality</a>, <a href=\"http://lesswrong.com/tag/deontology\">Deontology</a>, <a href=\"/tag/moral-uncertainty\">Moral Uncertainty</a>, <a href=\"https://www.lesswrong.com/tag/utilitarianism\">Utilitarianism</a></p><p>Consequentialism is often associated with maximizing the <a href=\"https://www.lesswrong.com/tag/expected-utility\">expected value</a> of a <a href=\"https://www.lesswrong.com/tag/utility-functions\">utility function</a>. However, it has been argued that consequentialism is not the same thing as having a utility function because it is possible to evaluate actions based on their consequences without obeying the <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">von Neuman-Morgenstern axioms</a> necessary for having a utility function, and because utility functions can also be used to implement moral theories similar to deontology.</p><h2>Blog posts</h2><ul><li><a href=\"https://www.lesswrong.com/lw/uv/ends_dont_justify_means_among_humans/\">Ends Don't Justify Means (Among Humans)</a></li><li><a href=\"https://www.lesswrong.com/lw/kn/torture_vs_dust_specks/\">Torture vs. Dust Specks</a></li><li><a href=\"https://www.lesswrong.com/lw/1og/deontology_for_consequentialists/\">Deontology for Consequentialists</a></li><li><a href=\"https://www.lesswrong.com/lw/2aa/virtue_ethics_for_consequentialists/\">Virtue Ethics for Consequentialists</a></li><li><a href=\"https://www.lesswrong.com/lw/778/consequentialism_need_not_be_nearsighted/\">Consequentialism Need Not Be Shortsighted</a></li></ul><h2>External links</h2><ul><li><a href=\"http://plato.stanford.edu/archives/win2011/entries/consequentialism/\">Consequentialism entry on Stanford Encyclopedia of Philosophy</a></li><li><a href=\"http://www.raikoth.net/consequentialism.html\">Consequentialism FAQ</a> by Scott Alexander</li><li><a href=\"http://people.howstuffworks.com/trolley-problem.htm\">Description and discussion about trolley problems</a></li></ul><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/utilitarianism\">Utilitarianism</a></li><li><a href=\"https://www.lesswrong.com/tag/utility\">Utility</a>, <a href=\"https://www.lesswrong.com/tag/utility-functions\">utility function</a>, <a href=\"https://www.lesswrong.com/tag/expected-utility\">expected utility</a></li><li><a href=\"https://www.lesswrong.com/tag/metaethics-sequence\">Metaethics sequence</a></li><li><a href=\"https://www.lesswrong.com/tag/ethical-injunction\">Ethical injunction</a></li><li><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\">Shut up and multiply</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Hedons\">Hedons</a>, <a href=\"https://wiki.lesswrong.com/wiki/utils\">utils</a>, <a href=\"https://www.lesswrong.com/tag/fuzzies\">fuzzies</a></li></ul><h2>References</h2><ul><li>Jeremy Bentham (1907). <i>An Introduction to the Principles of Morals and Legislation</i>. Library of Economics and Liberty.</li><li>Perter Fishburn (1970). <i>Utility Theory for Decision Making</i>. Huntington, NY.</li><li>Walter Sinnot-Armstrong (2011). \"<a href=\"http://plato.stanford.edu/archives/win2011/entries/consequentialism/\"><u>Consequentialism</u></a>\". <i>The Stanford Encyclopedia of Philosophy (Winter 2011 Edition)</i>.</li><li>Judith Jarvis Thonson (1975). \"Killing, Letting Die, and the Trolley Problem\". <i>The Monist</i> <strong>59</strong>: 204-217.</li></ul>",
    "description_length": 4258,
    "viewCount": 105,
    "parentTagId": "world-optimization-moral-theory"
  },
  {
    "core-tag": "World Optimization",
    "_id": "yeJFqsWrP2pjYfNEr",
    "name": "Deontology",
    "slug": "deontology",
    "postCount": 35,
    "description_html": null,
    "description_length": null,
    "viewCount": 70,
    "parentTagId": "world-optimization-moral-theory"
  },
  {
    "core-tag": "World Optimization",
    "_id": "nSHiKwWyMZFdZg5qt",
    "name": "Ethics & Morality",
    "slug": "ethics-and-morality",
    "postCount": 532,
    "description_html": "<p>For general discussion about <strong>ethics and morality</strong>. Example posts: <a href=\"https://www.lesswrong.com/posts/fATPBv4pnHC33EmJ2/fake-morality\">Fake Morality</a>; <a href=\"https://www.lesswrong.com/posts/iGH7FSrdoCXa5AHGs/what-would-you-do-without-morality\">What Would You Do Without Morality?</a>; <a href=\"https://www.lesswrong.com/posts/B5K3hg8FgrMDHuXjH/the-terrible-horrible-no-good-very-bad-truth-about-morality\">The Terrible, Horrible, No Good, Very Bad Truth About Morality and What To Do About It</a>; <a href=\"https://www.lesswrong.com/posts/faHbrHuPziFH7Ef7p/why-are-individual-iq-differences-ok\">Why Are Individual IQ Differences OK?</a>; <a href=\"https://www.lesswrong.com/posts/Aq8BQMXRZX3BoFd4c/morality-is-awesome\">Morality is Awesome</a>.</p><p>See also <a href=\"https://www.lesswrong.com/tag/consequentialism\">Consequentialism</a>, <a href=\"https://www.lesswrong.com/tag/deontology\">Deontology</a>, <a href=\"https://www.lesswrong.com/tag/metaethics\">Metaethics</a>, and <a href=\"https://www.lesswrong.com/tag/moral-uncertainty\">Moral Uncertainty</a>.</p>",
    "description_length": 1087,
    "viewCount": 196,
    "parentTagId": "world-optimization-moral-theory"
  },
  {
    "core-tag": "World Optimization",
    "_id": "Z8wZZLeLMJ3NSK7kR",
    "name": "Metaethics",
    "slug": "metaethics",
    "postCount": 104,
    "description_html": "<p><strong>Metaethics</strong> is one of the three branches of ethics usually recognized by philosophers, the others being <a href=\"http://en.wikipedia.org/wiki/Normative_ethics\">normative ethics</a> and <a href=\"http://en.wikipedia.org/wiki/Applied_ethics\">applied ethics</a>. It’s a field of study that tries to understand the metaphysical, epistemological and semantic characteristics as well as the foundations and scope of moral values. It worries about questions and problems such as \"Are moral judgments objective or subjective, relative or absolute?\", \"Do moral facts exist?\" or “How do we learn moral values?”. (As distinct from object-level moral questions like, \"Ought I to steal from banks in order to give the money to the deserving poor?\")</p><h2>Metaethics on LessWrong</h2><p>Eliezer Yudkowsky wrote a Sequence about metaethics, the <a href=\"https://www.lesswrong.com/s/W2fkmatEzyrmbbrDt\">Metaethics sequence</a>, which Yudkowsky worried failed to convey his central point (<a href=\"https://www.lesswrong.com/posts/3R2vH2Ar5AbC9m8Qj/what-is-eliezer-yudkowsky-s-meta-ethical-theory\">this post by Luke</a> tried to clarify); he approached the same problem again from a different angle in <a href=\"https://www.lesswrong.com/s/SqFbMbtxGybdS2gRs\">Highly Advanced Epistemology 101 for Beginners</a>. From a standard philosophical standpoint, Yudkowsky's philosophy is closest to Frank Jackson's moral functionalism / analytic descriptivism; Yudkowsky could be loosely characterized as moral cognitivist - someone who believes moral sentences are either true or false - but not a moral realist - thus denying that moral sentences refer to facts about the world. Yudkowsky believes that moral cognition in any single human is at least potentially about a subject matter that is 'logical' in the sense that its semantics can be pinned down by axioms, and hence that moral cognition can bear truth-values; also that human beings both using similar words like \"morality\" can be talking about highly overlapping subject matter; but not that all possible minds would find the truths about this subject matter to be psychologically compelling.</p><p>Luke Muehlhauser has written a sequence, <a href=\"https://www.lessestwrong.com/s/bQgRsy23biR52poMf\">No-Nonsense Metaethics</a>, where he claims that many of the questions of metaethics can be answered today using modern neuroscience and rationality. He explains how conventional metaethics or \"Austere Metaethics\" is capable of, after assuming a definition of 'right', choosing the right action given a situation - but useless without assuming some criteria for 'right'. He proposes instead \"Empathic Metaethics\" which utilizes your underlying cognitive algorithms to understand what you think 'right' means, helps clarify any emotional and cognitive contradictions in it, and then tells you what the right thing to do is, <i>according to your definition of right</i>. This approach is highly relevant for the <a href=\"https://www.lesswrong.com/tag/friendly-artificial-intelligence\">Friendly AI</a> problem as a way of defining human-like goals and motivations when designing AIs.</p><h2>Further Reading &amp; References</h2><ul><li>Garner, Richard T.; Bernard Rosen (1967). Moral Philosophy: A Systematic Introduction to Normative Ethics and Meta-ethics. New York: Macmillan. pp. 215</li><li><a href=\"http://plato.stanford.edu/entries/metaethics/\">Metaethics</a> in the Stanford Encyclopedia of Philosophy</li></ul><h2>See Also</h2><ul><li><a href=\"https://lessestwrong.com/tag/complexity-of-value\">Complexity of value</a></li><li><a href=\"https://lessestwrong.com/tag/utility\">Utility</a></li></ul>",
    "description_length": 3652,
    "viewCount": 104,
    "parentTagId": "world-optimization-moral-theory"
  },
  {
    "core-tag": "World Optimization",
    "_id": "ouT6wKhACJRouGokM",
    "name": "Moral Uncertainty",
    "slug": "moral-uncertainty",
    "postCount": 79,
    "description_html": "<p><strong>Moral uncertainty</strong> (or <strong>normative uncertainty</strong>) is uncertainty about what we ought, morally, to do given the diversity of moral doctrines. For example, suppose that we knew for certain that new technology would enable more humans to live on another planet with slightly less well-being than on Earth<a href=\"https://www.lesswrong.com/tag/moral-uncertainty#fn1\"><sup>1</sup></a>. An average <a href=\"https://www.lesswrong.com/tag/utilitarianism\">utilitarian</a> would consider these consequences bad, while a total utilitarian would endorse such technology. If we are uncertain about which of these two theories are right, what should we do?</p><p>Moral uncertainty includes a level of uncertainty above the more usual uncertainty of <a href=\"https://www.lesswrong.com/tag/decision-theory\">what to do given incomplete information</a> since it deals also with uncertainty about which moral theory is right. Even with complete information about the world, this kind of uncertainty would still remain <a href=\"https://www.lesswrong.com/tag/moral-uncertainty#fn1\"><sup>1</sup></a>. In one level of uncertainty, one can have doubts on how to act because all the relevant empirical information isn’t available, for example, choosing whether to implement or not a new technology (e.g.: <a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\">AGI</a>, <a href=\"https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement\">Biological Cognitive Enhancement</a>, <a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\">Mind Uploading</a>) not fully knowing about its consequences and nature. But even if we ideally get to know each and every consequence of new technology, we would still need to know which is the right ethical perspective for analyzing these consequences.</p><p>One approach is to follow only the most probable theory. This has its own problems. For example, what if the most probable theory points only weakly in one way, and other theories point strongly the other way? A better approach is to “perform the action with the highest expected moral value. We get the expected moral value of an action by multiplying the subjective probability that some theory is true by the value of that action if it is true, doing the same for all of the other theories, and adding up the results.” <a href=\"https://www.lesswrong.com/tag/moral-uncertainty#fn2\"><sup>2</sup></a> However, we would still need a method of comparing value intertheories, an <a href=\"https://www.lesswrong.com/tag/utility\">utilon</a> in one theory may not be the same with an utilon in another theory. Outside <a href=\"https://www.lesswrong.com/tag/consequentialism\">consequentialism</a>, many ethical theories don’t use utilions or even any quantifiable values. This is still an open problem.</p><p><a href=\"https://www.lesswrong.com/tag/nick-bostrom\">Nick Bostrom</a> and <a href=\"https://en.wikipedia.org/wiki/Toby_Ord\">Toby Ord</a> have proposed a <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">parliamentary model</a>. In this model, each theory sends a number of delegates to a parliament in proportion to its probability. The theories then bargain for support as if the probability of each action were proportional to its votes. However, the actual output is always the action with the most votes. Bostrom and Ord's proposal lets probable theories determine most actions, but still gives less probable theories influence on issues they consider unusually important.</p><p>Even with a high degree of moral uncertainty and a wide range of possible moral theories, there are still certain actions that seem highly valuable in any theory. Bostrom argues that <a href=\"https://www.lesswrong.com/tag/existential-risk\">Existential risk</a> reduction is among them, showing that it is not only the most important task given most versions of consequentialism but highly recommended by many of the other widely acceptable moral theories<a href=\"https://www.lesswrong.com/tag/moral-uncertainty#fn3\"><sup>3</sup></a>.</p><h2>External links</h2><ul><li><a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">Moral uncertainty — towards a solution?</a></li></ul><h2>Sequences</h2><ul><li><a href=\"https://www.lesswrong.com/s/4NFwxwzLzpiikfkk3\">Moral uncertainty</a></li></ul><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/expected-utility\">Expected utility</a></li><li><a href=\"https://www.lesswrong.com/tag/value-learning\">Value learning</a></li><li><a href=\"https://www.lesswrong.com/tag/metaethics\">Metaethics</a></li></ul><h2>References</h2><ol><li>Crouch, William. (2010) “Moral Uncertainty and Intertheoretic Comparisons of Value” BPhil Thesis, 2010. p. 6. Available at: <a href=\"http://oxford.academia.edu/WilliamCrouch/Papers/873903/Moral_Uncertainty_and_Intertheoretic_Comparisons_of_Value\">http://oxford.academia.edu/WilliamCrouch/Papers/873903/Moral_Uncertainty_and_Intertheoretic_Comparisons_of_Value</a><a href=\"https://www.lesswrong.com/tag/moral-uncertainty#fnref1\">↩</a></li><li>Sepielli, Andrew. (2008) “Moral Uncertainty and the Principle of Equity among Moral Theories\". ISUS-X, Tenth Conference of the International Society for Utilitarian Studies, Kadish Center for Morality, Law and Public Affairs, UC Berkeley. Available at: <a href=\"http://escholarship.org/uc/item/7h5852rr.pdf\">http://escholarship.org/uc/item/7h5852rr.pdf</a><a href=\"https://www.lesswrong.com/tag/moral-uncertainty#fnref2\">↩</a></li><li>Bostrom, Nick. (2012) \"Existential Risk Reduction as the Most Important Task for Humanity\" Global Policy, forthcoming, 2012. p. 22. Available at: <a href=\"http://www.existential-risk.org/concept.pdf\">http://www.existential-risk.org/concept.pdf</a><a href=\"https://www.lesswrong.com/tag/moral-uncertainty#fnref3\">↩</a></li></ol>",
    "description_length": 5883,
    "viewCount": 84,
    "parentTagId": "world-optimization-moral-theory"
  },
  {
    "core-tag": "World Optimization",
    "_id": "LMFBzsJaCRADQqw3F",
    "name": "Trolley Problem",
    "slug": "trolley-problem",
    "postCount": 19,
    "description_html": "<p>The <a href=\"https://en.wikipedia.org/wiki/Trolley_problem\"><strong>trolley problem</strong></a> is a decision scenario which illustrates/tests ethical reasoning. (\"Trolley problems\" are modified scenarios inspired by the basic one.) In the classic scenario, you are faced with a situation where five people will die if you do nothing. However, you can kill a bystander to save the lives of the five. Many people will choose to do nothing, even though many utilitarian/consequentialist positions would say to kill the bystander.</p>",
    "description_length": 535,
    "viewCount": 41,
    "parentTagId": "world-optimization-moral-theory"
  },
  {
    "core-tag": "World Optimization",
    "_id": "t7t9nW6BtJhfGNSR6",
    "name": "Aging",
    "slug": "aging",
    "postCount": 68,
    "description_html": null,
    "description_length": null,
    "viewCount": 183,
    "parentTagId": "world-optimization-causes"
  },
  {
    "core-tag": "World Optimization",
    "_id": "frcrRgCk9PDbEScua",
    "name": "Climate Change",
    "slug": "climate-change",
    "postCount": 59,
    "description_html": null,
    "description_length": null,
    "viewCount": 109,
    "parentTagId": "world-optimization-causes"
  },
  {
    "core-tag": "World Optimization",
    "_id": "Rz5jb3cYHTSRmqNnN",
    "name": "Existential Risk",
    "slug": "existential-risk",
    "postCount": 440,
    "description_html": "<p>An <strong>existential risk</strong> (or <strong>x-risk</strong>) is a risk that poses astronomically large negative consequences for humanity, such as human extinction or permanent global totalitarianism.</p>\n<p><a href=\"https://lesswrong.com/tag/nick-bostrom\">Nick Bostrom</a> introduced the term \"existential risk\" in his 2002 paper \"<a href=\"https://www.nickbostrom.com/existential/risks.pdf\">Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards</a>.\"<a href=\"https://lesswrong.com/tag/existential-risk?revision=0.0.39#fn1\"><sup>1</sup></a> In the paper, Bostrom defined an existential risk as:</p>\n<blockquote>\n<p>One where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.</p>\n</blockquote>\n<p>The Oxford <a href=\"https://www.lesswrong.com/tag/future-of-humanity-institute-fhi\">Future of Humanity Institute</a> (FHI) was founded by Bostrom in 2005 in part to study existential risks. Other institutions with a generalist focus on existential risk include the <a href=\"https://www.cser.ac.uk/\">Centre for the Study of Existential Risk</a>.</p>\n<p>FHI's <a href=\"https://www.existential-risk.org/faq.html\">existential-risk.org FAQ</a> notes regarding the definition of \"existential risk\":</p>\n<blockquote>\n<p>An existential risk is one that threatens the entire future of humanity. [...]</p>\n<p>“Humanity”, in this context, does not mean “the biological species <em>Homo sapiens</em>”. If we humans were to evolve into another species, or merge or replace ourselves with intelligent machines, this would not necessarily mean that an existential catastrophe had occurred — although it might if the quality of life enjoyed by those new life forms turns out to be far inferior to that enjoyed by humans.</p>\n</blockquote>\n<h2>Classification of Existential Risks</h2>\n<p>Bostrom<a href=\"https://lesswrong.com/tag/existential-risk?revision=0.0.39#fn2\"><sup>2</sup></a> proposes a series of classifications for existential risks:</p>\n<ul>\n<li><strong>Bangs</strong> - Earthly intelligent life is extinguished relatively suddenly by any cause; the prototypical end of humanity. Examples of bangs include deliberate or accidental misuse of nanotechnology, nuclear holocaust, <a href=\"https://lesswrong.com/tag/simulation-argument\">the end of our simulation</a>, or an <a href=\"https://wiki.lesswrong.com/wiki/unfriendly_AI\">unfriendly AI</a>.</li>\n<li><strong>Crunches</strong> - The potential humanity had to enhance itself indefinitely is forever eliminated, although humanity continues. Possible crunches include an exhaustion of resources, social or governmental pressure ending technological development, and even future technological development proving an unsurpassable challenge before the creation of a <a href=\"https://lesswrong.com/tag/superintelligence\">superintelligence</a>.</li>\n<li><strong>Shrieks</strong> - Humanity enhances itself, but explores only a narrow portion of its desirable possibilities. As the <a href=\"https://lesswrong.com/tag/complexity-of-value\">criteria for desirability haven't been defined yet</a>, this category is mainly undefined. However, a flawed <a href=\"https://wiki.lesswrong.com/wiki/friendly_AI\">friendly AI</a> incorrectly interpreting our values, a superhuman <a href=\"https://wiki.lesswrong.com/wiki/WBE\">upload</a> deciding its own values and imposing them on the rest of humanity, and an intolerant government outlawing social progress would certainly qualify.</li>\n<li><strong>Whimpers</strong> - Though humanity is enduring, only a fraction of our potential is ever achieved. Spread across the galaxy and expanding at near light-speed, we might find ourselves doomed by ours or another being's catastrophic physics experimentation, destroying reality at light-speed. A prolonged galactic war leading to our extinction or severe limitation would also be a whimper. More darkly, humanity might develop until its <a href=\"https://lesswrong.com/tag/complexity-of-value\">values</a> were disjoint with ours today, making their civilization worthless by present values.</li>\n</ul>\n<p>The total negative results of an existential risk could amount to the total of potential future lives not being realized. A rough and conservative calculation<a href=\"https://lesswrong.com/tag/existential-risk?revision=0.0.39#fn3\"><sup>3</sup></a> gives us a total of 10^54 potential future humans lives – smarter, happier and kinder then we are. Hence, almost no other task would amount to so much positive impact than existential risk reduction.</p>\n<p>Existential risks also present an unique challenge because of their irreversible nature. We will never, by definition, experience and survive an extinction risk<a href=\"https://lesswrong.com/tag/existential-risk?revision=0.0.39#fn4\"><sup>4</sup></a> and so cannot learn from our mistakes. They are subject to strong <a href=\"https://lesswrong.com/tag/observation-selection-effect\">observational selection effects</a> <a href=\"https://lesswrong.com/tag/existential-risk?revision=0.0.39#fn5\"><sup>5</sup></a>. One cannot estimate their future probability based on the past, because <a href=\"https://lesswrong.com/tag/bayesian-probability\">bayesianly</a> speaking, the conditional probability of a past existential catastrophe given our present existence is always 0, no matter how high the probability of an existential risk really is. Instead, indirect estimates have to be used, such as possible existential catastrophes happening elsewhere. A high extinction risk probability could be functioning as a <a href=\"https://lesswrong.com/tag/great-filter\">Great Filter</a> and explain why there is no evidence of spacial colonization.</p>\n<p>Another related idea is that of a <a href=\"https://lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\">suffering risk</a> (or s-risk).</p>\n<h2>History</h2>\n<p>The focus on existential risks on LessWrong dates back to Bostrom's 2002 paper <a href=\"https://www.nickbostrom.com/astronomical/waste.html\">Astronomical Waste: The Opportunity Cost of Delayed Technological Development</a>. It argues that \"the chief goal for utilitarians should be to reduce existential risk\". Bostrom writes:</p>\n<blockquote>\n<p>If what we are concerned with is (something like) maximizing the expected number of worthwhile lives that we will create, then in addition to the opportunity cost of delayed colonization, we have to take into account the risk of failure to colonize at all. We might fall victim to an <em>existential risk</em>, one where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.[8] Because the lifespan of galaxies is measured in billions of years, whereas the time-scale of any delays that we could realistically affect would rather be measured in years or decades, the consideration of risk trumps the consideration of opportunity cost. For example, a single percentage point of reduction of existential risks would be worth (from a utilitarian expected utility point-of-view) a delay of over 10 million years.<br>\nTherefore, if our actions have even the slightest effect on <em>the probability</em> of eventual colonization, this will outweigh their effect on <em>when</em> colonization takes place. For standard utilitarians, priority number one, two, three and four should consequently be to reduce existential risk. The utilitarian imperative “Maximize expected aggregate utility!” can be simplified to the maxim “Minimize existential risk!”.</p>\n</blockquote>\n<p>The concept is expanded upon in his 2012 paper <a href=\"https://www.existential-risk.org/concept.html\">Existential Risk Prevention as Global Priority</a></p>\n<h2>Organizations</h2>\n<ul>\n<li><a href=\"http://intelligence.org/\">Machine Intelligence Research Institute</a></li>\n<li><a href=\"http://www.fhi.ox.ac.uk/\">The Future of Humanity Institute</a></li>\n<li><a href=\"http://www.futuretech.ox.ac.uk/\">The Oxford Martin Programme on the Impacts of Future Technology</a></li>\n<li><a href=\"http://www.gcrinstitute.org/\">Global Catastrophic Risk Institute</a></li>\n<li><a href=\"http://shfhs.org/\">Saving Humanity from Homo Sapiens</a></li>\n<li><a href=\"http://www.skollglobalthreats.org/\">Skoll Global Threats Fund (To Safeguard Humanity from Global Threats)</a></li>\n<li><a href=\"http://www.foresight.org/\">Foresight Institute</a></li>\n<li><a href=\"http://nuclearrisk.org/\">Defusing the Nuclear Threat</a></li>\n<li><a href=\"http://www.leverageresearch.org/\">Leverage Research</a></li>\n<li><a href=\"http://lifeboat.com/\">The Lifeboat Foundation</a></li>\n</ul>\n<h2>References</h2>\n<ol>\n<li>BOSTROM, Nick. (2002) \"<a href=\"http://www.nickbostrom.com/existential/risks.pdf\">Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards</a>\". Journal of Evolution and Technology, Vol. 9, March 2002.</li>\n<li>BOSTROM, Nick. (2012) \"<a href=\"http://www.existential-risk.org/concept.pdf\">Existential Risk Reduction as the Most Important Task for Humanity</a>\". Global Policy, forthcoming, 2012.</li>\n<li>BOSTROM, Nick &amp; SANDBERG, Anders &amp; CIRKOVIC, Milan. (2010) \"Anthropic Shadow: Observation Selection Effects and Human Extinction Risks\" Risk Analysis, Vol. 30, No. 10 (2010): 1495-1506.</li>\n<li>Nick Bostrom, Milan M. Ćirković, ed (2008). <em>Global Catastrophic Risks</em>. Oxford University Press.</li>\n<li>Milan M. Ćirković (2008). <a href=\"http://books.google.com/books?id=-Jxc88RuJhgC&amp;lpg=PP1&amp;pg=PA120#v=onepage&amp;q=&amp;f=false\">\"Observation Selection Effects and global catastrophic risks\"</a>. <em>Global Catastrophic Risks</em>. Oxford University Press.</li>\n<li>Eliezer S. Yudkowsky (2008). <a href=\"http://yudkowsky.net/rational/cognitive-biases\">\"Cognitive Biases Potentially Affecting Judgment of Global Risks\"</a>. <em>Global Catastrophic Risks</em>. Oxford University Press. (<a href=\"http://intelligence.org/files/CognitiveBiases.pdf\">PDF</a>)</li>\n<li>Richard A. Posner (2004). <a href=\"http://books.google.ca/books?id=SDe59lXSrY8C\"><em>Catastrophe Risk and Response</em></a>. Oxford University Press. (<a href=\"http://www.avturchin.narod.ru/posner.doc\">DOC</a>)</li>\n</ol>\n",
    "description_length": 10203,
    "viewCount": 324,
    "parentTagId": "world-optimization-causes"
  },
  {
    "core-tag": "World Optimization",
    "_id": "pGqRLe9bFDX2G2kXY",
    "name": "Futurism",
    "slug": "futurism",
    "postCount": 141,
    "description_html": "<p><strong>Futurism</strong> is speculation about technologies or social trends that might exist in the near or distant future.</p><p>Less Wrong&apos;s favorite type of futurism is speculation about <a href=\"https://www.lesswrong.com/tag/ai-risk\">AI risk</a>. Other speculative future technologies include <a href=\"http://lesswrong.com/tag/life-extension\">life extension</a>, <a href=\"http://lesswrong.com/tag/mind-uploading\">mind uploading</a>, <a href=\"http://lesswrong.com/tag/nanotechnology\">nanotechnology</a>, and <a href=\"https://www.lesswrong.com/tag/space-exploration-and-colonization\">space colonization</a>.</p><p>For efforts to predict future trends see <a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\">Forecasting &amp; Prediction</a> and <a href=\"https://www.lesswrong.com/tag/forecasts-lists-of\">Forecasts (Lists of)</a>.</p><p>See also: <a href=\"http://lesswrong.com/tag/transhumanism\">Transhumanism</a>, <a href=\"http://lesswrong.com/tag/fun-theory\">Fun Theory</a></p>",
    "description_length": 1003,
    "viewCount": 47,
    "parentTagId": "world-optimization-causes"
  },
  {
    "core-tag": "World Optimization",
    "_id": "vmvTYnmaKA73fYDe5",
    "name": "Life Extension",
    "slug": "life-extension",
    "postCount": 85,
    "description_html": "<p><strong>Life Extension </strong>is the theory / practice of extending human lifespans – for decades, centuries. or much longer. This includes advice that applies to individuals, research projects that might extend human lifespans as a whole, or philosophical discussion of the concept.&nbsp;</p><p>See also <a href=\"https://www.lesswrong.com/tag/aging\">Aging</a>, and <a href=\"https://www.lessestwrong.com/tag/cryonics\">Cryonics</a> - a particular life extension technique that has received a lot of discussion on LessWrong.</p>",
    "description_length": 531,
    "viewCount": 120,
    "parentTagId": "world-optimization-causes"
  },
  {
    "core-tag": "World Optimization",
    "_id": "nzvHaqwdXtvWkbonG",
    "name": "Risks of Astronomical Suffering (S-risks)",
    "slug": "risks-of-astronomical-suffering-s-risks",
    "postCount": 61,
    "description_html": "<p><strong>(Astronomical) suffering risks</strong>, also known as <strong>s-risks</strong>, are risks of the creation of intense suffering in the far future on an astronomical scale, vastly exceeding all suffering that has existed on Earth so far.</p><p>S-risks are an example of <a href=\"https://www.lesswrong.com/tag/existential-risk\">existential risk</a> (also known as <i>x-risks</i>) according to Nick Bostrom's original definition, as they threaten to \"permanently and drastically curtail [Earth-originating intelligent life's] potential\". Most existential risks are of the form \"event E happens which drastically reduces the number of conscious experiences in the future\". S-risks therefore serve as a useful reminder that some x-risks are scary because they cause <i>bad</i> experiences, and not just because they prevent good ones.</p><p>Within the space of x-risks, we can distinguish x-risks that are s-risks, x-risks involving human extinction, x-risks that involve immense suffering <i>and</i> human extinction, and x-risks that involve neither. For example:</p><figure class=\"table\"><table><tbody><tr><td>&nbsp;</td><td><strong>extinction risk</strong></td><td><strong>non-extinction risk</strong></td></tr><tr><td><strong>suffering risk</strong></td><td>Misaligned AGI wipes out humans, simulates many suffering alien civilizations.</td><td>Misaligned AGI tiles the universe with experiences of severe suffering.</td></tr><tr><td><strong>non-suffering risk</strong></td><td>Misaligned AGI wipes out humans.</td><td>Misaligned AGI keeps humans as \"pets,\" limiting growth but not causing immense suffering.</td></tr></tbody></table></figure><p>A related concept is <a href=\"https://arbital.com/p/hyperexistential_separation/\"><strong>hyperexistential risk</strong></a>, the risk of \"fates worse than death\" on an astronomical scale. It is not clear whether all hyperexistential risks are s-risks per se. But arguably all s-risks are hyperexistential, since \"tiling the universe with experiences of severe suffering\" would likely be worse than death.</p><p>There are two <a href=\"https://wiki.lesswrong.com/wiki/EA\">EA</a> organizations with s-risk prevention research as their primary focus: the <a href=\"https://www.lesswrong.com/tag/center-on-long-term-risk-clr\">Center on Long-Term Risk</a> (CLR) and the <a href=\"https://centerforreducingsuffering.org/\">Center for Reducing Suffering</a>. Much of CLR's work is on suffering-focused <a href=\"https://wiki.lesswrong.com/wiki/AI_safety\">AI safety</a> and <a href=\"https://www.lesswrong.com/tag/crucial-considerations\">crucial considerations</a>. Although to a much lesser extent, the <a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri\">Machine Intelligence Research Institute</a> and <a href=\"https://www.lesswrong.com/tag/future-of-humanity-institute-fhi\">Future of Humanity Institute</a> have investigated strategies to prevent s-risks too.&nbsp;</p><p>Another approach to reducing s-risk is to \"expand the moral circle\" <a href=\"https://magnusvinding.com/2018/09/04/moral-circle-expansion-might-increase-future-suffering/\"><i>together</i></a> with raising concern for suffering, so that future (post)human civilizations and AI are less likely to <a href=\"https://www.lesswrong.com/tag/instrumental-value\">instrumentally</a> cause suffering to non-human minds such as animals or digital sentience. <a href=\"http://www.sentienceinstitute.org/\">Sentience Institute</a> works on this value-spreading problem.</p><p>&nbsp;</p><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/center-on-long-term-risk-clr\">Center on Long-Term Risk</a></li><li><a href=\"https://www.lesswrong.com/tag/existential-risk\">Existential risk</a></li><li><a href=\"https://www.lesswrong.com/tag/abolitionism\">Abolitionism</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Mind_crime\">Mind crime</a></li><li><a href=\"https://www.lesswrong.com/tag/utilitarianism\">Utilitarianism</a>, <a href=\"https://www.lesswrong.com/tag/hedonism\">Hedonism</a></li></ul><p>&nbsp;</p><h2>External links</h2><ul><li><a href=\"https://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-global-priority/\">Reducing Risks of Astronomical Suffering: A Neglected Global Priority (FRI)</a></li><li><a href=\"https://foundational-research.org/s-risks-talk-eag-boston-2017/\">Introductory talk on s-risks (FRI)</a></li><li><a href=\"https://foundational-research.org/risks-of-astronomical-future-suffering/\">Risks of Astronomical Future Suffering (FRI)</a></li><li><a href=\"https://foundational-research.org/files/suffering-focused-ai-safety.pdf\">Suffering-focused AI safety: Why \"fail-safe\" measures might be our top intervention PDF (FRI)</a></li><li><a href=\"https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering\">Artificial Intelligence and Its Implications for Future Suffering (FRI)</a></li><li><a href=\"https://sentience-politics.org/expanding-moral-circle-reduce-suffering-far-future/\">Expanding our moral circle to reduce suffering in the far future (Sentience Politics)</a></li><li><a href=\"https://sentience-politics.org/philosophy/the-importance-of-the-future/\">The Importance of the Far Future (Sentience Politics)</a></li></ul>",
    "description_length": 5268,
    "viewCount": 405,
    "parentTagId": "world-optimization-causes"
  },
  {
    "core-tag": "World Optimization",
    "_id": "jiuackr7B5JAetbF6",
    "name": "Transhumanism",
    "slug": "transhumanism",
    "postCount": 87,
    "description_html": "<p><strong>Transhumanism</strong> is the belief or movement in favour of human enhancement, especially beyond current human limitations and with advanced technology such as AI, cognitive enhancement, and life extension.</p><h2>References</h2><ul><li><a href=\"http://yudkowsky.net/singularity/simplified\">Transhumanism as Simplified Humanism</a> by <a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky</a></li><li>A <a href=\"http://www.ted.com/talks/nick_bostrom_on_our_biggest_problems.html\">TED talk</a> by transhumanist <a href=\"https://www.lesswrong.com/tag/nick-bostrom\">Nick Bostrom</a> on humanity's biggest problems</li><li><a href=\"http://www.nickbostrom.com/views/transhumanist.pdf\">The Transhumanist FAQ</a> (PDF) by <a href=\"https://www.lesswrong.com/tag/nick-bostrom\">Nick Bostrom</a> (<a href=\"http://whatistranshumanism.org/\">HTML version</a>)</li><li><a href=\"http://www.nickbostrom.com/ethics/values.html\">Transhumanist Values</a> by <a href=\"https://www.lesswrong.com/tag/nick-bostrom\">Nick Bostrom</a></li><li><a href=\"http://www.nickbostrom.com/papers/history.pdf\">A History of Transhumanist Thought</a> (PDF) by <a href=\"https://www.lesswrong.com/tag/nick-bostrom\">Nick Bostrom</a></li><li><a href=\"https://web.archive.org/web/20130115205756/http://www.acceleratingfuture.com/michael/blog/2007/09/seven-definitions-of-transhumanism/\">Seven Definitions of Transhumanism</a> by <a href=\"https://www.lesswrong.com/tag/michael-anissimov\">Michael Anissimov</a></li><li><a href=\"https://www.youtube.com/watch?v=bTMS9y8OVuY\">PostHuman: An Introduction to Transhumanism</a> (video)</li></ul><h2>See Also</h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/H+Pedia\">H+Pedia</a></li><li><a href=\"https://www.lesswrong.com/tag/fun-theory\">Fun theory</a></li><li><a href=\"https://www.lesswrong.com/tag/metaethics-sequence\">Metaethics sequence</a></li><li><a href=\"https://www.lesswrong.com/tag/mind-uploading\">Mind uploading</a></li><li><a href=\"https://www.lesswrong.com/tag/cryonics\">Cryonics</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a></li><li><a href=\"https://www.lesswrong.com/tag/abolitionism\">Abolitionism</a></li></ul><h2>External links</h2><ul><li><a href=\"https://hpluspedia.org/\">H+Pedia, the transhumanist wiki</a></li></ul>",
    "description_length": 2306,
    "viewCount": 99,
    "parentTagId": "world-optimization-causes"
  },
  {
    "core-tag": "World Optimization",
    "_id": "mPuSAzJN7CyrMiKrf",
    "name": "Voting Theory",
    "slug": "voting-theory",
    "postCount": 60,
    "description_html": null,
    "description_length": null,
    "viewCount": 102,
    "parentTagId": "world-optimization-causes"
  },
  {
    "core-tag": "World Optimization",
    "_id": "kCuRQE5Tkv9zeKyzK",
    "name": "Common Knowledge",
    "slug": "common-knowledge",
    "postCount": 30,
    "description_html": "<p><strong>Common knowledge</strong> is information that everyone knows and, importantly, that everyone knows that everyone knows, and so on, ad infinitum. If information is <i>common knowledge</i> in a group of people, that information that can be relied and acted upon with the trust that everyone else is also coordinating around that information. This stands, in contrast, to merely publicly known information where one person cannot be sure that another person knows the information, or that another person knows that they know the information. Establishing true common knowledge is, in fact, rather hard.</p><p><strong>Related Pages:</strong> <a href=\"https://www.lesswrong.com/tag/public-discourse\">Public discourse</a>, <a href=\"https://www.lesswrong.com/tag/consensus\">Consensus</a>, <a href=\"https://www.lesswrong.com/tag/inferential-distance\">Inferential Distance</a></p><p><strong>External posts:</strong>&nbsp;<br><a href=\"https://www.scottaaronson.com/blog/?p=3376\">The Kolmogorov option</a> by Scott Aaronson<br><a href=\"https://slatestarcodex.com/2017/10/23/kolmogorov-complicity-and-the-parable-of-lightning/\">kolmogorov complicity and-the parable of lightning</a> by Scott Alexander</p>",
    "description_length": 1204,
    "viewCount": 287,
    "parentTagId": "world-optimization-working-with-humans"
  },
  {
    "core-tag": "World Optimization",
    "_id": "chuP2QqQycjD8qakL",
    "name": "Coordination / Cooperation",
    "slug": "coordination-cooperation",
    "postCount": 260,
    "description_html": "<p><strong>Coordination</strong> is the challenge of distinct actors being able to jointly choose their actions to achieve a favorable outcome. An example of easy coordination is coordinating to drive on the same side of the road. A hard example is coordinating everyone to move a superior yet different social media platform. Many failures of civilization are failures of coordination. A closely related concept is that of <strong>cooperation </strong>– multiple actors choosing their actions in ways that maximize collective value despite the temptation of greater short-term individual gain by acting to the detriment of the group/other actors. The Prisoner's Dilemma is the canonical cooperation/defection game, but the term is used in other games too, e.g. Stag Hunt.</p><p>Coordination and cooperation are fundamental concepts in <a href=\"https://www.lesswrong.com/tag/game-theory\">Game Theory</a>. LessWrong discussion has long been interested in overcoming the gnarly coordination and cooperation challenges that prevent many improvements to <a href=\"https://www.lesswrong.com/tag/world-optimization\">optimizing the world</a>. There is also interest because failures to coordinate/cooperate are likely causes of <a href=\"https://www.lesswrong.com/tag/existential-risk\">existential risks</a> such as <a href=\"https://www.lesswrong.com/tag/ai-risk\">AI Risk</a> or nuclear war.</p><p>See also: <a href=\"https://www.lesswrong.com/tag/game-theory\">Game Theory</a>, <a href=\"https://www.lesswrong.com/tag/moloch\">Moloch</a></p><p>Another related term in this space is <i>collective action problem.&nbsp;</i></p><p><strong>Related Sequences:</strong> <a href=\"https://www.lesswrong.com/s/vz9Zrj3oBGsttG3Jh\">Kickstarter for Coordinated Action</a></p>",
    "description_length": 1750,
    "viewCount": 156,
    "parentTagId": "world-optimization-working-with-humans"
  },
  {
    "core-tag": "World Optimization",
    "_id": "b8FHrKqyXuYGWc6vn",
    "name": "Game Theory",
    "slug": "game-theory",
    "postCount": 316,
    "description_html": "<p><strong>Game theory</strong> is the formal study of how rational actors interact to pursue incentives. It investigates situations of conflict and cooperation.</p>\n<p><em>See also:</em> <a href=\"https://www.lesswrong.com/tag/coordination-cooperation?showPostCount=true&amp;useTagName=true\">Coalition/coordination</a>, <a href=\"https://www.lesswrong.com/tag/coalitional-instincts?showPostCount=true&amp;useTagName=true\">Coalitional Instincts</a>, <a href=\"https://www.lesswrong.com/tag/decision-theory\">Decision theory</a>, <a href=\"https://www.lesswrong.com/tag/moloch?showPostCount=true&amp;useTagName=true\">Moloch</a>, <a href=\"https://www.lesswrong.com/tag/utility-functions\">Utility functions</a>, <a href=\"https://lesswrong.com/tag/decision-theory\">Decision Theory</a>, <a href=\"https://lesswrong.com/tag/prisoner-s-dilemma\">Prisoner's Dilemma</a></p>\n<p>Game theory is an extremely powerful and robust tool in analyzing much more complex situations, such as: mergers and acquisitions, political economy, voting systems, war bargaining and biological evolution. Eight game-theorists have won the Nobel Prize in Economic Sciences.</p>\n<h2>References</h2>\n<ul>\n<li><a href=\"http://levine.sscnet.ucla.edu/general/whatis.htm\">Naïve introduction to Game Theory</a></li>\n<li><a href=\"http://plato.stanford.edu/entries/game-theory/\">Stanford Encyclopedia entry on Game Theory</a></li>\n</ul>\n",
    "description_length": 1391,
    "viewCount": 896,
    "parentTagId": "world-optimization-working-with-humans"
  },
  {
    "core-tag": "World Optimization",
    "_id": "zv7v2ziqexSn5iS9v",
    "name": "Group Rationality",
    "slug": "group-rationality",
    "postCount": 99,
    "description_html": "<p>In almost anything, individuals are inferior to groups. Several articles address this concern regarding rationality, i.e., the topic of <strong>Group Rationality</strong>.</p><h2>External links</h2><ul><li><a href=\"https://medium.com/@ThingMaker/open-problems-in-group-rationality-5636440a2cd1\">Open Problems in Group Rationality</a> is one of the main articles about the subject.</li><li><a href=\"http://ratio.huji.ac.il/dp/dp154.pdf\">Individual and Group Behavior in the Ultimatum Game</a></li><li><a href=\"http://www.andrew.cmu.edu/user/kzollman/research/Presentations/LRR%20-%20IndividualVsSocial.pdf\">Individual vs. Group Rationality in Inquiry</a></li></ul><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/rationality-as-martial-art\">Rationality as martial art</a></li><li><a href=\"https://www.lesswrong.com/tag/problem-of-verifying-rationality\">Problem of verifying rationality</a></li><li><a href=\"/community\">Less Wrong meetup groups</a></li><li><a href=\"https://www.lesswrong.com/tag/the-craft-and-the-community\">The Craft and the Community</a></li></ul>",
    "description_length": 1082,
    "viewCount": 103,
    "parentTagId": "world-optimization-working-with-humans"
  },
  {
    "core-tag": "World Optimization",
    "_id": "o9aQASibdsECTfYF6",
    "name": "Moloch",
    "slug": "moloch",
    "postCount": 76,
    "description_html": "<p><strong>Moloch</strong> is the personification of the forces that coerce competing individuals to take actions which, although locally optimal, ultimately lead to situations where everyone is worse off. Moreover, no individual is able to unilaterally break out of the dynamic. The situation is a bad Nash equilibrium. A trap.</p>\n<p>It happens when \"In some competition optimizing for X, the opportunity arises to throw some other value under the bus for improved X. Those who take it prosper. Those who don’t take it die out. Eventually, everyone’s relative status is about the same as before, but everyone’s absolute status is worse than before. The process continues until all other values that can be traded off have been – in other words, until human ingenuity cannot possibly figure out a way to make things any worse.\" - Scott Alexander</p>\n<p>One example of a Molochian dynamic is a <a href=\"https://en.wikipedia.org/wiki/Red_Queen%27s_race\">Red Queen race</a> between scientists who must continually spend more time writing grant applications just to keep up with their peers doing the same. Through unavoidable competition, they have all lost time while not ending up with any more grant money. And any scientist who unilaterally tried to not engage in the competition would soon be replaced by one who still does. If they all promised to cap their grant writing time, everyone would face an incentive to defect.</p>\n<p>The topic of Moloch receives a formal treatment in the sequence <a href=\"https://www.lesswrong.com/s/oLGCcbnvabyibnG9d\">Inadequate Equilibria</a>, particularly in the chapter <a href=\"https://www.lesswrong.com/posts/x5ASTMPKPowLKpLpZ/moloch-s-toolbox-1-2\">Moloch's Toolbox</a>.</p>\n<h2>Origin</h2>\n<p><a href=\"https://www.lesswrong.com/users/yvain?sortedBy=top\">Scott Alexander</a> &nbsp;linked the name to the concept in his eponymous post, <a href=\"https://www.lesswrong.com/posts/TxcRbCYHaeL59aY7E/meditations-on-moloch\">Meditations on Moloch</a>. &nbsp;The post intersperses lines of Allan Ginsberg's poem, <a href=\"https://www.poetryfoundation.org/poems/49303/howl\">Howl</a>, with multiples examples of the dynamic including: the Prisoner's Dilemma, dollar auctions, <a href=\"https://web.archive.org/web/20160928190322/http://raikoth.net/libertarian.html\">fish farming story</a>, Malthusian trap, capitalism, two-income trap, agriculture, arms races, races to the bottom, education system, science, and government corruption and corporate welfare.</p>\n<p>From Allan Ginsberg's <a href=\"https://www.poetryfoundation.org/poems/49303/howl\">Howl</a>:</p>\n<blockquote>\n<p><em>What sphinx of cement and aluminum bashed open their skulls and ate up their brains and imagination?</em><br>\n<em>Moloch! Solitude! Filth! Ugliness! Ashcans and unobtainable dollars! Children screaming under the stairways! Boys sobbing in armies! Old men weeping in the parks!</em><br>\n<em>Moloch! Moloch! Nightmare of Moloch! Moloch the loveless! Mental Moloch! Moloch the heavy judger of men!</em><br>\n<em>Moloch the incomprehensible prison! Moloch the crossbone soulless jailhouse and Congress of sorrows! Moloch whose buildings are judgment! Moloch the vast stone of war! Moloch the stunned governments!</em><br>\n<em>Moloch whose mind is pure machinery! Moloch whose blood is running money! Moloch whose fingers are ten armies! Moloch whose breast is a cannibal dynamo! Moloch whose ear is a smoking tomb!</em><br>\n<em>Moloch whose eyes are a thousand blind windows! Moloch whose skyscrapers stand in the long streets like endless Jehovahs! Moloch whose factories dream and croak in the fog! Moloch whose smoke-stacks and antennae crown the cities!</em><br>\n<em>Moloch whose love is endless oil and stone! Moloch whose soul is electricity and banks! Moloch whose poverty is the specter of genius! Moloch whose fate is a cloud of sexless hydrogen! Moloch whose name is the Mind!</em></p>\n</blockquote>\n<p>See also: <a href=\"https://www.lesswrong.com/tag/eldritch-analogies\">Eldritch Analogies</a>, <a href=\"https://www.lesswrong.com/tag/game-theory\">Game Theory</a>, <a href=\"https://www.lesswrong.com/tag/group-rationality?showPostCount=true&amp;useTagName=true\">Group Rationality</a>, <a href=\"https://www.lesswrong.com/tag/social-reality\">Social Reality</a></p>\n",
    "description_length": 4277,
    "viewCount": 1176,
    "parentTagId": "world-optimization-working-with-humans"
  },
  {
    "core-tag": "World Optimization",
    "_id": "Q6P8jLn8hH7kbuXRr",
    "name": "Signaling",
    "slug": "signaling",
    "postCount": 84,
    "description_html": "<p><strong>Signaling</strong> is <a href=\"https://lesswrong.com/lw/did/what_is_signaling_really/\">defined</a> by <a href=\"https://wiki.lesswrong.com/wiki/Yvain\">Yvain</a> as \"a method of conveying information among not-necessarily-trustworthy parties by performing an action which is more likely or less costly if the information is true than if it is not true\". Some signaling is performed exclusively to impress others (to improve your <a href=\"https://lesswrong.com/tag/social-status\">status</a>), and in some cases <a href=\"http://www.overcomingbias.com/2007/01/excess_signalin.html\">isn't even worth that</a>. In other cases, signaling is a side-effect of an otherwise useful activity.</p>\n<p>For example, if doing something is easy for one type of person and hard for another type of person, you might do that thing just to get people to think you're the former type of person, even if the thing isn't in itself worth doing. This could explain many facets of human behavior, and reveal opportunities for reducing waste.</p>\n<p>Not all signaling is about abilities. Signaling can also be about personality, current emotional state, beliefs, loyalty to a particular group, status within a group, etc.</p>\n<p><strong>Countersignaling</strong> is signaling that a naive observer might take to mean that one is the <em>opposite</em> of X, when in fact, one is X, used as a means to signal that one is, in fact, X. For example, aristocrats (\"old money\") may forgo gaudy bling in order to signal that they are not <em>nouveau riche</em> (new money), which may lead some people to incorrectly assume that they are not rich.</p>\n<h2>Blog posts</h2>\n<p>by <a href=\"https://lesswrong.com/tag/robin-hanson\">Robin Hanson</a></p>\n<ul>\n<li><a href=\"http://www.overcomingbias.com/2006/12/do_helping_prof.html\">Do Helping Professions Help More?</a> and <a href=\"http://www.overcomingbias.com/2006/12/gifts_hurt.html\">Gifts Hurt</a></li>\n<li><a href=\"http://www.overcomingbias.com/2007/01/excess_signalin.html\">Excess Signaling Example</a></li>\n<li><a href=\"http://www.overcomingbias.com/2009/01/a-tale-of-two-tradeoffs.html\">A Tale Of Two Tradeoffs</a></li>\n<li><a href=\"http://www.overcomingbias.com/2009/06/why-signals-are-shallow.html\">Why Signals Are Shallow</a> - \"We all want to affiliate with high status people, but since status is about common distant perceptions of quality, we often care more about what distant observers would think about our associates than about how we privately evaluate them.\"</li>\n<li><a href=\"http://www.overcomingbias.com/2009/06/signals-are-forever.html\">Signals Are Forever</a></li>\n<li><a href=\"https://lesswrong.com/lw/g7/least_signaling_activities/\">Least Signaling Activities?</a></li>\n</ul>\n<p>by others</p>\n<ul>\n<li><a href=\"https://lesswrong.com/lw/did/what_is_signaling_really/\">What Is Signaling, Really?</a> by <a href=\"https://wiki.lesswrong.com/wiki/Yvain\">Yvain</a></li>\n<li><a href=\"https://lesswrong.com/lw/1y3/think_before_you_speak_and_signal_it/\">Think Before You Speak (And Signal It)</a> by <a href=\"http://weidai.com/\">Wei Dai</a></li>\n<li><a href=\"https://lesswrong.com/lw/b2/declare_your_signaling_and_hidden_agendas/\">Declare Your Signaling and Hidden Agendas</a> by <a href=\"https://wiki.lesswrong.com/wiki/Kaj_Sotala\">Kaj Sotala</a></li>\n<li><a href=\"https://lesswrong.com/lw/8ev/modularity_signaling_and_belief_in_belief/\">Modularity, Signaling, and Belief in Belief</a> by Kaj Sotala</li>\n</ul>\n<h2>See also</h2>\n<ul>\n<li><a href=\"https://lesswrong.com/tag/social-status\">Status</a></li>\n<li><a href=\"https://lesswrong.com/tag/near-far-thinking\">Near/far thinking</a></li>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Adaptation_executers\">Adaptation executers</a>, <a href=\"https://lesswrong.com/tag/superstimuli\">Superstimulus</a></li>\n<li><a href=\"https://lesswrong.com/tag/goodhart-s-law\">Goodhart's law</a></li>\n</ul>\n<h2>External links</h2>\n<ul>\n<li><a href=\"http://www.econtalk.org/archives/2008/05/hanson_on_signa.html\">Robin Hanson on Signaling (Econtalk Podcast)</a></li>\n</ul>\n",
    "description_length": 4043,
    "viewCount": 145,
    "parentTagId": "world-optimization-working-with-humans"
  },
  {
    "core-tag": "World Optimization",
    "_id": "5SPDtxJT6y6ZTXHBJ",
    "name": "Simulacrum Levels",
    "slug": "simulacrum-levels",
    "postCount": 41,
    "description_html": "<p><strong>Simulacrum Levels</strong> are a framework for analyzing different motivations people can have for making statements.</p><ul><li>Simulacrum Level 1: Attempt to describe the world accurately.</li><li>Simulacrum Level 2: Choose what to say based on what your statement will cause other people to do or believe.</li><li>Simulacrum Level 3: Say things that <a href=\"https://www.lesswrong.com/tag/signaling\">signal</a> membership to your ingroup.</li><li>Simulacrum Level 4: Choose which group to signal membership to based on what the benefit would be for you.</li></ul><p>One way to test which level someone is on is what would make them say the opposite of what they say now:</p><ul><li>Level 1: If they see enough evidence in the opposite direction.</li><li>Level 2: If people begin responding the opposite way to the same statement.</li><li>Level 3: If your group starts saying the opposite.</li><li>Level 4: If you benefit more from saying the opposite.</li></ul><h2>More descriptions of the four levels:</h2><p>By <a href=\"https://www.lesswrong.com/posts/fEX7G2N7CtmZQ3eB5/simulacra-and-subjectivity?commentId=FgajiMrSpY9MxTS8b\">Strawperson</a>:</p><ul><li>Level 1: “There’s a lion across the river.” = There’s a lion across the river.</li><li>Level 2: “There’s a lion across the river.” = I don’t want to go (or have other people go) across the river.</li><li>Level 3: “There’s a lion across the river.” = I’m with the popular kids who are too cool to go across the river.</li><li>Level 4: “There’s a lion across the river.” = A firm stance against trans-river expansionism focus grouped well with undecided voters in my constituency.</li></ul><p>By <a href=\"https://www.lesswrong.com/posts/QdppEcbhLTZqDDtDa/unifying-the-simulacra-definitions\">Zvi</a>:</p><ul><li>Level 1: Symbols describe reality.</li><li>Level 2: Symbols pretend to describe reality.</li><li>Level 3: Symbols pretend to pretend to describe reality.</li><li>Level 4: Symbols need not pretend to describe reality.</li></ul><p>A concrete example of the above from Michael Vassar:</p><ul><li>Level 1: A court reflects justice.</li><li>Level 2: A corrupt judge distorts justice.</li><li>Level 3: A Soviet show trial conceals the absence of real Soviet courts.</li><li>Level 4: A trial by ordeal or trial by combat lacks and denies the concept of justice entirely.</li></ul><p>Zvi <a href=\"https://www.lesswrong.com/posts/v6NBpQc2AzWjzxJ4S/the-four-children-of-the-seder-as-the-simulacra-levels\">describes</a> the four children of the Seder (Passover) as the four (and one extra) simulacrum levels:</p><ul><li>Level 1: The wise child.</li><li>Level 2: The wicked child.</li><li>Level 3: The simple child.</li><li>Level 4: The one who does not know how to ask.</li><li>Level 5: The one who is not there.</li></ul><blockquote><p>\"The wicked understand, acknowledge and value the Wise—they depend on the Wise for their own cynical gain. The simple don’t see the point of wisdom. Those who do not know how to ask don’t even know wisdom is a thing.\" —<a href=\"https://www.lesswrong.com/posts/v6NBpQc2AzWjzxJ4S/the-four-children-of-the-seder-as-the-simulacra-levels\">The Four Children of the Seder as the Simulacra Levels</a></p></blockquote><hr><p>The origin of this framework is in <a href=\"https://en.wikipedia.org/wiki/Simulacra_and_Simulation\">Simulacra and Simulation</a> by sociologist <a href=\"https://en.wikipedia.org/wiki/Jean_Baudrillard\">Jean Baudrillard</a>.</p>",
    "description_length": 3447,
    "viewCount": 371,
    "parentTagId": "world-optimization-working-with-humans"
  },
  {
    "core-tag": "World Optimization",
    "_id": "2EFq8dJbxKNzforjM",
    "name": "Social Status",
    "slug": "social-status",
    "postCount": 112,
    "description_html": "<p><strong>Social Status </strong>is an abstraction to model how people relate to each other, how social hierarchies are formed, and how people facilitate trade in the absence of financial accounting (as well as a variety of other stuff). I mean, everyone knows what status is, but here is where we break that down into its components and really try to understand what's happening on a mechanistic level.</p><h2>See Also</h2><ul><li><a href=\"https://lessestwrong.com/tag/signaling\">Signaling</a></li></ul><h2>Notable Posts</h2><ul><li><a href=\"https://lessestwrong.com/lw/13s/the_nature_of_offense/\">The Nature of Offense</a> by <a href=\"http://weidai.com/\">Wei Dai</a> - People are <a href=\"https://lessestwrong.com/tag/offense\">offended</a> by grabs for status.</li><li><a href=\"https://lessestwrong.com/lw/154/why_real_men_wear_pink/\">Why Real Men Wear Pink</a> by <a href=\"https://wiki.lesswrong.com/wiki/Yvain\">Yvain</a></li><li><a href=\"http://www.overcomingbias.com/2009/08/actors-see-status.html\">Actors See Status</a> by <a href=\"https://lessestwrong.com/tag/robin-hanson\">Robin Hanson</a>, quoting <a href=\"https://en.wikipedia.org/wiki/Keith_Johnstone\">Keith Johnstone</a></li><li><a href=\"https://lessestwrong.com/lw/1kr/that_other_kind_of_status/\">That Other Kind of Status</a> by Yvain</li></ul><h2>External</h2><ul><li>Melting Asphault (by Kevin Simler) has many great posts on status</li><li>Elephant in the Brain by Simler and Hanson</li><li>Impro (book on improv covering status relations)</li><li>Writings by Venkatesh Rao such as Gervais Principle and something, something Psychopath</li></ul>",
    "description_length": 1613,
    "viewCount": 198,
    "parentTagId": "world-optimization-working-with-humans"
  },
  {
    "core-tag": "World Optimization",
    "_id": "tgJoX7PGDDh2vJNqT",
    "name": "Acausal Trade",
    "slug": "acausal-trade",
    "postCount": 70,
    "description_html": "<p>In <strong>acausal trade</strong>, two agents each benefit by predicting what the other wants and doing it, even though they might have no way of communicating or affecting each other, nor even any direct evidence that the other exists.</p><h2>Background: Superrationality and the one-shot Prisoner's Dilemma</h2><p>This concept emerged out of the much-debated question of how to achieve cooperation on a one-shot <a href=\"https://www.lesswrong.com/tag/prisoner-s-dilemma\">Prisoner's Dilemma</a>, where, by design, the two players are not allowed to communicate. On the one hand, a player who is considering the causal consequences of a decision (\"<a href=\"https://www.lesswrong.com/tag/causal-decision-theory\">Causal Decision Theory</a>\") finds that defection always produces a better result. On the other hand, if the other player symmetrically reasons this way, the result is a Defect/Defect equilibrium, which is bad for both agents. If they could somehow converge on Cooperate, they would each individually do better. The question is what variation on decision theory would allow this beneficial equilibrium.</p><p>Douglas Hofstadter (see references) coined the term \"<a href=\"https://www.lesswrong.com/tag/superrationality\">superrationality</a>\" to express this state of convergence. He illustrated it with a game in which twenty players, who do not know each other's identities, each get an offer. If exactly one player asks for the prize of a billion dollars, they get it, but if none or multiple players ask, no one gets it. Players cannot communicate, but each might reason that the others are reasoning similarly. The \"correct\" decision--the decision which maximizes expected utility for each player, <i>if</i> all players symmetrically make the same decision--is to randomize a one-in-20 chance of asking for the prize.</p><p>Gary Drescher (see references) developed the concept further, introducing an ethical system called \"acausal subjunctive morality.\" Drescher's approach relies on the agents being identical or at least similar, so that each agent can reasonably guess what the other will do based on facts about its own behavior, or even its own \"source code.\" If it cooperates, it can use this correlation to infer that the other will probably also cooperate.</p><p>Acausal trade goes one step beyond this. The agents do not need to be identical, nor similar, nor have the same utility function. Moreover, they do not need to know what the other agents are like, nor even if they exist. In acausal trade, an agent may have to surmise the probability that other agents, with their utility function and proclivities, exist.</p><h2>Description</h2><p>We have two agents, separated so that no interaction is possible. The separation can be simply because each is not aware of the location of the other; or else each may be prevented from communicating with or affecting the other.</p><p>In an asymmetrical example, one agent may be in the other's future.</p><p>Other less prosaic thought experiments can be used to emphasize that interaction may be absolutely impossible. For example, agents that are outside each other's light cones, or in separate parts of an Everett multiverse. And abstracting away from those scenarios, we can talk of counterfactual \"impossible possible worlds\" as a model for probability distributions.</p><p>In truly <i>acausal</i> trade, the agents cannot count on reputation, retaliation, or outside enforcement to ensure cooperation. The agents cooperate because each knows that the other can somehow predict its behavior very well. (Compare Omega in <a href=\"https://www.lesswrong.com/tag/newcomb-s-problem\">Newcomb's problem</a>.) Each knows that if it defects or cooperates, the other will (probabilistically) know this, and defect or cooperate, respectively.</p><p>Acausal trade can also be described in terms of <a href=\"https://www.lesswrong.com/tag/pre-commitment\">(pre)commitment</a>: Both agents commit to cooperate, and each has reason to think that the other is also committing.</p><h2>Prediction mechanisms</h2><p>For acausal trade to occur, each agent must infer there is some probability that an agent, of the sort that will acausally trade with it, exists.</p><p>The agent might be told, exogenously (as part of the scenario), that the other exists. But more interesting is the case in which the agent surmises the probability that the other exists.</p><p>A <a href=\"https://www.lesswrong.com/tag/superintelligence\">superintelligence </a>might conclude that other superintelligences would tend to exist because increased intelligence <a href=\"https://www.lesswrong.com/tag/instrumental-convergence\">is a convergent instrumental goal</a> for agents. Given the existence of a superintelligence, acausal trade is one of the tricks it would tend to use.</p><p>To take a more prosaic example, we humans realize that humans tend to be alike: Even without knowing about specific trading partners, we know that there exist other people with similar situations, goals, desires, challenges, resource constraints, and mental architectures.</p><p>Once an agent realizes that another agent might exist, there are different ways that might predict the other agent's behavior, and specifically that the other agent can be an acausal trading partner.</p><ol><li>They might know or surmise each other's mental architectures (source code).</li><li>In particular, they might know that they have identical or similar mental architecture, so that each one knows that its own mental processes approximately simulate the other's.</li><li>They might be able to simulate each other (perhaps probabalistically), or to predict the other's behavior analytically. (Even we humans simulate each other's thoughts to guess what the other would do.)</li><li>More broadly, it is enough to know (probabilistically) that the other is a powerful optimizer, that it has a certain utility function, and that it can derive utility from resources. Seen mathematically, this is just an optimization problem: What is the best possible algorithm for an agent's utility function? Cooperate/Cooperate is optimal under certain assumptions, for if one agent could achieve optimal utility by defecting, then, symmetrically, so could the other, resulting in Defect/Defect which generates inferior utility.</li></ol><h2>Decision Theories</h2><p>Acausal trade is a special case of <a href=\"https://www.lesswrong.com/tag/updateless-decision-theory\">Updateless decision theory</a> (or a variant like <a href=\"https://www.lesswrong.com/tag/functional-decision-theory\">Functional Decision Theory,</a> see references). Unlike better-known variations of <a href=\"https://www.lesswrong.com/tag/decision-theory\">Decision theory</a>, such as <a href=\"https://www.lesswrong.com/tag/causal-decision-theory\">Causal decision theory</a>, acausal trade and UDT take into account the agent's own algorithm as cause and caused.</p><p>In Causal Decision Theory, the agent's algorithm (implementation) is treated as uncaused by the rest of the universe, so that though the agent's <i>decision</i> and subsequent action can make a difference, its internal make-up cannot (except through that decision). In contrast, in UDT, the agents' own algorithms are treated as causal nodes, influenced by other factors, such as the logical requirement of optimality in a utility-function maximizer. In UDT, as in acausal trade, the agent cannot escape the fact that its decision to defect or cooperate constitutes strong Bayesian evidence as to what the other agent will do, and so it is better off cooperating.</p><h2>Limitations and Objections</h2><p>Acausal trade only works if the agents are smart enough to predict each other's behavior, and then smart enough to acausally trade. If one agent is stupid enough to defect, and the second is smart enough to predict the first, then neither will cooperate.</p><p>Also, as in regular trade, acausal trade only works if the two sides are close enough in power that the weaker side can do something worthwhile enough for the stronger.</p><p>A common objection to this idea: Why shouldn't an agent \"cheat\" and choose to defect? Can't it \"at the last moment\" back out after the other agent has committed? However, this approach takes into account only the direct effect of the decision, while a sufficiently intelligent trading partner could predict the agent's choice, including that one, rendering the \"cheating\" approach suboptimal.</p><p>Another objection: Can an agent care about (have a utility function that takes into account) entities with which it can never interact, and about whose existence it is not certain? However, this is quite common even for humans today. We care about the suffering of other people in faraway lands about whom we know next to nothing. We are even disturbed by the suffering of long-dead historical people, and wish that, counterfactually, the suffering had not happened. We even care about entities that we are not sure exist. For example: &nbsp;We might be concerned by news report that a valuable archaeological artifact was destroyed in a distant country, yet at the same time read other news reports stating that the entire story is a fabrication and the artifact never existed. People even get emotionally attached to the fate of a fictional character.</p><h2>An example of acausal trade with simple resource requirements</h2><p>At its most abstract, the agents are simply optimization algorithms. As a toy example, let T be a utility function for which time is most valuable as a resource; while for utility function S, space is most valuable, and assume that these are the only two resources.</p><p>We will now choose the best algorithms for optimizing T. To avoid anthropomorphizing, we simply ask which algorithm--which string of LISP, for example--would give the highest expected utility for a given utility function. Thus, the choice of source code is \"timeless\": We treat it as an optimization problem across all possible strings of LISP. We assume that computing power is unlimited. Mathematically, we are asking about argmax T.</p><p>We specify that there is a probability that either agent will be run in an environment where time is in abundance, and if not, some probability that it will be run in a space-rich universe.</p><p>If the algorithm for T is instantiated in a space-rich environment, it will only be able to gain a small amount of utility for itself, but S would be able to gain a lot of utility; and vice versa.</p><p>The question is: What algorithm for T provides the most optimization power, the highest expected value of utility function T?</p><p>If it turns out that the environment is space-rich, the agent for T may run the agent (the algorithm) for S, increasing the utility for S, and symmetrically the reverse. This will happen if each concludes, that the optimum occurs when the other agent has the \"trading\" feature. Given that this is the optimal case, the acausal trade will occur.</p><h2>Acausal trade with complex resource requirements</h2><p>In the toy example above, resource requirements are very simple. In general, given that agents can have complex and arbitrary goals requiring a complex mix of resources, an agent might not be able to conclude that a specific trading partner has a meaningful chance of existing and trading.</p><p>However, an agent can analyze the distribution of probabilities for the existence of other agents, and weight its actions accordingly. It will do acausal \"favors\" for one or more trading partners, weighting its effort according to its subjective probability that the trading partner exists. The expectation on utility given and received will come into a good enough balance to benefit the traders, in the limiting case of increasing super-intelligence.</p><h2>Ordinary trade</h2><p>Even ordinary trade can be analyzed acausally, using a perspective similar to that of <a href=\"https://www.lesswrong.com/tag/updateless-decision-theory\">Updateless decision theory</a>. We ask: Which algorithm should an agent have to get the best expected value, summing across all possible environments weighted by their probability? The possible environments include those in which threats and promises have been made.</p><h2>See also</h2><ul><li><a href=\"http://aibeliefs.blogspot.com/2007/11/non-technical-introduction-to-ai.html?a=1\">\"AI deterrence\"</a></li><li><a href=\"https://www.lesswrong.com/lw/1pz/the_ai_in_a_box_boxes_you\">\"The AI in a box boxes you\"</a></li><li><a href=\"https://slatestarcodex.com/2017/03/21/repost-the-demiurges-older-brother/\">A story</a> that shows acausal trade in action.</li><li><a href=\"http://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/\">Scott Alexander</a> explains Acausal Trade. (Most of that article is tongue-in-cheek, however.)</li><li>\"<a href=\"http://www.nickbostrom.com/papers/porosity.pdf\">Hail Mary, Value Porosity, and Utility Diversification</a>,\" Nick Bostrom, the first paper from academia to rely on the concept of acausal trade.</li><li><a href=\"http://intelligence.org/files/TowardIdealizedDecisionTheory.pdf\">Towards an idealized decision theory</a>, by Nate Soares and Benja Fallenstein discusses acausal interaction scenarios that shed light on new directions in decision theory.</li><li><a href=\"https://ie.technion.ac.il/~moshet/progeqnote4.pdf\">Program Equilibrium</a>, by Moshe Tennenholtz. In: Games and Economic Behavior.</li><li><a href=\"https://arxiv.org/abs/1401.5577\">Robust Cooperation in the Prisoner's Dilemma: Program Equilibrium via Provability Logic</a>, by Mihaly Barasz, Paul Christiano, Benja Fallenstein, Marcello Herreshoff, Patrick LaVictoire and Eliezer Yudkowsky</li><li><a href=\"https://arxiv.org/abs/1602.04184\">Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents</a>, by Andrew Critch</li><li><a href=\"https://link.springer.com/article/10.1007/s11238-018-9679-3\">Robust Program Equilibrium</a>, by Caspar Oesterheld. In: Theory and Decision.</li><li><a href=\"https://foundational-research.org/multiverse-wide-cooperation-via-correlated-decision-making/\">Multiverse-wide Cooperation via Correlated Decision Making</a>, by Caspar Oesterheld</li></ul><h2>References</h2><ul><li><a href=\"http://www.gwern.net/docs/1985-hofstadter\">Hofstadter's Superrationality essays, published in <i>Metamagical Themas</i></a> (<a href=\"https://www.lesswrong.com/lw/bxi/hofstadters_superrationality/\">LW discussion</a>)</li><li>Jaan Tallinn, <a href=\"https://www.youtube.com/watch?v=29AgSo6KOtI\">Why Now? A Quest in Metaphysics</a>.</li><li><a href=\"https://wiki.lesswrong.com/wiki/Gary_Drescher\">Gary Drescher</a>, <i>Good and Real</i>, MIT Press, 1996.</li><li><a href=\"https://arxiv.org/abs/1710.05060\">Functional Decision Theory</a></li></ul>",
    "description_length": 14775,
    "viewCount": 1019,
    "parentTagId": "world-optimization-applied-topics"
  },
  {
    "core-tag": "World Optimization",
    "_id": "CYMR6p5iZG75QAT8a",
    "name": "Censorship",
    "slug": "censorship",
    "postCount": 31,
    "description_html": "<p><strong>Censorship</strong> is the suppression of speech, public communication, or other information, on the basis that such material is considered objectionable, harmful, sensitive, or \"inconvenient.\" Censorship can be conducted by governments, private institutions, and other controlling bodies. <a href=\"https://en.wikipedia.org/wiki/Censorship\">(From Wikipedia)</a></p>",
    "description_length": 376,
    "viewCount": 38,
    "parentTagId": "world-optimization-applied-topics"
  },
  {
    "core-tag": "World Optimization",
    "_id": "ntahi2tr7e9DjCYdu",
    "name": "Chesterton's Fence",
    "slug": "chesterton-s-fence",
    "postCount": 15,
    "description_html": null,
    "description_length": null,
    "viewCount": 207,
    "parentTagId": "world-optimization-applied-topics"
  },
  {
    "core-tag": "World Optimization",
    "_id": "E9ihK6bA9YKkmJs2f",
    "name": "Death",
    "slug": "death",
    "postCount": 85,
    "description_html": "<blockquote><p>Even if the stars should die in heaven<br>Our sins can never be undone<br>No single death will be forgiven<br>When fades at last the last lit sun.<br>Then in the cold and silent black<br>As light and matter end<br>We’ll have ourselves a last look back<br>And toast an absent friend.</p><p>– Song of Dath Ilan, Eliezer Yudkowsky</p></blockquote><p>First you're there, and then you're not there, and they can't change you from being not there to being there, because there's nothing there to be changed from being not there to being there. That's <strong>death</strong>. This tag includes posts about the deaths of particular people, or about death in general.</p><p><a href=\"https://www.lesswrong.com/tag/cryonics\">Cryonicists</a> use the concept of <a href=\"https://en.wikipedia.org/wiki/information-theoretic_death\">information-theoretic death</a>, which is what happens when the information needed to reconstruct you even in principle is no longer present. Anything less, to them, is just a flesh wound.</p><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/cryonics\">Cryonics</a></li><li><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\">Shut up and multiply</a></li><li><a href=\"https://www.lesswrong.com/tag/transhumanism\">Transhumanism</a></li><li><a href=\"https://www.lesswrong.com/tag/personal-identity\">Personal identity</a></li></ul>",
    "description_length": 1382,
    "viewCount": 220,
    "parentTagId": "world-optimization-applied-topics"
  },
  {
    "core-tag": "World Optimization",
    "_id": "cHoCqtfE9cF7aSs9d",
    "name": "Deception",
    "slug": "deception",
    "postCount": 122,
    "description_html": "<p><strong>Deception</strong> is the act of sharing information in a way which intentionally misleads others.</p><p><strong>Related Pages:</strong> <a href=\"https://www.lesswrong.com/tag/deceptive-alignment\">Deceptive Alignment,</a> <a href=\"https://www.lesswrong.com/tag/honesty\">Honesty</a>, <a href=\"https://www.lesswrong.com/tag/meta-honesty\">Meta-Honesty</a>, <a href=\"https://www.lesswrong.com/tag/self-deception\">Self-Deception</a>, <a href=\"https://www.lesswrong.com/tag/simulacrum-levels\">Simulacrum Levels</a></p>",
    "description_length": 523,
    "viewCount": 87,
    "parentTagId": "world-optimization-applied-topics"
  },
  {
    "core-tag": "World Optimization",
    "_id": "nANxo5C4sPG9HQHzr",
    "name": "Honesty",
    "slug": "honesty",
    "postCount": 72,
    "description_html": "<p><strong>Honesty</strong> means telling the truth and not being <a href=\"http://lesswrong.com/tag/deception\">deceptive</a>.</p><p><strong>External Links:</strong><br><a href=\"https://slatestarcodex.com/2019/07/16/against-lie-inflation/\">Against Lie Inflation</a> by Scott Alexander</p><p><strong>Related Pages:</strong> <a href=\"http://lesswrong.com/tag/meta-honesty\">Meta-Honesty</a>,<a href=\"http://lesswrong.com/tag/deception\"> Deception</a>.</p>",
    "description_length": 451,
    "viewCount": 56,
    "parentTagId": "world-optimization-applied-topics"
  },
  {
    "core-tag": "World Optimization",
    "_id": "cpBfacd22cJsm5fuL",
    "name": "Hypocrisy",
    "slug": "hypocrisy",
    "postCount": 17,
    "description_html": null,
    "description_length": null,
    "viewCount": 27,
    "parentTagId": "world-optimization-applied-topics"
  },
  {
    "core-tag": "World Optimization",
    "_id": "7w6XkYe5YPx9YL59j",
    "name": "Information Hazards",
    "slug": "information-hazards",
    "postCount": 73,
    "description_html": "<p>An <strong>Information Hazard</strong> (or <strong>infohazard </strong>for short) is some true information that could harm people, or other sentient beings, if known. It is tricky to determine policies on information hazards. Some information might genuinely be dangerous, but excessive controls on information has its own perils.&nbsp;</p><p><strong>This tag is for discussing the phenomenon of Information Hazards and what to do with them. Not for actual Information Hazards themselves.</strong><br><br>An example might be a formula for easily creating cold fusion in your garage, which would be very dangerous. Alternatively, it might be an idea that causes great mental harm to people.</p><h2>Bostrom's Typology of Information Hazards</h2><p>Nick Bostrom coined the term <i>information hazard </i>in a 2011 paper [1] for Review of Contemporary Philosophy. He defines it as follows:</p><blockquote><p><span>Information hazard: A risk that arises from the dissemination or the potential dissemination of (true) information that may cause harm or enable some agent to cause harm.</span></p></blockquote><p>Bostrom points out that this is in contrast to the generally accepted principle of information freedom and that, while rare, the possibility of information hazards needs to be considered when making information policies. He proceeds to categorize and define a large number of sub-types of information hazards. For example, he defines artificial intelligence hazard as:</p><blockquote><p><span>Artificial intelligence hazard: There could be computer-related risks in which the threat would derive primarily from the cognitive sophistication of the program rather than the specific properties of any actuators to which the system initially has access.</span></p></blockquote><p>The following table is reproduced from Bostrom 2011 [1].</p><figure class=\"table\"><table style=\"background-color:white\"><tbody><tr><td style=\"text-align:center\" colspan=\"3\"><strong>TYPOLOGY OF INFORMATION HAZARDS</strong></td></tr><tr><td colspan=\"3\">I. By information transfer mode</td></tr><tr><td rowspan=\"6\">&nbsp;</td><td>Data hazard</td><td rowspan=\"6\">&nbsp;</td></tr><tr><td>Idea hazard</td></tr><tr><td>Attention hazard</td></tr><tr><td>Template hazard</td></tr><tr><td>Signaling hazard</td></tr><tr><td>Evocation hazard</td></tr><tr><td colspan=\"3\">II. By effect</td></tr><tr><td>&nbsp;</td><td>TYPE</td><td>SUBTYPE</td></tr><tr><td rowspan=\"4\">ADVERSARIAL RISKS</td><td rowspan=\"4\">Competiveness hazard</td><td>Enemy Hazard</td></tr><tr><td>Intellectual property hazard</td></tr><tr><td>Commitment hazard</td></tr><tr><td>Knowing-too-much hazard</td></tr><tr><td rowspan=\"3\">RISKS TO SOCIAL ORGANIZATION AND MARKETS</td><td rowspan=\"3\">Norm hazard</td><td>Information asymmetry Hazard</td></tr><tr><td>Unveiling hazard</td></tr><tr><td>Recognition hazard</td></tr><tr><td rowspan=\"7\">RISKS OF IRRATIONALITY AND ERROR</td><td>Ideological hazard</td><td rowspan=\"7\">&nbsp;</td></tr><tr><td>Distraction and temptation hazard</td></tr><tr><td>Role model hazard</td></tr><tr><td>Biasing hazard</td></tr><tr><td>De-biasing hazard</td></tr><tr><td>Neuropsychological hazard</td></tr><tr><td>Information-burying hazard</td></tr><tr><td rowspan=\"5\">RISKS TO VALUABLE STATES AND ACTIVITIES</td><td rowspan=\"3\">Psychological reaction hazard</td><td>Disappointment hazard</td></tr><tr><td>Spoiler hazard</td></tr><tr><td>Mindset hazard</td></tr><tr><td>Belief-constituted value hazard</td><td>&nbsp;</td></tr><tr><td>(mixed)</td><td>Embarrassment hazard</td></tr><tr><td rowspan=\"3\">RISKS FROM INFORMATION TECHNOLOGY SYSTEMS</td><td rowspan=\"3\">Information system hazard</td><td>Information infrastructure failure hazard</td></tr><tr><td>Information infrastructure misuse hazard</td></tr><tr><td>Artificial intelligence hazard</td></tr><tr><td>RISKS FROM DEVELOPMENT</td><td>Development hazard</td><td>&nbsp;</td></tr></tbody></table></figure><h2>See Also</h2><ul><li><a href=\"https://lessestwrong.com/tag/dangerous-knowledge\">Dangerous Knowledge</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Computation_Hazards\">Computation Hazards</a></li></ul><h2>References</h2><ol><li>Bostrom, N. (2011). \"<a href=\"http://www.nickbostrom.com/information-hazards.pdf\"><u>Information Hazards: A Typology of Potential Harms from Knowledge</u></a>\". <i>Review of Contemporary Philosophy</i> <strong>10</strong>: 44-79.</li></ol>",
    "description_length": 4405,
    "viewCount": 1882,
    "parentTagId": "world-optimization-applied-topics"
  },
  {
    "core-tag": "World Optimization",
    "_id": "cRaweRcZcXnb9Qryt",
    "name": "Meta-Honesty",
    "slug": "meta-honesty",
    "postCount": 16,
    "description_html": "<html><head></head><body><p><strong>Meta-Honesty </strong>is the attempt to be honest about in which situations one will not be honest. It derives from the recognition that an object-level commitment to never lie under any possible circumstance is untenable. A meta-honest person might say something like \"I will lie in circumstances similar to an axe-wielding murderer coming to my door and enquiring after the location of my friend.\"</p></body></html>",
    "description_length": 453,
    "viewCount": 89,
    "parentTagId": "world-optimization-applied-topics"
  },
  {
    "core-tag": "World Optimization",
    "_id": "HNJiR8Jzafsv8cHrC",
    "name": "Pascal's Mugging",
    "slug": "pascal-s-mugging",
    "postCount": 48,
    "description_html": null,
    "description_length": null,
    "viewCount": 183,
    "parentTagId": "world-optimization-applied-topics"
  },
  {
    "core-tag": "World Optimization",
    "_id": "xXX3n22DQZuKqXEdT",
    "name": "War",
    "slug": "war",
    "postCount": 101,
    "description_html": null,
    "description_length": null,
    "viewCount": 64,
    "parentTagId": "world-optimization-applied-topics"
  },
  {
    "core-tag": "World Optimization",
    "_id": "hrezrpGqXXdSe76ks",
    "name": "Ambition",
    "slug": "ambition",
    "postCount": 45,
    "description_html": null,
    "description_length": null,
    "viewCount": 118,
    "parentTagId": "world-optimization-value-virtue"
  },
  {
    "core-tag": "World Optimization",
    "_id": "KDpqtN3MxHSmD4vcB",
    "name": "Art",
    "slug": "art",
    "postCount": 118,
    "description_html": "<p><strong>Art</strong> is material created for aesthetic appreciation, including visual art, comics, and music. This tag includes both posts sharing works of art and posts discussing art conceptually.</p><p>Some types of art have their own tag category: <a href=\"http://lesswrong.com/tag/poetry\">Poetry</a>, <a href=\"http://lesswrong.com/tag/fiction\">Fiction</a>, <a href=\"https://www.lesswrong.com/tag/fiction-topic\">Fiction(topic)</a>, <a href=\"https://www.lesswrong.com/tag/gaming-videogames-tabletop\">Games</a></p><p><strong>Related Sequences:</strong> <a href=\"https://www.lesswrong.com/s/WPgA9x5ZvKu9oYvgB\">Drawing Less Wrong</a></p>",
    "description_length": 640,
    "viewCount": 200,
    "parentTagId": "world-optimization-value-virtue"
  },
  {
    "core-tag": "World Optimization",
    "_id": "36RkM85iDocrnaypb",
    "name": "Aesthetics",
    "slug": "aesthetics",
    "postCount": 38,
    "description_html": "<html><head></head><body><p><strong>Aesthetics</strong>:<strong>&nbsp;</strong>\"Imagine if we could talk about why things seem beautiful and appealing, or ugly and unappealing.&nbsp; Where do these preferences come from, in a causal sense? Do we still endorse them when we know their origins?&nbsp; What happens when we bring tacit things into consciousness, when we talk carefully about what aesthetics evoke in us, and how that might be the same or different from person to person?\"&nbsp; – <a href=\"/posts/4ZwGqkMTyAvANYEDw/naming-the-nameless\">Naming the Nameless, Sarah Constantin</a></p></body></html>",
    "description_length": 607,
    "viewCount": 108,
    "parentTagId": "world-optimization-value-virtue"
  },
  {
    "core-tag": "World Optimization",
    "_id": "R6uagTfhhBeejGrrf",
    "name": "Complexity of Value",
    "slug": "complexity-of-value",
    "postCount": 91,
    "description_html": "<p><strong>Complexity of value</strong> is the thesis that human values have high <a href=\"https://wiki.lesswrong.com/wiki/Kolmogorov_complexity\">Kolmogorov complexity</a>; that our <a href=\"https://wiki.lesswrong.com/wiki/preferences\">preferences</a>, the things we care about, cannot be summed by a few simple rules, or compressed. <strong><a href=\"https://www.lesswrong.com/lw/y3/value_is_fragile/\">Fragility of value</a></strong> is the thesis that losing even a small part of the rules that make up our values could lead to results that most of us would now consider as unacceptable (just like dialing nine out of ten phone digits correctly does not connect you to a person 90% similar to your friend). For example, all of our values <em>except</em> novelty might yield a future full of individuals replaying only one optimal experience through all eternity.</p><p>Related: <a href=\"https://www.lesswrong.com/tag/metaethics\">Ethics &amp; Metaethics</a>, <a href=\"https://www.lesswrong.com/tag/fun-theory\">Fun Theory</a>, <a href=\"https://www.lesswrong.com/tag/preference\">Preference</a>, <a href=\"https://www.lesswrong.com/tag/wireheading\">Wireheading</a></p><p>Many human choices can be compressed, by representing them by simple rules - the desire to survive produces innumerable actions and subgoals as we fulfill that desire. But people don't <em>just</em> want to survive - although you can compress many human activities to that desire, you cannot compress all of human existence into it. The human equivalents of a utility function, our terminal values, contain many different elements that are not strictly reducible to one another. William Frankena offered <a href=\"http://plato.stanford.edu/entries/value-intrinsic-extrinsic/#WhaHasIntVal\">this list</a> of things which many cultures and people seem to value (for their own sake rather than strictly for their external consequences):</p><blockquote>Life, consciousness, and activity; health and strength; pleasures and satisfactions of all or certain kinds; happiness, beatitude, contentment, etc.; truth; knowledge and true opinions of various kinds, understanding, wisdom; beauty, harmony, proportion in objects contemplated; aesthetic experience; morally good dispositions or virtues; mutual affection, love, friendship, cooperation; just distribution of goods and evils; harmony and proportion in one's own life; power and experiences of achievement; self-expression; freedom; peace, security; adventure and novelty; and good reputation, honor, esteem, etc.</blockquote><p>The \"etc.\" at the end is the tricky part, because there may be a great many values not included on this list.</p><p>One hypothesis is that natural selection reifies selection pressures as <a href=\"https://www.lesswrong.com/tag/adaptation-executors\">psychological drives, which then continue to execute</a> <a href=\"https://www.lesswrong.com/lw/yi/the_evolutionarycognitive_boundary/\">independently of any consequentialist reasoning in the organism</a>. This may also continue without that organism explicitly representing, let alone caring about, the original evolutionary context. Under this view, we have no reason to expect these terminal values to be reducible to any one thing, or each other.</p><p>Taken in conjunction with another LessWrong claim, that all values are morally relevant, this would suggest that those philosophers who seek to do so are mistaken in trying to find cognitively tractable overarching principles of ethics. However, it is coherent to suppose that not all values are morally relevant, and that the morally relevant ones form a tractable subset.</p><p>Complexity of value also runs into underappreciation in the presence of bad <a href=\"https://www.lesswrong.com/tag/metaethics\">metaethics</a>. The local flavor of metaethics could be characterized as cognitivist, without implying \"thick\" notions of instrumental rationality; in other words, moral discourse can be about a coherent subject matter, without all possible minds and agents necessarily finding truths about that subject matter to be psychologically compelling. An <a href=\"https://www.lesswrong.com/tag/paperclip-maximizer\">expected paperclip maximizer</a> doesn't disagree with you about morality any more than you disagree with it about \"which action leads to the greatest number of expected paperclips\", it is just constructed to find the latter subject matter psychologically compelling but not the former. Failure to appreciate that \"But it's just paperclips! What a dumb goal! No sufficiently intelligent agent would pick such a dumb goal!\" is a judgment carried out on a local brain that evaluates paperclips as inherently low-in-the-preference-ordering means that someone will expect all moral judgments to be automatically reproduced in a sufficiently intelligent agent, since, after all, they would not lack the intelligence to see that paperclips are so obviously inherently-low-in-the-preference-ordering. This is a particularly subtle species of <a href=\"https://www.lesswrong.com/tag/anthropomorphism\">anthropomorphism</a> and <a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy\">mind projection fallacy</a>.</p><p>Because the human brain very often fails to grasp all these difficulties involving our values, we tend to think building an awesome future is much less problematic than it really is. Fragility of value is relevant for building <a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a>, because an <a href=\"https://wiki.lesswrong.com/wiki/AGI\">AGI</a> which does not respect human values is likely to create a world that we would consider devoid of value - not necessarily full of explicit attempts to be evil, but perhaps just a dull, boring loss.</p><p>As values are orthogonal with intelligence, they can freely vary no matter how intelligent and efficient an AGI is [<a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">1</a>]. Since human / humane values have high Kolmogorov complexity, a random AGI is highly unlikely to maximize human / humane values. The fragility of value thesis implies that a poorly constructed AGI might e.g. turn us into blobs of perpetual orgasm. Because of this relevance the complexity and fragility of value is a major theme of <a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky</a>'s writings.</p><p>Wrongly designing the future because we wrongly encoded human values is a serious and difficult to assess type of <a href=\"https://www.lesswrong.com/tag/existential-risk\">Existential risk</a>. \"Touch too hard in the wrong dimension, and the physical representation of those values will shatter - <em>and not come back, for there will be nothing left to want to bring it back</em>. And the referent of those values - a worthwhile universe - would no longer have any physical reason to come into being. Let go of the steering wheel, and the Future crashes.\" [<a href=\"https://www.lesswrong.com/lw/y3/value_is_fragile/\">2</a>]</p><h2>Complexity of Value and AI</h2><p>Complexity of value poses a problem for <a href=\"http://lesswrong.com/tag/ai\">AI alignment</a>. If you can't easily compress what humans want into a simple function that can be fed into a computer, it isn't easy to make a powerful AI that does things humans want and doesn't do things humans don't want. <a href=\"https://www.lesswrong.com/tag/value-learning\">Value Learning</a> attempts to address this problem.</p><h2>Major posts</h2><ul><li><a href=\"https://www.lesswrong.com/lw/xy/the_fun_theory_sequence/\">The Fun Theory Sequence</a> describes some of the many complex considerations that determine <em>what sort of happiness</em> we most prefer to have - given that many of us would decline to just have an electrode planted in our pleasure centers.</li><li><a href=\"https://www.lesswrong.com/lw/l3/thou_art_godshatter/\">Thou Art Godshatter</a> describes the <a href=\"https://www.lesswrong.com/tag/evolutionary-psychology\">evolutionary psychology</a> behind the complexity of human values - how they got to be complex, and why, given that origin, there is no reason in hindsight to expect them to be simple. We certainly are not built to <a href=\"https://wiki.lesswrong.com/wiki/adaptation_executers\">maximize genetic fitness</a>.</li><li><a href=\"https://www.lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/\">Not for the Sake of Happiness (Alone)</a> tackles the <a href=\"https://www.lesswrong.com/tag/hollywood-rationality\">Hollywood Rationality</a> trope that \"rational\" preferences must reduce to selfish hedonism - caring strictly about personally experienced pleasure. An ideal Bayesian agent - implementing strict Bayesian decision theory - can have a utility function that <a href=\"https://www.lesswrong.com/lw/l4/terminal_values_and_instrumental_values/\">ranges over anything, not just internal subjective experiences</a>.</li><li><a href=\"https://www.lesswrong.com/lw/lq/fake_utility_functions/\">Fake Utility Functions</a> describes the seeming fascination that many have with trying to compress morality down to a single principle. The <a href=\"https://www.lesswrong.com/lw/lp/fake_fake_utility_functions/\">sequence leading up</a> to this post tries to explain the cognitive twists whereby people <a href=\"https://www.lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/\">smuggle</a> all of their complicated <em>other</em> preferences into their choice of <em>exactly</em> which acts they try to <em><a href=\"https://www.lesswrong.com/lw/kq/fake_justification/\">justify using</a></em> their single principle; but if they were <em>really</em> following <em>only</em> that single principle, they would <a href=\"https://www.lesswrong.com/lw/kz/fake_optimization_criteria/\">choose other acts to justify</a>.</li></ul><h2>Other posts</h2><ul><li><a href=\"https://www.lesswrong.com/lw/y3/value_is_fragile/\">Value is Fragile</a></li><li><a href=\"https://www.lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/\">The Hidden Complexity of Wishes</a></li><li><a href=\"https://www.lesswrong.com/lw/ky/fake_morality/\">Fake Morality</a></li><li><a href=\"https://www.lesswrong.com/lw/1o9/welcome_to_heaven/\">Welcome to Heaven</a></li><li><a href=\"https://www.lesswrong.com/lw/1oj/complexity_of_value_complexity_of_outcome/\">Complexity of Value ≠ Complexity of Outcome</a></li><li><a href=\"https://www.lesswrong.com/lw/65w/not_for_the_sake_of_pleasure_alone/\">Not for the Sake of Pleasure Alone</a></li><li><a href=\"https://casparoesterheld.com/2017/02/10/a-non-comprehensive-list-of-human-values/\">A Non-Comprehensive List of Human Values</a></li></ul><h2>See also</h2><ul><li><a href=\"http://intelligence.org/files/ComplexValues.pdf\">Complex Value Systems are Required to Realize Valuable Futures</a></li><li><a href=\"https://www.lesswrong.com/tag/human-universal\">Human universal</a></li><li><a href=\"https://www.lesswrong.com/tag/fake-simplicity\">Fake simplicity</a></li><li><a href=\"https://www.lesswrong.com/tag/metaethics-sequence\">Metaethics sequence</a></li><li><a href=\"https://www.lesswrong.com/tag/fun-theory\">Fun theory</a></li><li><a href=\"https://www.lesswrong.com/tag/magical-categories\">Magical categories</a></li><li><a href=\"https://www.lesswrong.com/tag/friendly-artificial-intelligence\">Friendly Artificial Intelligence</a></li><li><a href=\"https://www.lesswrong.com/tag/preference\">Preference</a></li><li><a href=\"https://www.lesswrong.com/tag/wireheading\">Wireheading</a></li><li><a href=\"https://www.lesswrong.com/tag/the-utility-function-is-not-up-for-grabs\">The utility function is not up for grabs</a></li></ul>",
    "description_length": 11571,
    "viewCount": 347,
    "parentTagId": "world-optimization-value-virtue"
  },
  {
    "core-tag": "World Optimization",
    "_id": "ymWzfKxBchRvmCTNX",
    "name": "Courage",
    "slug": "courage",
    "postCount": 14,
    "description_html": "<p><strong>Courage</strong> is important to being rational. You will not be very rational if you are too scared to act differently to the people around you, or too scared to even think about very big ideas—these are the two main threads touched on in this tag.</p><p><em>Related Tags: <a href=\"https://www.lesswrong.com/tag/fallacies\">Fallacies</a>, <a href=\"https://www.lesswrong.com/tag/groupthink?showPostCount=true&amp;useTagName=true\">Groupthink</a>, <a href=\"https://www.lesswrong.com/tag/heroic-responsibility?showPostCount=true&amp;useTagName=true\">Heroic Responsibility</a>, <a href=\"https://www.lesswrong.com/tag/heuristics-and-biases?showPostCount=true&amp;useTagName=true\">Heuristics and Biases</a>, <a href=\"https://www.lesswrong.com/tag/motivated-reasoning?showPostCount=true&amp;useTagName=true\">Motivated Reasoning</a>, <a href=\"https://www.lesswrong.com/tag/rationalization?showPostCount=true&amp;useTagName=true\">Rationalisation</a>, <a href=\"https://www.lesswrong.com/tag/self-deception?showPostCount=true&amp;useTagName=true\">Self-Deception</a></em></p><p>More thorough explanations of the importance of courage can be found <a href=\"https://www.lesswrong.com/posts/WHK94zXkQm7qm7wXk/asch-s-conformity-experiment\">here</a> and <a href=\"https://www.lesswrong.com/posts/ovvwAhKKoNbfcMz8K/on-expressing-your-concerns\">here</a>. Suggestions for how to improve or become more courageous in this sense can be found <a href=\"https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat\">here</a> and <a href=\"https://www.lesswrong.com/posts/HYWhKXRsMAyvRKRYz/you-can-face-reality\">here</a>.</p><br><p>If a fire alarm goes off, but everybody around you is sat still as though they can't hear it, what do you do? Well, apparently, <a href=\"https://www.lesswrong.com/posts/BEtzRE2M5m9YEAQpX/there-s-no-fire-alarm-for-artificial-general-intelligence\">nothing</a> (for 90% of people). Even when your options are 'be a little embarrassed' or 'maybe burn to death', people still struggle to have the courage to stand up and act how they know makes sense. Cultivating the ability to know when you're right, regardless of how other people are acting, may be an important step for you becoming more rational.</p><p>Similarly, it's important to rationalists to <a href=\"https://www.lesswrong.com/s/wnQWakxdRodnKm5kH\">take ideas seriously,</a> because otherwise you can't make any progress to figuring out if they're right or wrong. Many of the really important-seeming ideas (like <a href=\"https://www.lesswrong.com/tag/ai-risk?showPostCount=true&amp;useTagName=true\">AI risk</a>, other <a href=\"https://www.lesswrong.com/tag/existential-risk?showPostCount=true&amp;useTagName=true\">existential risk</a>, and even what would happen if AI went 'right') are <em>scary</em>. Even just visualising their consequences if they <em>were</em> true can be scary—just like theists who insist that, <a href=\"https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat\">without god, there would be no morality</a> (despite the notable absence of atheist baby-murderers). However, these ideas may also be the most important to think about before they happen <em>specifically because they are so scary. </em>You can't dodge a knife if you're pretending it doesn't exist.</p>",
    "description_length": 3289,
    "viewCount": 36,
    "parentTagId": "world-optimization-value-virtue"
  },
  {
    "core-tag": "World Optimization",
    "_id": "sSNtcEQsqHgN8ZmRF",
    "name": "Fun Theory",
    "slug": "fun-theory",
    "postCount": 63,
    "description_html": "<p><strong>Fun Theory</strong> is the field of knowledge studying how to design for fun in future society: it deals in questions such as \"How much fun is there in the universe?\", \"Will we ever run out of fun?\", \"Are we having fun yet?\" and \"Could we be having more fun?\"</p>\n<p>From <a href=\"https://www.lesswrong.com/posts/K4aGvLnHvYgX9pZHS/the-fun-theory-sequence\">The Fun Theory Sequence</a>:</p>\n<blockquote>\n<p>Many critics (including <a href=\"https://www.lesswrong.com/lw/xl/eutopia_is_scary/\">George Orwell</a>) have commented on the inability of authors to imagine Utopias where anyone would actually want to live. If no one can imagine a Future where anyone would want to live, that may drain off motivation to work on the project. The prospect of endless boredom is routinely fielded by conservatives as a knockdown argument against research on lifespan extension, against cryonics, against all transhumanism, and occasionally against the entire Enlightenment ideal of a better future.</p>\n<p>Fun Theory is also the fully general reply to religious theodicy (attempts to justify why God permits evil). Our present world has flaws even from the standpoint of such eudaimonic considerations as freedom, personal responsibility, and self-reliance. Fun Theory tries to describe the dimensions along which a benevolently designed world can and should be optimized, and our present world is clearly <em>not</em> the result of such optimization. Fun Theory also highlights the flaws of any particular religion's perfect afterlife - you wouldn't want to go to their Heaven.</p>\n</blockquote>\n<h2>The argument against Enlightenment</h2>\n<p>Some critiques of <a href=\"https://www.lesswrong.com/tag/transhumanism\">transhumanism</a> (and related fields such as cryonics or lifespan extension) suggest that human enhancement would be accompanied boredom and the end of fun as we know it. For example: \"if we self-improve human minds to extreme levels of intelligence, all challenges known today may bore us.\" Likewise, \"if superhumanly intelligent machines take care of our every need, it is apparent that no challenges nor fun will remain.\"</p>\n<p>However, we can work towards determining whether and how the universe will offer, or whether we ourselves can create, ever more complex and sophisticated opportunities to delight, entertain and challenge ever more powerful and resourceful minds.</p>\n<h2>The concept of Utopia</h2>\n<p>Transhumanists are usually seen as working towards a better human future. This future is sometimes conceptualized, as George Orwell <a href=\"http://www.orwell.ru/library/articles/socialists/english/e_fun\">aptly describes it</a>, as Utopia:</p>\n<blockquote>\n<p>\"It is a commonplace [view] that the Christian Heaven, as usually portrayed, would attract nobody. Almost all Christian writers dealing with Heaven either say frankly that it is indescribable or conjure up a vague picture of gold, precious stones, and the endless singing of hymns... [W]hat it could not do was to describe a condition in which the ordinary human being actively wanted to be.\"</p>\n</blockquote>\n<p>Imagining this perfect future where every problem is solved and where there is constant peace and rest--as seen, a close parallel to several religious Heavens--rapidly leads to the conclusion that no one would actually want to live there.</p>\n<h2>Complex values and fun theory's solution</h2>\n<p>A key insight of fun theory, in its current embryonic form, is that <em>eudaimonia</em> - the classical framework where happiness is the ultimate human goal - is <a href=\"https://www.lesswrong.com/tag/complexity-of-value\">complicated</a>. That is, there are many properties which contribute to a life worth living. We humans require many things to experience a fulfilled life: Aesthetic stimulation, pleasure, love, social interaction, learning, challenge, and much more.</p>\n<p>It is a common mistake in discussion of future society to extract only one element of the human preferences and advocate that it alone be maximized. This would neglect all other human values. For example, if we simply optimize for pleasure or happiness, <a href=\"https://www.lesswrong.com/tag/wireheading\">\"wirehead\"</a>, we'll stimulate the relevant parts of our brain and experience bliss for eternity, but pursue no other experiences. If almost <em>any</em> element of our value system is absent, then the human future will likely be very unpleasant.</p>\n<p>Enhanced humans are also seen to have the value system of humans today, but we may choose to change it as we self-enhance. We may want to alter our own value system, by eliminating values, like bloodlust, which on reflection we wish were absent. But there are many values which we, on reflection, want to keep, and since we humans have no basis for a value system other than our current value system, fun theory must seek to maximize the value system that we have, rather than inventing new values.</p>\n<p>Fun theory thus seeks to let us keep our curiosity and love of learning intact, while preventing the extremes of boredom possible in a transhuman future if our strongly boosted intellects have exhausted all challenges. More broadly, fun theory seeks to allow humanity to enjoy life when all needs are easily satisfied and avoid the fall into the un-fun utopian futures in literature.</p>\n<h2>External links</h2>\n<ul>\n<li>George Orwell, <a href=\"http://www.orwell.ru/library/articles/socialists/english/e_fun\">Why Socialists Don't Believe in Fun</a></li>\n<li>David Pearce, <a href=\"http://paradise-engineering.com/\">Paradise Engineering</a> and <a href=\"http://www.hedweb.com/hedab.htm\">The Hedonistic Imperative</a> (<a href=\"https://www.lesswrong.com/tag/abolitionism\">Abolitionism</a>) provides a more nuanced alternative to wireheading.</li>\n</ul>\n<h2>See also</h2>\n<ul>\n<li><a href=\"https://www.lesswrong.com/tag/the-fun-theory-sequence\">The Fun Theory Sequence</a></li>\n<li><a href=\"http://lesswrong.com/tag/happiness-1\">Happiness</a></li>\n<li><a href=\"https://www.lesswrong.com/tag/complexity-of-value\">Complexity of value</a></li>\n<li><a href=\"https://www.lesswrong.com/tag/metaethics-sequence\">Metaethics sequence</a></li>\n<li><a href=\"https://www.lesswrong.com/tag/abolitionism\">Abolitionism</a></li>\n</ul>\n",
    "description_length": 6262,
    "viewCount": 243,
    "parentTagId": "world-optimization-value-virtue"
  },
  {
    "core-tag": "World Optimization",
    "_id": "vcvfjGJwRmFbMMS3d",
    "name": "Principles",
    "slug": "principles",
    "postCount": 22,
    "description_html": "<p><i>“<strong>Principles </strong>are fundamental truths that serve as the foundations for behavior that gets you what you want out of life. They can be applied again and again in similar situations to help you achieve your goals.”</i> ― Ray Dalio, Principles: Life and Work.<br><br>Principles, Heuristics, and rules of thumb are generalizations that aim to produce an optimal outcome relative to their cheapness as decision rules.&nbsp;<br><br>Not using generalizations, Rules of thumb and Heuristics isn't possible. precise decision rules such as <a href=\"https://www.lesswrong.com/tag/bayes-theorem-bayesianism\">Bayes Theorem </a>are <a href=\"https://en.wikipedia.org/wiki/Combinatorial_explosion\">computationally intractable </a>even to computers, and surely aren't feasible for humans. the question become whether you have good and effective principles to guide you.</p><p>See also: <a href=\"https://www.lesswrong.com/tag/chesterton-s-fence\">Chesterton's Fence</a>, <a href=\"https://www.lesswrong.com/tag/conservation-of-expected-evidence\">Conservation of Expected Evidence</a>, <a href=\"https://www.lesswrong.com/tag/occam-s-razor\">Occam's razor</a>, <a href=\"https://www.lesswrong.com/tag/goodhart-s-law\">Goodhart's Law</a>, <a href=\"https://www.lesswrong.com/tag/more-dakka\">More Dakka</a></p>",
    "description_length": 1302,
    "viewCount": 67,
    "parentTagId": "world-optimization-value-virtue"
  },
  {
    "core-tag": "World Optimization",
    "_id": "LaDu5bKDpe8LxaR7C",
    "name": "Suffering",
    "slug": "suffering",
    "postCount": 86,
    "description_html": null,
    "description_length": null,
    "viewCount": 89,
    "parentTagId": "world-optimization-value-virtue"
  },
  {
    "core-tag": "World Optimization",
    "_id": "kEX5CzbfiAzGn4q8B",
    "name": "Superstimuli",
    "slug": "superstimuli",
    "postCount": 23,
    "description_html": "<p>Humans evolved various desires that promoted survival and reproductive success in the <a href=\"https://www.psychologytoday.com/intl/blog/darwins-subterranean-world/201806/3-things-we-know-about-the-ancestral-environment\">ancestral environment</a>. <strong>Superstimuli</strong> are modern inventions that satisfy desires better than anything in the ancestral environment could but are detrimental to survival, reproduction, or other high-level goals.</p><p><i>See also:</i><a href=\"https://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&amp;useTagName=true\"> Evolutionary Psychology</a>, <a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\">Goodhart's Law</a>, <a href=\"https://www.lesswrong.com/tag/wireheading?showPostCount=true&amp;useTagName=true\">Wireheading</a></p><blockquote><p><i>A candy bar is a </i>superstimulus<i>: it contains more concentrated sugar, salt, and fat than anything that exists in the ancestral environment.&nbsp; &nbsp;A candy bar matches taste buds that evolved in a hunter-gatherer environment, but it matches those taste buds much more strongly than anything that actually existed in the hunter-gatherer environment. The signal that once reliably correlated to healthy food has been hijacked, blotted out with a point in tastespace that wasn't in the training dataset - an impossibly distant outlier on the old ancestral graphs.&nbsp;</i></p></blockquote><p><i>-- </i>Eliezer Yudkowsky, <a href=\"https://www.lesswrong.com/posts/Jq73GozjsuhdwMLEG/superstimuli-and-the-collapse-of-western-civilization\">Superstimuli and the Collapse of Western Civilisation</a></p><h2>Notable Posts</h2><ul><li><a href=\"https://lessestwrong.com/lw/h3/superstimuli_and_the_collapse_of_western/\">Superstimuli and the Collapse of Western Civilization</a></li><li><a href=\"https://lessestwrong.com/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">Adaptation-Executers, not Fitness-Maximizers</a></li></ul><h2>External Links</h2><ul><li><a href=\"http://www.ted.com/talks/dan_dennett_cute_sexy_sweet_funny.html\">Cute, Sexy, Sweet, Funny</a> by <a href=\"https://en.wikipedia.org/wiki/Daniel_Dennett\">Daniel Dennett</a> at TED</li></ul><h2>See Also</h2><ul><li><a href=\"https://lessestwrong.com/tag/akrasia\">Akrasia</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Adaptation_executers\">Adaptation executers</a></li><li><a href=\"https://lessestwrong.com/tag/evolutionary-psychology\">Evolutionary psychology</a></li></ul>",
    "description_length": 2491,
    "viewCount": 153,
    "parentTagId": "world-optimization-value-virtue"
  },
  {
    "core-tag": "World Optimization",
    "_id": "yEs5Tdwfw5Zw8yGWC",
    "name": "Wireheading",
    "slug": "wireheading",
    "postCount": 38,
    "description_html": "<p><strong>Wireheading</strong> is the artificial stimulation of the brain to experience pleasure, usually through the direct stimulation of an individual's brain's reward or pleasure center with electrical current. It can also be used in a more expanded sense, to refer to any kind of method that produces a form of <em>counterfeit utility</em> by directly maximizing a good feeling, but that fails to realize what we value.</p>\n<p><strong>Related pages:</strong>  <a href=\"https://www.lesswrong.com/tag/complexity-of-value\">Complexity of Value</a>,  <a href=\"https://www.lesswrong.com/tag/goodhart-s-law\">Goodhart's Law</a>, <a href=\"https://www.lesswrong.com/tag/inner-alignment\">Inner Alignment</a></p>\n<p>In both thought experiments and <a href=\"http://www.mindhacks.com/blog/2008/09/erotic_selfstimulat.html\">laboratory experiments</a> direct stimulation of the brain’s reward center makes the individual feel happy. In theory, wireheading with a powerful enough current would be the most pleasurable experience imaginable. There is some evidence that <a href=\"https://lesswrong.com/lw/1lb/are_wireheads_happy/\">reward is distinct from pleasure</a>, and that most currently hypothesized forms of wireheading just motivate a person to continue the wirehead experience, not to feel happy. However, there seems to be no reason to believe that a different form of wireheading which does create subjective pleasure could not be found. The possibility of wireheading raises difficult ethical questions for <a href=\"https://lesswrong.com/tag/hedonism\">those who believe that morality is based on human happiness</a>. A civilization of wireheads \"blissing out\" all day while being fed and maintained by robots would be a state of maximum happiness, but such a civilization would have no art, love, scientific discovery, or any of the other things humans find valuable.</p>\n<p>If we take wireheading as a more general form of producing counterfeit utility, there are many examples of ways of directly stimulating of the reward and pleasure centers of the brain, without actually engaging in valuable experiences. Cocaine, heroin, cigarettes and gambling are all examples of current methods of directly achieving pleasure or reward, but can be seen by many as lacking much of what we value and are potentially extremely detrimental. <a href=\"https://en.wikipedia.org/wiki/Steve_Omohundro\">Steve Omohundro</a> argues<a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">1</a> that: “An important class of vulnerabilities arises when the subsystems for measuring utility become corrupted. Human pleasure may be thought of as the experiential correlate of an assessment of high utility. But pleasure is mediated by neurochemicals and these are subject to manipulation.”</p>\n<p>Wireheading is also an illustration of the complexities of creating a <a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a>. Any AGI naively programmed to increase human happiness could devote its energies to wireheading people, possibly without their consent, in preference to any other goals. Equivalent problems arise for any simple attempt to create AGIs who care directly about human feelings (\"love\", \"compassion\", \"excitement\", etc). An AGI could wirehead people to feel in love all the time, but this wouldn’t correctly realize what we value when we say love is a virtue. For Omohundro, because exploiting those vulnerabilities in our subsystems for measuring utility is much easier than truly realizing our values, a wrongly designed AGI would most certainly prefer to wirehead humanity instead of pursuing human values. In addition, an AGI itself could be vulnerable to wirehead and would need to implement “police forces” or “immune systems” to ensure its measuring system doesn’t become corrupted by trying to produce counterfeit utility.</p>\n<h2>See also</h2>\n<ul>\n<li><a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">Steve Omohundro's paper, section 4 deals with vulnerabilities to counterfeit utility and wireheading</a></li>\n<li><a href=\"https://lesswrong.com/tag/hedonism\">Hedonism</a></li>\n<li><a href=\"https://lesswrong.com/tag/complexity-of-value\">Complexity of value</a></li>\n<li><a href=\"https://lesswrong.com/tag/wanting-and-liking\">Wanting and liking</a></li>\n<li><a href=\"https://lesswrong.com/tag/near-far-thinking\">Near/far thinking</a></li>\n<li><a href=\"https://wiki.lesswrong.com/wiki/Hedonium\">Hedonium</a></li>\n<li><a href=\"https://lesswrong.com/tag/abolitionism\">Abolitionism</a></li>\n</ul>\n<h2>External links</h2>\n<ul>\n<li><a href=\"https://www.hedweb.com/wirehead/index.html\">Wirehead Hedonism versus paradise engineering</a> by David Pearce</li>\n</ul>\n",
    "description_length": 4733,
    "viewCount": 412,
    "parentTagId": "world-optimization-value-virtue"
  },
  {
    "core-tag": "World Optimization",
    "_id": "se3XDuQ4xbeWvu4eF",
    "name": "80,000 Hours",
    "slug": "80-000-hours",
    "postCount": 14,
    "description_html": "<p><strong>80,000 Hours</strong> is an organization in the Effective Altruist community that gives career advice. The name comes from an average career containing about 80,000 hours of work.</p><h3>See also</h3><p><a href=\"https://80000hours.org/\">Official Website</a></p>",
    "description_length": 272,
    "viewCount": 154,
    "parentTagId": "world-optimization-meta"
  },
  {
    "core-tag": "World Optimization",
    "_id": "EeSkeTcT4wtW2fWsL",
    "name": "Cause Prioritization",
    "slug": "cause-prioritization",
    "postCount": 52,
    "description_html": null,
    "description_length": null,
    "viewCount": 30,
    "parentTagId": "world-optimization-meta"
  },
  {
    "core-tag": "World Optimization",
    "_id": "io2ExA7GEeTgHFTFW",
    "name": "Center on Long-Term Risk (CLR)",
    "slug": "center-on-long-term-risk-clr",
    "postCount": 23,
    "description_html": "<p>The <strong>Center on Long-Term Risk</strong>, formerly <em>Foundational Research Institute, </em>is a research group that investigates cooperative strategies to reduce <a href=\"https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\">risks of astronomical suffering</a> (s-risks). This includes not only (post-)human suffering, but also potential digital sentience. Their research is interdisciplinary, drawing on insights from <a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\">artificial intelligence</a>, <a href=\"https://wiki.lesswrong.com/wiki/anthropic_reasoning\">anthropic reasoning</a>, international relations, philosophy, and other fields. Its research agenda focuses on encouraging cooperative behavior in and avoiding conflict between transformative AI systems.</p><h2><strong>See also</strong></h2><ul><li><a href=\"https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\">Suffering risk</a></li><li><a href=\"https://www.lesswrong.com/tag/mindcrime\">Mindcrime</a></li></ul><h2><strong>External links</strong></h2><ul><li><a href=\"https://longtermrisk.org/\">CLR website</a></li></ul>",
    "description_length": 1147,
    "viewCount": 43,
    "parentTagId": "world-optimization-meta"
  },
  {
    "core-tag": "World Optimization",
    "_id": "qAvbtzdG2A2RBn7in",
    "name": "Effective Altruism",
    "slug": "effective-altruism",
    "postCount": 340,
    "description_html": "<p><strong>Effective Altruism</strong> (EA) is a movement trying to invest time and money in causes that do the most good per some unit of effort. The label applies broadly, including a philosophy, a community, a set of organisations and set of behaviours. Likewise it also sometimes means how to donate effectively to charities, choose one's career, do the most good per $, do good in general or ensure the most good happens. &nbsp;All of these different framings have slightly different implications.</p><p>The basic concept behind EA is that one would really struggle to donate 100 times more money or time to charity than you currently do but, spending a little time researching who to donate to <i>could</i> have an impact on roughly this order of magnitude. The same argument works for doing good with your career or volunteer hours.</p><p>The<strong> Effective Altruism movement </strong>also has its own forum,&nbsp;<strong> </strong><a href=\"https://forum.effectivealtruism.org/\"><strong>The EA Forum</strong></a>. It runs on the same software as LessWrong.</p><h1>Key Concepts</h1><h2>The Scale, Neglectedness, Tractability, (Personal Fit) criteria</h2><p>Despite a broad diversity of ideas within the EA community on which areas are most pressing, there are a handful of criteria that are generally agreed make an area potentially impactful to work on (either directly or through donation). These are:</p><ul><li>The area has the potential for impact at <strong>scale, </strong>either in human lives saved, animal or human suffering alleviated,&nbsp;catastrophic crises averted, etc. Sometimes this is called \"importance\"</li><li>The area is generally <strong>neglected, </strong>that is, it has capacity for more support either financially or in terms of skills. &nbsp;An area with lots of resources should lead us to think we are less likely to be able to make improvements.</li><li>The area is <strong>tractable,</strong> it is a solvable problem, or is solvable with minimal resource investment (relative to other&nbsp;problem areas)</li></ul><p>A fourth semi-area is:</p><ul><li>Does the individual have good <strong>personal fit</strong>?<strong> </strong>Do they have unique skills which will make them more effective in an area.</li></ul><h2>Impartiality (geographic, species, time)&nbsp;</h2><h3>Global health and wellbeing (geographic impartiality)</h3><blockquote><p>One morning, I say to them, you notice a child has fallen in and appears to be drowning. To wade in and pull the child out would be easy but it will mean that you get your clothes wet and muddy, and by the time you go home and change you will have missed your first class.</p><p>I then ask the students: do you have any obligation to rescue the child? Unanimously, the students say they do. The importance of saving a child so far outweighs the cost of getting one’s clothes muddy and missing a class, that they refuse to consider it any kind of excuse for not saving the child. Does it make a difference, I ask, that there are other people walking past the pond who would equally be able to rescue the child but are not doing so? No, the students reply, the fact that others are not doing what they ought to do is no reason why I should not do what I ought to do.</p><p>Once we are all clear about our obligations to rescue the drowning child in front of us, I ask: would it make any difference if the child were far away, in another country perhaps, but similarly in danger of death, and equally within your means to save, at no great cost – and absolutely no danger – to yourself?<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"hvkrwieksw4\" role=\"doc-noteref\" id=\"fnrefhvkrwieksw4\"><sup><a href=\"#fnhvkrwieksw4\">[1]</a></sup></span></p></blockquote><p>It is not clear why, under many moral systems we should care more about people who are in our country than to those who aren't. But those who are in developing nations can be helped about 100x more cheaply than those in the US.&nbsp;</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qAvbtzdG2A2RBn7in/tlikzfkbsypreepzjxrk\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qAvbtzdG2A2RBn7in/knnlh9mo2zsunqw7ooqj 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qAvbtzdG2A2RBn7in/raekgx9cc4qtrfcaorgx 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qAvbtzdG2A2RBn7in/ldcbnfaf0wqrjm93us7u 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qAvbtzdG2A2RBn7in/ojblarskq6ah3jloqzpo 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qAvbtzdG2A2RBn7in/nchwknzlm0gnyps4ugfk 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qAvbtzdG2A2RBn7in/espodr4npufxbd33ldsy 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qAvbtzdG2A2RBn7in/gevye0ws3mqap8aebesx 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qAvbtzdG2A2RBn7in/hf3co586x81inr54shuk 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qAvbtzdG2A2RBn7in/bjwgeu0prgmt2i3nywda 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qAvbtzdG2A2RBn7in/l2ovbc1o4nmwbaoirqil 1232w\"></figure><h3>Animal Welfare (species impartiality)</h3><blockquote><p>The question is not, Can they reason?, nor Can they talk? but, Can they suffer?</p></blockquote><p>If states of wellbeing matter, then they matter regardless of a being's ability to express or change the situation. A sleeping person can be tormented by nightmares but we still consider that suffering meaningful. Likewise animals are capable of states of pleasure and pain, regardless of their ability to tell us of their situation.</p><p>And there are many animals. Likewise, they cannot vote and cannot earn money so are unable to change their own situation. This suggests that supporting animal welfare legislation might be a very cheap way to improve wellbeing.</p><p>On a deeper level, EAs say that species is not the marker of moral worth. If we had evolved from dolphins rather than apes, would we be less deserving of moral consideration? If this logic follows, it implies significant low-cost opportunities to improve welfare. &nbsp;</p><h3>Longtermism (time impartiality)</h3><p>A large portion of the EA community are by and large, <a href=\"https://www.lesswrong.com/tag/longtermism\"><strong>longtermist</strong>.</a> This refers to the idea that, if there are many future generations (100s, 1000s or more), and their lives are as valuable as ours, then even very small impacts on all of their lives-- or things like moving good changes forwards in time or bad ones back-- far outweigh impacts on people who are currently alive. Because this concept is less broadly-accepted than charity for currently-alive people, longtermist solutions are also generally considered to be neglected. Longtermist interventions generally focus on <a href=\"https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\">S-risks</a> or <a href=\"https://www.lesswrong.com/tag/existential-risk\">X-risks</a>.</p><p>Examples of longtermist interventions include <a href=\"https://www.lesswrong.com/tag/ai\">AI</a> safety, pandemic preparedness, and <a href=\"https://www.lesswrong.com/tag/nanotechnology\">nanotechnology</a> security. Examples of other popular EA interventions include global poverty alleviation, malaria treatments, and vitamin supplementation in sub-saharan Africa.</p><h2>Suspicious convergence</h2><p>If many unrelated factors point towards doing the same action, beware that you may be using motivated reasoning<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"2\" data-footnote-id=\"3irzmetbyct\" role=\"doc-noteref\" id=\"fnref3irzmetbyct\"><sup><a href=\"#fn3irzmetbyct\">[2]</a></sup></span>.</p><h2>Charity effectiveness</h2><p>From [scale tractability neglectendness], we can see a vast number of charities do not meet all or indeed any of these criteria. A major issue with EA is that some areas are much easier to track progress in than others (think tracking the cost per life saved of malaria nets vs existential <a href=\"https://www.lesswrong.com/tag/ai?showPostCount=false\">AI</a> risk, for instance). What is clear, however, is that some of the more effective charities (of those which <i>are</i> easy to track) have <a href=\"https://80000hours.org/2017/05/most-people-report-believing-its-incredibly-cheap-to-save-lives-in-the-developing-world/\">far more benefit over the average charity than people think</a>-- perhaps as much as 10,000% as effective.</p><figure class=\"image\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qAvbtzdG2A2RBn7in/isf7mbbqc6tr2uuhoc7e\" alt=\"Image\"></figure><h2>An attempt at a minimal set of effective altruism axioms</h2><p>Zvi wrote a set of axioms of EA as well as his disagreement with them in <a href=\"https://www.lesswrong.com/posts/pohTfSGsNQZYbGpCy/criticism-of-ea-criticism-contest\">Criticism of EA Criticism Contest</a>. This list is very roughly based on that, though with very substantial changes.</p><ol><li>Consequentialism. Or something that looks similar in most situations to most people.</li><li>Importance of Suffering. Suffering is The Bad. Happiness/pleasure is The Good.</li><li>Quantification. It is good to quantify things, with made-up numbers if necessary.</li><li>Weirdness humility. If the answer is strange, double-check your math.</li><li>Scope Sensitivity. Shut up and multiply, two are twice as good as one.</li><li>Openness to criticism. Create low-effort ways for people to test your decisions/ theories of change/ sums</li><li>Intentionality. If you plan you will probably fail, but if you don't plan it's you are more likely to.</li><li>Effectiveness. Do what works. The goal is to actually win.</li><li>Altruism. The best way to do good yourself is to act selflessly to do good.</li><li>Impartiality. Beyond close friends and family, we should treat all others equally.</li><li>Evangelicalism. Belief that it is good to add skills and resources to EA.</li><li>Existential Risk. Wiping out all value in the universe is really, really bad.</li><li>Appreciate norms. It is usually good to be predictable to outsiders so you can work together well. It is very tempting to find reasons to break this rule.</li><li>Seriousness. Our actions have real-world consequences. People live and die based on our choices.</li><li>Grace. In practice, people can’t live up to this list fully and that’s acceptable.</li><li>Totalization. Everything of value can be expressed in terms of this framework, though it's often better to confine only part of your life to it and do what you want with the rest.</li></ol><h3>&nbsp;Additional axioms for longtermism</h3><ol><li>Expected value still applies with very small chances of very large outcomes.</li></ol><h1>Total resources and how they are split</h1><figure class=\"image\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qAvbtzdG2A2RBn7in/xxohgmnbmhmfijuawnge\" alt=\"Image\"><figcaption><a href=\"https://forum.effectivealtruism.org/posts/ZbaDmowkXbTBsxvHn/historical-ea-funding-data\">https://forum.effectivealtruism.org/posts/ZbaDmowkXbTBsxvHn/historical-ea-funding-data</a> Spreadsheet <a href=\"https://docs.google.com/spreadsheets/d/1IeO7NIgZ-qfSTDyiAFSgH6dMn1xzb6hB2pVSdlBJZ88/edit#gid=771773474\">https://docs.google.com/spreadsheets/d/1IeO7NIgZ-qfSTDyiAFSgH6dMn1xzb6hB2pVSdlBJZ88/edit#gid=771773474</a>&nbsp;</figcaption></figure><h2>Current EA billionaires</h2><ul><li>Dustin Moskovitz</li><li>Cari Tuna</li><li>Bill Gates - Vaccination is clearly an EA cause</li><li>Vitalik gave $100m - $1Bn in crypto to GiveWell,&nbsp;</li></ul><h2>Maybe</h2><ul><li>Melinda French Gates - Vaccination is clearly an EA cause</li><li>Elon Musk - Civilisation on mars would reduce existential risk from nuclear/biorisk</li><li>Notion founder</li><li>Canva founder</li><li>Crypto people?</li></ul><h2>Number of Community Members</h2><h2>Funding in general&nbsp;</h2><h1>Impact</h1><h3>Global health and economic development</h3><p>Lives saved - 90% CI [50,000, 10mn] - Nathan Young</p><p>The <a href=\"https://www.againstmalaria.com/\">Against Malaria Foundation</a> has distributed more than 70 million bednets to protect people (mostly children) from a debilitating parasite. (<a href=\"https://www.againstmalaria.com/Distributions.aspx\">Source</a>) [number of lives saved]</p><p><a href=\"https://givedirectly.org/\">GiveDirectly</a> has facilitated more than $100 million in direct cash transfers to families living in extreme poverty, who determine for themselves how best to spend the money. (<a href=\"https://www.givedirectly.org/financials/\">Source</a>) [number of lives saved]</p><p>The <a href=\"https://www.imperial.ac.uk/schistosomiasis-control-initiative\">Schistosomiasis Control Initiative</a> and <a href=\"http://www.evidenceaction.org/dewormtheworld/\">Deworm the World Initiative</a> invests in people's health and future well-being by treating preventable diseases that often get little attention. They have given out hundreds of millions of deworming treatments to fight intestinal parasites, which may help people earn higher incomes later in life. (Sources for <a href=\"https://schistosomiasiscontrolinitiative.org/reach\">SCI</a> and <a href=\"https://www.evidenceaction.org/dewormtheworld-2/\">DWI</a>)</p><h3>Animal welfare</h3><p>Chicken equivalent lives saved per year: 90% CI [10m , 100T] - Nathan Young</p><p><a href=\"https://thehumaneleague.org/\">The Humane League</a> and <a href=\"https://mercyforanimals.org/\">Mercy for Animals</a>, alongside many other organizations, have orchestrated corporate campaigns and legal reforms to fight the use of battery cages. Because of this work, more than 100 million hens that would have been caged instead live cage-free. (This includes all cage-free reform work, of which a sizable fraction was funded by EA-aligned donors.)</p><p><a href=\"https://gfi.org/\">The Good Food Institute</a> works with scientists, entrepreneurs, and investors to develop and promote meat alternatives that don't require the suffering of farmed animals.</p><h3>Existential risk and the long-term future</h3><p>[how much lower higher? risk of existential catastrphe as a result]<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"3\" data-footnote-id=\"wpmnksww10j\" role=\"doc-noteref\" id=\"fnrefwpmnksww10j\"><sup><a href=\"#fnwpmnksww10j\">[3]</a></sup></span></p><p>Organizations like the <a href=\"https://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> and the <a href=\"https://www.cser.ac.uk/\">Centre for the Study of Existential Risk</a> work on research and policy related to some of the biggest threats facing humanity, from pandemics and climate change to nuclear war and superintelligent AI systems.</p><p>Some organizations in this space, like the <a href=\"https://humancompatible.ai/\">Center for Human-Compatible AI</a> and the <a href=\"https://intelligence.org/\">Machine Intelligence Research Institute</a>, focus entirely on solving issues posed by advances in artificial intelligence. AI systems of the future could be very powerful and difficult to control --- a dangerous combination.</p><p><a href=\"https://sherlock.bio/\">Sherlock Biosciences</a> is developing a diagnostic platform that could reduce threats from viral pandemics. (They are a private company, but much of their capital comes from a <a href=\"https://www.openphilanthropy.org/focus/scientific-research/sherlock-biosciences-research-viral-diagnostics\">grant</a> made by Open Philanthropy, an EA-aligned grantmaker.)</p><h1>Criticisms&nbsp;</h1><ul><li>EA is incoherent. Consequentialism applies to one's whole life, but many EAs don’t take it this seriously<ul><li>This argument applies to virtue ethics too, but no one criticises it - “why aren’t you constantly seeking to always do the virtuous action”. People in practice seem to take statements from consequentialist philosophies more seriously than they do from others</li><li>It is more intellectually honest to surface incoherence in your worldview - \"I use 80% of my time as effectively as possible\" is more honest that \"I try and always do the most good</li></ul></li><li>EA frames all value in terms of impact creation and this makes members sad<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"4\" data-footnote-id=\"xswvdhaxpgk\" role=\"doc-noteref\" id=\"fnrefxswvdhaxpgk\"><sup><a href=\"#fnxswvdhaxpgk\">[4]</a></sup></span><ul><li>How widespread is this?</li><li>Many EAs don't feel this way</li><li>Some people control orders of magnitude more resources than others. They could use their time and money to improve the lives of many other people. It is not idea to say these people should feel free to not create benefit</li></ul></li><li>EA supports a culture of guilt [Kerry thread]<ul><li>How does EA compare in terms of mental wellbeing to other communities centred around \"doing good\" eg \"Protestant Work Ethic\" and \"Catholic Guilt\"?</li><li>If you struggle with this, consider reading<a href=\"https://forum.effectivealtruism.org/s/a2LBRPLhvwB83DSGq\">&nbsp;<u>Replacing Guilt</u></a>, which is one of only 3 sequences with a permanent place sidebar of the EA Forum.</li></ul></li><li>EA is spending too much money<ul><li>EA is spending <i>more</i> money but it's not immediately obvious it is spending too much. <a href=\"https://forum.effectivealtruism.org/posts/cfdnJ3sDbCSkShiSZ/ea-and-the-current-funding-situation#Risks_of_omission__squandering_the_opportunity_\">It might be spending too little</a>.&nbsp;</li></ul></li><li>EA is too focused on people in developing nations<ul><li>Dollars go much further in developing nations which does lead to a natural bias in spending</li></ul></li><li>EA isn't focused enough on systemic change in America<ul><li>Note that this is often used in very similar situations to the above criticism. &nbsp;And in some of these, they can't both be true.&nbsp;</li></ul></li><li>EA is too focused on longtermism and existential risk to the detriment of people who are alive now<ul><li>People who are alive now are far less neglected, they can participate in markets, democracies, and self-advocacy</li><li>A significant portion of funding goes to present causes</li><li>Existential risk is arguably high enough to be relevant even to people alive today</li></ul></li><li>EAs defer too much to authority</li><li>EAs don't listen to outside experts enough</li><li>EA doesn't care about [insert issue]</li><li>The repugnant conclusions is bad</li><li>Utilitarianism is wrong</li><li>EAs lie a bit<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"5\" data-footnote-id=\"k58wv7lptap\" role=\"doc-noteref\" id=\"fnrefk58wv7lptap\"><sup><a href=\"#fnk58wv7lptap\">[5]</a></sup></span><ul><li>Nick Bostrom said he won a national record when really he did more courses than anyone he ever talked to<ul><li>It's hard to say. &nbsp;This is the sort of thing many people write in book bios. &nbsp;But regardless, he removed it when pressed</li></ul></li><li>If these are the best accusations of dishonesty one can get from a 1000s strong decade long movement, then it sounds like they are pretty honest</li></ul></li><li>There is a culture of suppressing disagreement while claiming to welcome it<ul><li>This seems much better than comparable communities.&nbsp;</li></ul></li><li>Add from Zvi's list</li></ul><h3>Criticisms to add</h3><p><a href=\"https://stefanfschubert.com/blog/2020/12/30/five-common-ea-self-criticisms-i-disagree-with\"><u>Stefan Shubert's criticisms and responses</u></a></p><p>Kuhn, Ben (2013)&nbsp;<a href=\"https://www.benkuhn.net/ea-critique/\">A critique of effective altruism</a>,&nbsp;<i>Ben Kuhn’s Blog</i>, December 2.</p><p>McMahan, Jeff (2016)&nbsp;<a href=\"https://doi.org/10.5840/tpm20167379\">Philosophical critiques of effective altruism</a>,&nbsp;<i>The Philosophers’ Magazine</i>, vol. 73, pp. 92–99.</p><p>Nielsen, Michael (2022)&nbsp;<a href=\"https://michaelnotebook.com/eanotes/\">Notes on effective altruism</a>,&nbsp;<i>Michael’s Notebook</i>, June 2.</p><p>Rowe, Abraham (2022)&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/n3WwTz4dbktYwNQ2j/critiques-of-ea-that-i-want-to-read\">Critiques of EA that I want to read</a>,&nbsp;<i>Effective Altruism Forum</i>, June 19.</p><p>Wiblin, Robert &amp; Keiran Harris (2019)&nbsp;<a href=\"https://80000hours.org/podcast/episodes/vitalik-buterin-new-ways-to-fund-public-goods/\">Vitalik Buterin on effective altruism, better ways to fund public goods, the blockchain’s problems so far, and how it could yet change the world</a>,&nbsp;<i>80,000 Hours</i>, September 3.</p><p>Zhang, Linchuan (2021)&nbsp;<a href=\"https://forum.effectivealtruism.org/posts/pxALB46SEkwNbfiNS/the-motivated-reasoning-critique-of-effective-altruism\">The motivated reasoning critique of effective altruism</a>,&nbsp;<i>Effective Altruism Forum</i>, September 14.</p><p>The winners of this <a href=\"https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest\">https://forum.effectivealtruism.org/posts/YgbpxJmEdFhFGpqci/winners-of-the-ea-criticism-and-red-teaming-contest</a>&nbsp;</p><h1>Related pages</h1><ul><li><a href=\"https://www.lesswrong.com/tag/altruism\">Altruism</a></li><li><a href=\"https://www.lesswrong.com/tag/cause-prioritization\">Cause Prioritization</a></li><li><a href=\"https://lessestwrong.com/tag/utilitarianism\">Utilitarianism</a></li><li><a href=\"https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\">S-risk</a></li><li><a href=\"https://www.lesswrong.com/tag/existential-risk\">X-risk</a></li></ul><h1>Notable EA orgs</h1><ul><li><a href=\"https://80000hours.org/\">80,000 Hours</a>, who offer advice for how to have a maximally globally impactful career</li><li><a href=\"https://www.effectivealtruism.org/\">Effective Altruism,</a> who offer support for local EA groups, as well as articles and advice surrounding EA</li><li><a href=\"https://www.givewell.org/\">GiveWell,</a> a charity doing research into the effectiveness of other charities to provide information for donors</li><li><a href=\"https://www.thelifeyoucansave.org/the-book/?gclid=CjwKCAjwjqT5BRAPEiwAJlBuBXb3m1FKunezyfsYzYkjmgzSCHScRgZpzMH097cbAAGC5lmHUP-J3BoCcnAQAvD_BwE\">The Life You Can Save</a>, a free eBook outlining reasons for donating more and more effectively</li><li><a href=\"https://www.lesswrong.com/tag/center-on-long-term-risk-clr\">Center on Long-Term Risk</a></li></ul><ol class=\"footnote-section footnotes\" data-footnote-section=\"\" role=\"doc-endnotes\"><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"1\" data-footnote-id=\"hvkrwieksw4\" role=\"doc-endnote\" id=\"fnhvkrwieksw4\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"hvkrwieksw4\"><sup><strong><a href=\"#fnrefhvkrwieksw4\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Peter Singer - https://newint.org/features/1997/04/05/peter-singer-drowning-child-new-internationalist</p></div></li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"2\" data-footnote-id=\"3irzmetbyct\" role=\"doc-endnote\" id=\"fn3irzmetbyct\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"3irzmetbyct\"><sup><strong><a href=\"#fnref3irzmetbyct\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>https://forum.effectivealtruism.org/posts/omoZDu8ScNbot6kXS/beware-surprising-and-suspicious-convergence</p></div></li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"3\" data-footnote-id=\"wpmnksww10j\" role=\"doc-endnote\" id=\"fnwpmnksww10j\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"wpmnksww10j\"><sup><strong><a href=\"#fnrefwpmnksww10j\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>Because existential risk is so important compared to anything else, there is some chance that EA has made this a little worse and so is a net negative enterprise</p></div></li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"4\" data-footnote-id=\"xswvdhaxpgk\" role=\"doc-endnote\" id=\"fnxswvdhaxpgk\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"xswvdhaxpgk\"><sup><strong><a href=\"#fnrefxswvdhaxpgk\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>https://twitter.com/KerryLVaughan/status/1545063368695898112?s=20&amp;t=xgaSuh22V6y44Wkcebo22Q</p></div></li><li class=\"footnote-item\" data-footnote-item=\"\" data-footnote-index=\"5\" data-footnote-id=\"k58wv7lptap\" role=\"doc-endnote\" id=\"fnk58wv7lptap\"><span class=\"footnote-back-link\" data-footnote-back-link=\"\" data-footnote-id=\"k58wv7lptap\"><sup><strong><a href=\"#fnrefk58wv7lptap\">^</a></strong></sup></span><div class=\"footnote-content\" data-footnote-content=\"\"><p>https://twitter.com/xriskology/status/1579832304503259136?s=20&amp;t=e8IFDZuxC5gLO2vdCldwyg</p></div></li></ol>",
    "description_length": 25401,
    "viewCount": 242,
    "parentTagId": "world-optimization-meta"
  },
  {
    "core-tag": "World Optimization",
    "_id": "xEZwTHPd5AWpgQx9w",
    "name": "GiveWell",
    "slug": "givewell",
    "postCount": 27,
    "description_html": "<p><strong>GiveWell</strong> is an organization that evaluates the effectiveness of charities and recommends effective charities. It is associated with the <a href=\"http://lesswrong.com/tag/effective-altruism\">Effective Altruist</a> movement.</p>",
    "description_length": 246,
    "viewCount": 21,
    "parentTagId": "world-optimization-meta"
  },
  {
    "core-tag": "World Optimization",
    "_id": "NzSTgAtKwgivkfeYm",
    "name": "Heroic Responsibility",
    "slug": "heroic-responsibility",
    "postCount": 36,
    "description_html": "<p><strong>Heroic responsibility</strong> is the responsibility to get the job done no matter what, including not shifting any responsibility for its completion on to others.</p><p><i>\"You could call it <strong>heroic responsibility</strong>, maybe,” Harry Potter said. “Not like the usual sort. It means that whatever happens, no matter what, it’s always your fault. Even if you tell Professor McGonagall, she’s not responsible for what happens, you are. Following the school rules isn’t an excuse, someone else being in charge isn’t an excuse, even trying your best isn’t an excuse. There just aren’t any excuses, you’ve got to get the job done no matter what.” Harry’s face tightened. “That’s why I say you’re not thinking responsibly, Hermione. Thinking that your job is done when you tell Professor McGonagall—that isn’t heroine thinking. Like Hannah being beat up is okay then, because it isn’t your fault anymore. Being a heroine means your job isn’t finished until you’ve done whatever it takes to protect the other girls, permanently.” In Harry’s voice was a touch of the steel he had acquired since the day Fawkes had been on his shoulder. “You can’t think as if just following the rules means you’ve done your duty. –</i><a href=\"http://hpmor.com/chapter/75\"><i>HPMOR</i></a><i>, chapter 75.</i><br>&nbsp;</p><h2>External Links</h2><ul><li>The discussion at this <a href=\"http://www.reddit.com/r/HPMOR/comments/yj2kb/ethical_solipsism_chapter_75/\">Reddit post</a> is excellent. <i>This wiki requires work.</i></li></ul>",
    "description_length": 1530,
    "viewCount": 149,
    "parentTagId": "world-optimization-meta"
  },
  {
    "core-tag": "Community",
    "_id": "Z6DgiCrMtpSNxwuYW",
    "name": "Grants & Fundraising Opportunities",
    "slug": "grants-and-fundraising-opportunities",
    "postCount": 103,
    "description_html": "<html><head></head><body><p>Many LessWrong readers actively rely on <strong>grants or fundraising opportunities</strong> to support their work (for example by running non-profits or startups, working as independent researchers, or being supported by academic grants).&nbsp;</p><p>This tag lists concrete opportunities for fundraising of interest to the LessWrong community. This typically means projects working on improving the long-term future, refining the art of rationality, and related missions. &nbsp;</p><p>This tag should not be used for meta-discussion, e.g. of fundraising strategies, coordination between funders, or cost-benefit analyses of particular funding opportunities.&nbsp;</p></body></html>",
    "description_length": 711,
    "viewCount": 102,
    "parentTagId": "community-all"
  },
  {
    "core-tag": "Community",
    "_id": "irYLXtT9hkPXoZqhH",
    "name": "Growth Stories",
    "slug": "growth-stories",
    "postCount": 81,
    "description_html": "<p>Recollections of personal progress, lessons learned, memorable experiences, coming of age, in autobiographical form.</p><p><strong>Sequences:</strong>&nbsp;</p><ul><li><a href=\"https://www.lesswrong.com/s/SXurf2mWFw8LX2mkG\">Yudkowsky's Coming of Age</a></li></ul><p><strong>Related Pages:</strong> <a href=\"https://www.lesswrong.com/tag/postmortems-and-retrospectives\">Postmortems &amp; Retrospectives</a>, <a href=\"https://www.lesswrong.com/tag/updated-beliefs-examples-of\">Updated Beliefs (examples of)</a>, <a href=\"https://www.lesswrong.com/tag/self-improvement\">Self Improvement</a>, <a href=\"https://www.lesswrong.com/tag/progress-studies\">Progress Studies</a> (society level)</p>",
    "description_length": 689,
    "viewCount": 105,
    "parentTagId": "community-all"
  },
  {
    "core-tag": "Community",
    "_id": "EXgFbrqoRRkCRgnDy",
    "name": "Online Socialization",
    "slug": "online-socialization",
    "postCount": 38,
    "description_html": "<p><strong>Online Socialization</strong> is, among other things, something you might have to do a lot of if there's a worldwide pandemic.</p>",
    "description_length": 141,
    "viewCount": 33,
    "parentTagId": "community-all"
  },
  {
    "core-tag": "Community",
    "_id": "2i3w84KCkqZzpnQ4d",
    "name": "Petrov Day",
    "slug": "petrov-day",
    "postCount": 49,
    "description_html": "<p><strong>Petrov Day</strong> is a tradition celebrating Soviet military officer Stanislav Petrov, who played a key role in preventing a nuclear attack during a false alarm incident on September 26, 1983.</p><h3>How to celebrate [stub]</h3><p><a href=\"http://petrovday.com/\">petrov.com </a>has an organizer's guide for a small quiet ceremony, and <a href=\"https://www.lesswrong.com/posts/XJxwFMSL5TPN2usC6/modes-of-petrov-day\">Modes of Petrov Day</a> contains additional ideas. Otherwise see the many posts tagged Petrov Day for things people have done in various times/places/groups.</p><h3>LessWrong Website Celebrations</h3><p>The LessWrong site has an annual tradition of commemorating the day involving users having the potential to take down the LessWrong frontpage for 24 hours. This has happened since 2019. Here are the relevant posts.</p><figure class=\"table\"><table style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><tbody><tr><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><strong>Year</strong></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><strong>Announcement</strong></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><strong>Retrospective</strong></td></tr><tr><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><strong>2019</strong></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><a href=\"https://www.lesswrong.com/posts/vvzfFcbmKgEsDBRHh/honoring-petrov-day-on-lesswrong-in-2019\">Honoring Petrov Day on LessWrong, in 2019</a></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><a href=\"https://www.lesswrong.com/posts/krgNxiooRfnP9L4ZD/follow-up-to-petrov-day-2019\">Follow-Up to Petrov Day, 2019</a></td></tr><tr><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><strong>2020</strong></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><a href=\"https://www.lesswrong.com/posts/XfHXQPPKNY8BXkn72/honoring-petrov-day-on-lesswrong-in-2020\">Honoring Petrov Day on LessWrong, in 2020</a></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><a href=\"https://www.lesswrong.com/posts/KQnYogkFTKc9wpWjY/postmortem-to-petrov-day-2020\">Postmortem to Petrov Day, 2020</a></td></tr><tr><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><strong>2021</strong></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><a href=\"https://www.lesswrong.com/posts/EW8yZYcu3Kff2qShS/petrov-day-2021-mutually-assured-destruction\">Petrov Day 2021: Mutually Assured Destruction?</a></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><a href=\"https://www.lesswrong.com/posts/wgreyTFhrNmgJBjMD/petrov-day-retrospective-2021\">Petrov Day Retrospective: 2021</a></td></tr><tr><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><strong>2022</strong></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><a href=\"https://www.lesswrong.com/posts/KTEciTeFwL2tTujZk/lw-petrov-day-2022-monday-9-26\">LW Petrov Day 2022 (Monday, 9/26)</a></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><a href=\"https://www.lesswrong.com/posts/ytuLbHbdQweAwGk9L/petrov-day-retrospective-2022\">Petrov Day Retrospective: 2022</a></td></tr><tr><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><strong>2023</strong></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\">N/A</td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><a href=\"https://www.lesswrong.com/posts/c7uQLGrCBLjkbZhPq/petrov-day-retrospective-2023-re-the-most-important-virtue\">Petrov Day Retrospective, 2023 (re: the most important virtue of Petrov Day &amp; unilaterally promoting it)</a></td></tr><tr><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><strong>2024</strong></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><a href=\"https://www.lesswrong.com/posts/6LJ6xcHEjKF9zWKzs/completed-the-2024-petrov-day-scenario\">[Completed] The 2024 Petrov Day Scenario</a></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid\"><a href=\"https://www.lesswrong.com/posts/NsxFcB2EAxcY6cdGh/2024-petrov-day-retrospective\">2024 Petrov Day Retrospective</a></td></tr></tbody></table></figure><h3>See also</h3><p><a href=\"https://www.lesswrong.com/tag/existential-risk\">Existential Risk</a>, <a href=\"https://www.lesswrong.com/tag/ritual\">Ritual</a></p>",
    "description_length": 4351,
    "viewCount": 492,
    "parentTagId": "community-all"
  },
  {
    "core-tag": "Community",
    "_id": "MXcpQvaPGtXpB6vkM",
    "name": "Public Discourse",
    "slug": "public-discourse",
    "postCount": 176,
    "description_html": "<html><head></head><body><p><strong>Public discourse </strong>refers to our ability to have conversations <i>in large groups</i>, both as a society, and in smaller communities; as well as conversations between a few well-defined participants (such as presidential debates) that take place publicly.&nbsp;</p><p>This tag is for understanding the nature of public discourse (How good is it? What makes it succeed or fail?), and ways of improving it using technology or novel institutions.&nbsp;</p><p>See also: <a href=\"https://www.lesswrong.com/tag/conversation-topic\">Conversation (topic)</a></p></body></html>",
    "description_length": 610,
    "viewCount": 47,
    "parentTagId": "community-all"
  },
  {
    "core-tag": "Community",
    "_id": "tdt83ChxnEgwwKxi6",
    "name": "Reading Group",
    "slug": "reading-group",
    "postCount": 42,
    "description_html": "<p>Discussing book chapters together, sharing summaries, questions and notes.</p>",
    "description_length": 81,
    "viewCount": 40,
    "parentTagId": "community-all"
  },
  {
    "core-tag": "Community",
    "_id": "Z38PqJbRyfwCxKvvL",
    "name": "Research Agendas",
    "slug": "research-agendas",
    "postCount": 205,
    "description_html": "<p><strong>Research Agendas</strong> lay out the areas of research which individuals or groups are working on, or those that they believe would be valuable for others to work on. They help make research more legible and encourage discussion of priorities.</p>",
    "description_length": 259,
    "viewCount": 170,
    "parentTagId": "community-all"
  },
  {
    "core-tag": "Community",
    "_id": "hXTqT62YDTTiqJfxG",
    "name": "Ritual",
    "slug": "ritual",
    "postCount": 76,
    "description_html": "<p><strong>Rituals</strong> are symbolic actions. In the context of LessWrong, it's significant that many rituals have some impact on your cognition, which makes them appropriate to be careful with. Nonetheless, some LessWrongers have worked to explore the space of ritual through a rationalist lens.</p><p>It's a bit tricky to define. The book <a href=\"https://www.amazon.com/Secular-Wholeness-Skeptics-Paths-Richer/dp/155369175X\">Secular Wholeness</a> notes:</p><blockquote><p><i>There’s a hazy boundary between the words “ritual,” “habit,” and “custom.” I think the difference between a ritual act and a habitual one lies in awareness and assent. An act becomes a ritual for you when you perform it with conscious awareness of its symbolic and emotional meaning, and with willing assent to those meanings. Unless you act with both awareness and assent, your act is merely a habit (if it is unique to you) or a custom (if you share it with others).</i></p></blockquote><p>Two key questions relating to ritual and rationality are:</p><ul><li>How can we capture the value of ritual, without incurring epistemic risk?</li><li>Can rituals be actively helpful for rationality?</li></ul><p><strong>Sequences:</strong></p><ul><li><a href=\"https://www.lesswrong.com/s/3bbvzoRA8n6ZgbiyK\">Rational Ritual</a> by <a href=\"https://www.lesswrong.com/users/raemon\">Raemon</a></li></ul><p><strong>Related Pages:</strong> <a href=\"https://www.lesswrong.com/tag/secular-solstice\">Secular Solstice</a>, <a href=\"https://www.lesswrong.com/tag/petrov-day\">Petrov Day</a>, <a href=\"https://www.lesswrong.com/tag/grieving\">Grieving</a>, <a href=\"https://www.lesswrong.com/tag/marriage\">Marriage</a>, <a href=\"https://www.lesswrong.com/tag/religion\">Religion</a>, <a href=\"https://www.lesswrong.com/tag/art\">Art</a>, <a href=\"https://www.lesswrong.com/tag/music\">Music</a>, <a href=\"https://www.lesswrong.com/tag/poetry\">Poetry</a>, <a href=\"https://www.lesswrong.com/tag/meditation\">Meditation</a>, <a href=\"https://www.lesswrong.com/tag/circling\">Circling</a></p>",
    "description_length": 2044,
    "viewCount": 70,
    "parentTagId": "community-all"
  },
  {
    "core-tag": "Community",
    "_id": "MfpEPj6kJneT9gWT6",
    "name": "Site Meta",
    "slug": "site-meta",
    "postCount": 721,
    "description_html": "<p><strong>Site Meta </strong>is the category for discussion about the LessWrong and AI Alignment Forum websites for each of them. It includes technical updates. It applies to<a href=\"https://www.lesswrong.com/tag/lw-team-announcements\"> team announcements</a> such as updates, features, events, moderation activity and policy, downtime, requests for feedback, as well as site documentation, and the team’s writings about site philosophy/strategic thinking.</p><p>The tag also applies to any discussion of the site norms/moderation, feature requests, questions, and ideas about what the site should do – regardless of author.</p>",
    "description_length": 629,
    "viewCount": 105,
    "parentTagId": "community-lesswrong"
  },
  {
    "core-tag": "Community",
    "_id": "h96z2Xt4h6zt2wiw2",
    "name": "GreaterWrong Meta",
    "slug": "greaterwrong-meta",
    "postCount": 10,
    "description_html": "<html><head></head><body><p><strong>GreaterWrong</strong> is an alternative front-end/viewer for the LessWrong site. The viewer is accessible at <a href=\"https://www.greaterwrong.com\">www.greaterwrong.com</a>. The project uses the API of the main LessWrong project but is maintained independently by LessWrong users, <a href=\"https://www.lessestwrong.com/users/clone-of-saturn\">clone of saturn</a> and <a href=\"https://www.lessestwrong.com/users/saidachmiz\">Said Achmiz</a>.</p></body></html>",
    "description_length": 492,
    "viewCount": 73,
    "parentTagId": "community-lesswrong"
  },
  {
    "core-tag": "Community",
    "_id": "a3W2TSzPuxKr3Hm9j",
    "name": "Intellectual Progress via LessWrong",
    "slug": "intellectual-progress-via-lesswrong",
    "postCount": 31,
    "description_html": null,
    "description_length": null,
    "viewCount": 32,
    "parentTagId": "community-lesswrong"
  },
  {
    "core-tag": "Community",
    "_id": "hGzywXvWhSdJi5F2a",
    "name": "LW Moderation",
    "slug": "lw-moderation",
    "postCount": 34,
    "description_html": "<p><strong>LessWrong Moderation</strong> posts deal with how the site moderators comments and posts. It includes laying out policies, decisions about who's on the mod team, and concrete moderation decisions.</p><p>For general posts on the <i>topic</i> of moderation, see <a href=\"https://www.lesswrong.com/tag/moderation-topic\">Moderation (topic)</a></p>",
    "description_length": 354,
    "viewCount": 27,
    "parentTagId": "community-lesswrong"
  },
  {
    "core-tag": "Community",
    "_id": "oraLTPkETL5xKmhx3",
    "name": "Moderation (topic)",
    "slug": "moderation-topic",
    "postCount": 26,
    "description_html": "<p><strong>Moderation</strong>, in online communities especially, deals with what decision and limitation should be made in order for that community to thrive.&nbsp;</p><p>For posts regarding LW moderation policies and decisions (such as ban notices) see <a href=\"https://www.lesswrong.com/tag/lw-moderation\">LW Moderation</a>.</p>",
    "description_length": 331,
    "viewCount": 13,
    "parentTagId": "community-lesswrong"
  },
  {
    "core-tag": "Community",
    "_id": "Zz3HWyByyKF64Sfns",
    "name": "The SF Bay Area",
    "slug": "the-sf-bay-area",
    "postCount": 42,
    "description_html": "<p><strong>The San Francisco Bay Area</strong> is a region in the US state of California. Many members of <a href=\"http://lesswrong.com/tag/community\">the rationalist community</a> are located there, as are the <a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri\">Machine Intelligence Research Institute</a> and the <a href=\"https://www.lesswrong.com/tag/center-for-applied-rationality-cfar\">Center For Applied Rationality</a></p><p><i>Note: Covid-19 resulted in many people moving and the state of living affairs described below may no longer be accurate.</i></p><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/rationalist-movement\">Rationalist movement</a></li><li><a href=\"https://www.lesswrong.com/tag/history-of-less-wrong\">History of Less Wrong</a></li><li><a href=\"/community\">LessWrong Meetup Groups</a></li></ul><h2>History</h2><p>How did it become a hub?</p><p>The <a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri\">Machine Intelligence Research Institute</a> (then the Singularity Institute) moved to the Bay Area in February 2005.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn1\"><sup>1</sup></a></p><p>The charity evaluator <a href=\"https://en.wikipedia.org/wiki/GiveWell\">GiveWell</a> completed its move from New York to San Francisco in February 2013.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn2\"><sup>2</sup></a></p><p>The effective careers research organization 80,000 Hours announced it was moving to the Bay Area in May 2016, with the move completed by October.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn3\"><sup>3</sup></a> It was also in the Bay Area for summer 2016.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn4\"><sup>4</sup></a> Its 2017 review also mentions moving to the Bay Area in 2017.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn5\"><sup>5</sup></a> In 2019, 80,000 Hours moved from the Bay Area to the UK.</p><p>The <a href=\"https://wiki.lesswrong.com/wiki/Center_for_Applied_Rationality\">Center for Applied Rationality</a>'s 2017 Impact Report found that \"moved to the Bay Area due to CFAR\" is one of the strongest predictors for a CFAR participant having an \"increase in expected impact\".<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn6\"><sup>6</sup></a></p><p>Ward Street stuff.</p><h2>Debate</h2><p>Especially since 2016 or so (possibly earlier?), there has been a considerable amount of debate about whether moving to the Bay Area is good for individuals or the community as a whole.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn7\"><sup>7</sup></a></p><p>Some illustrative quotes for concern about people moving to the Bay Area:</p><blockquote><ul><li>\"The reasons for this are not immediately apparent. From the outside, people full of energy and enthusiasm make the pilgrimage to Berkeley, go quiet on social media, and when you finally hear from them six months later they don't seem like the person you once knew. <i>Something</i> is happening to them, although it isn't particularly clear what.\"<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn8\"><sup>8</sup></a></li><li>\"The theme of the Bay Solstice turned out to be 'Hey guys, so people keep coming to the Bay, running on a dream and a promise of community, but that community is not actually there, there's a tiny number of well-connected people who everyone is trying to get time with, and everyone seems lonely and sad. And we don't even know what to do about this.' \"<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn9\"><sup>9</sup></a><sup>,</sup><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn10\"><sup> 10</sup></a><sup>,</sup><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn11\"><sup>11,</sup></a><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn12\"><sup> 12</sup></a></li></ul></blockquote><p>&nbsp;</p><blockquote><p>\"It's seemed to me for awhile now that the stuff that people are actually talking about in-person (e.g. at CFAR workshops) has far outstripped the pace of what's publicly available in blog post format and I'm really happy to see progress on that front.\" <a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn13\"><sup>13</sup></a><sup>, </sup><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn14\"><sup>14</sup></a><sup>,</sup><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn15\"><sup> 15</sup></a><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn16\"><sup>,16,</sup></a><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn17\"><sup>17</sup></a></p></blockquote><p>&nbsp;</p><blockquote><p>4. Ward Street is quickly becoming the center of the rationalist scene in Berkeley. We’re trying to encourage that so that as many people as possible can live near each other and it can feel like more of a community. I’ll be staying there temporarily when I first get to California, and I know a lot of other people on the street and they’re all pretty interesting. Anyway, there’s a house opening up there as the current residents leave, and we’d like to get rationalist-adjacent people to move in. It’s three bedrooms, one bathroom, and it costs $4100/month total. If interested (either in renting the whole house with friends/family, or in just renting one room and hoping two other people want the same), email jsalvatier[at]gmail[dot]com and he can tell you more / help connect interested parties together.\"\"\"<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn18\"><sup>18</sup></a><sup>, </sup><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn19\"><sup>19</sup></a><sup>, </sup><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn20\"><sup>20,</sup></a> <a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn21\"><sup>21</sup></a></p></blockquote><p>&nbsp;</p><blockquote><p>\"Even if you know nothing else, you know to move to San Francisco or New York and hoping something good happens there, rather than sitting around in some dying small town where you know nothing will ever happen and being curious about anything beyond the town is a cultural transgression. This is a strategy open to all.\"<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn22\"><sup>22</sup></a></p></blockquote><h2>Sending people to the Bay</h2><blockquote><p>I know meetups causing people to move to the Bay is a controversial topic, but from my perspective, moving to the Bay is one of the best things a person can do in terms of expected impact on the existential risk landscape. It gives people the opportunity to work at aligned organizations, and to be around hundreds of like-minded people, which (in addition to its social benefits) allows people to find collaborators with whom to start new projects and organizations.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn23\"><sup>23</sup></a></p></blockquote><h2>External links</h2><ul><li><a href=\"http://www.bayrationality.com/\">Informational site about Bay Area rationality</a> (not sure who runs the site)</li><li><a href=\"http://www.rationalistgames.org/\">Rationalist House Games</a></li></ul><h2>References</h2><ol><li><a href=\"https://web.archive.org/web/20060220211402/http://www.singinst.org:80/news/\">\"News of the Singularity Institute for Artificial Intelligence\"</a>. Archived from <a href=\"http://www.singinst.org/news/\">the original</a> on February 20, 2006. Retrieved February 12, 2018. \"SIAI has moved to Silicon Valley. Executive Director Tyler Emerson and Advocacy Director Michael Anissimov are both located in the Bay Area of California, along with SIAI Research Fellow Eliezer Yudkowsky (since February). This should enable us to stay in better contact with donors, and cultivate team members and additional collaborators.\"<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref1\">↩</a></li><li>Holden Karnofsky. <a href=\"https://blog.givewell.org/2013/02/08/givewells-progress-in-2012/\">\"GiveWell's progress in 2012\"</a>. February 8, 2013. <i>The GiveWell Blog</i>. GiveWell. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref2\">↩</a></li><li>Benjamin Todd. <a href=\"https://groups.google.com/forum/#!msg/80k_updates/jAZNlgEhSsM/YIztgo9kAAAJ\">\"80k supporter update - moving to the Bay; cost\"</a>. May 15, 2016. Google Groups. Retrieved February 12, 2018. \"We decided to move and our trustees have approved. We plan to be out in the Bay by August, and fully moved by Oct. The next step is to get visas, which is in progress.\"<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref3\">↩</a></li><li>Benjamin Todd. <a href=\"https://groups.google.com/forum/#!msg/80k_updates/RxRGOUF0ii4/TTUsGDgyDAAJ\">\"80k supporter update\"</a>. August 23, 2016. Google Groups. Retrieved February 12, 2018. \"Moved to the Bay Area for the summer.\"<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref4\">↩</a></li><li>Benjamin Todd. <a href=\"https://80000hours.org/2017/12/annual-review/\">\"Annual review December 2017\"</a>. December 24, 2017. 80,000 Hours. Retrieved February 12, 2018. \"We completed our move to the Bay Area, securing visas for everyone on the team by April 2017, setting up our office, and doing the administration needed (though we’re yet to have the pleasure of filing our first personal US tax returns…).\"<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref5\">↩</a></li><li>Dan Keys. <a href=\"http://www.rationality.org/resources/updates/2017/cfar-2017-impact-report\">\"CFAR 2017 Impact Report\"</a>. Center for Applied Rationality. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref6\">↩</a></li><li><a href=\"https://thingofthings.wordpress.com/2017/05/03/why-do-all-the-rationalists-live-in-the-bay-area/\">\"Why Do All The Rationalists Live In The Bay Area?\"</a>. May 4, 2017. Thing of Things. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref7\">↩</a></li><li>bendini. <a href=\"https://www.lesswrong.com/posts/wmEcNP3KFEGPZaFJk/the-craft-and-the-community-a-post-mortem-and-resurrection\">\"The Craft &amp; The Community - A Post-Mortem &amp; Resurrection\"</a>. November 1, 2017. LessWrong. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref8\">↩</a></li><li>Raymond Arnold. <a href=\"https://www.lesswrong.com/lw/p1f/notes_from_the_hufflepuff_unconference_part_1/\">\"Notes from the Hufflepuff Unconference (Part 1)\"</a>. May 23, 2017. LessWrong. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref9\">↩</a></li><li>Zvi Mowshowitz. <a href=\"https://thezvi.wordpress.com/2017/08/12/what-is-rationalist-berkleys-community-culture/\">\"What Is Rationalist Berkeley’s Community Culture?\"</a>. November 3, 2017. Don't Worry About the Vase. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref10\">↩</a></li><li>Elizabeth Van Nostrand. <a href=\"https://www.facebook.com/li.van.nostrand/posts/10102872753725305\">\"It looks pretty likely I'll move to the bay...\"</a>. Facebook. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref11\">↩</a></li><li>Brent Dill. <a href=\"https://www.facebook.com/ialdabaoth/posts/10208221491793885\">\"Two years ago, to the day, I decided to move to the...\"</a>. Facebook. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref12\">↩</a></li><li><a href=\"https://www.lesswrong.com/posts/tMhEv28KJYWsu6Wdo/kensh#vDST8wTjsiWKCjZ3y\">https://www.lesserwrong.com/posts/tMhEv28KJYWsu6Wdo/kensh#vDST8wTjsiWKCjZ3y</a><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref13\">↩</a></li><li>Raymond Arnold. <a href=\"https://www.facebook.com/raymond.arnold.5/posts/10211225174359187\">\"It took me a disturbing amount of time to realize...\"</a>. Facebook. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref14\">↩</a></li><li>Alyssa Vance. <a href=\"https://www.facebook.com/groups/effective.altruists/permalink/1366133853442968/\">\"Many Effective Altruists think about moving to the San Francisco Bay Area, or have already done so …\"</a>. April 14, 2017. Facebook. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref15\">↩</a></li><li>Scott Alexander. <a href=\"http://slatestarcodex.com/2017/07/03/to-the-great-city/\">\"To The Great City!\"</a>. July 3, 2017. Slate Star Codex. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref16\">↩</a></li><li><a href=\"http://slatestarcodex.com/2017/07/03/to-the-great-city/#comment-518132\">http://slatestarcodex.com/2017/07/03/to-the-great-city/#comment-518132</a><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref17\">↩</a></li><li>Scott Alexander. <a href=\"http://slatestarcodex.com/2017/07/03/ot79-open-road/\">\"OT79: Open Road\"</a>. July 5, 2017. Slate Star Codex. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref18\">↩</a></li><li>Zvi Mowshowitz. <a href=\"https://thezvi.wordpress.com/2017/04/04/responses-to-tyler-cohen-on-rationality/\">\"Responses to Tyler Cowen on Rationality\"</a>. September 10, 2017. Don't Worry About the Vase. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref19\">↩</a></li><li>Katja Grace. <a href=\"https://meteuphoric.wordpress.com/2017/07/20/be-my-neighbor/\">\"Be my neighbor\"</a>. July 20, 2017. Meteuphoric. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref20\">↩</a></li><li>Alyssa Vance. <a href=\"https://www.facebook.com/alyssamvance/posts/10214122240439702\">\"'Why is the best place to live in the world so much...\"</a>. August 5, 2017. Facebook. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref21\">↩</a></li><li><a href=\"https://www.ribbonfarm.com/2017/08/17/the-premium-mediocre-life-of-maya-millennial/#more-6047\">https://www.ribbonfarm.com/2017/08/17/the-premium-mediocre-life-of-maya-millennial/#more-6047</a><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref22\">↩</a></li><li>mingyuan. <a href=\"https://www.lesswrong.com/posts/bDnFhJBcLQvCY3vJW/what-do-we-mean-by-meetups\">\"What Do We Mean By 'Meetups'?\"</a>. February 7, 2018. LessWrong. Retrieved February 12, 2018.<a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref23\">↩</a></li></ol>",
    "description_length": 15044,
    "viewCount": 186,
    "parentTagId": "community-lesswrong"
  },
  {
    "core-tag": "Practical",
    "_id": "4kQXps8dYsKJgaayN",
    "name": "Careers",
    "slug": "careers",
    "postCount": 207,
    "description_html": "<p>Posts relating to jobs, career development, etc.</p>",
    "description_length": 55,
    "viewCount": 583,
    "parentTagId": "practical-domains-wellbeing"
  },
  {
    "core-tag": "Practical",
    "_id": "3ee9k6NJfcGzL6kMS",
    "name": "Emotions",
    "slug": "emotions",
    "postCount": 193,
    "description_html": "<p>Contrary to the stereotype, <a href=\"https://www.lesswrong.com/tag/rationality\">rationality</a> doesn't mean denying <strong>emotion</strong>. When emotion is appropriate to the reality of the situation, it should be embraced; only when emotion isn't appropriate should it be suppressed.</p><h2>External links</h2><ul><li><a href=\"http://www.youtube.com/watch?v=tLgNZ9aTEwc\">The Straw Vulcan</a>, a talk introducing rationality, by <a href=\"http://lesswrong.com/user/Julia_Galef/\">Julia Galef</a> (<a href=\"https://www.lesswrong.com/lw/90n/summary_of_the_straw_vulcan/\">summary</a>) [TODO: this video has been removed,<a href=\"https://www.lesswrong.com/tag/emotions/discussion?commentId=qddytd5zQWrHRrPyh\"> find a good replacement</a>]</li><li><a href=\"http://www.overcomingbias.com/2006/12/vulcan_logic.html\">Vulcan Logic</a> by <a href=\"https://en.wikipedia.org/wiki/Hal_Finney_(cypherpunk)\">Hal Finney</a></li></ul><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/alief\">Alief</a></li><li><a href=\"https://www.lesswrong.com/tag/truth-semantics-and-meaning\">Truth</a>, <a href=\"https://www.lesswrong.com/tag/rationality\">Rationality</a></li><li><a href=\"https://www.lesswrong.com/tag/litany-of-tarski\">Litany of Tarski</a></li><li><a href=\"https://www.lesswrong.com/tag/hollywood-rationality\">Hollywood rationality</a></li></ul>",
    "description_length": 1348,
    "viewCount": 390,
    "parentTagId": "practical-domains-wellbeing"
  },
  {
    "core-tag": "Practical",
    "_id": "8nAXyYLu8eT72Hwuh",
    "name": "Exercise (Physical)",
    "slug": "exercise-physical",
    "postCount": 42,
    "description_html": null,
    "description_length": null,
    "viewCount": 326,
    "parentTagId": "practical-domains-wellbeing"
  },
  {
    "core-tag": "Practical",
    "_id": "jgcAJnksReZRuvgzp",
    "name": "Financial Investing",
    "slug": "financial-investing",
    "postCount": 167,
    "description_html": "<p>The <strong>Financial Investing</strong> tag covers concrete personal investment advice, specific investment opportunities (like Bitcoin), and analysis of existing financial investing practices, as well as broad analyses of things like the efficient market hypothesis.</p>",
    "description_length": 275,
    "viewCount": 803,
    "parentTagId": "practical-domains-wellbeing"
  },
  {
    "core-tag": "Practical",
    "_id": "a65Lgr7Q5jqRWHtM6",
    "name": "Gratitude",
    "slug": "gratitude",
    "postCount": 19,
    "description_html": "<p><strong>Gratitude </strong>is the feeling or showing of appreciation. Practices that aim to increase how much one is expressing gratitude (such as <strong>Gratitude Journaling</strong>) seem to have one of the highest correlation with actually increasing happiness and life satisfaction.</p>",
    "description_length": 294,
    "viewCount": 48,
    "parentTagId": "practical-domains-wellbeing"
  },
  {
    "core-tag": "Practical",
    "_id": "Jzm2mYuuDBCNWq8hi",
    "name": "Happiness",
    "slug": "happiness-1",
    "postCount": 67,
    "description_html": "<p>Posts about <strong>Happiness</strong>. One of the tricky things about Happiness is that sometimes directly pursuing it doesn't work or is even counterproductive. Happiness isn't (and can't be) <a href=\"https://www.lesswrong.com/posts/synsRtBKDeAFuo7e3/not-for-the-sake-of-happiness-alone\">the only important thing</a>, but it is nonetheless important. Thus LessWrong dealt a lot with questions about happiness and how to pursue it.</p><blockquote><p>“Happiness is a choice. If you’re so smart, how come you aren’t happy? How come you haven’t figured that out? That’s my challenge to all the people who think they’re so smart and so capable.” - Naval Ravikant</p></blockquote><p>See also: <a href=\"https://www.lesswrong.com/tag/gratitude\">Gratitude</a>, <a href=\"https://www.lesswrong.com/tag/well-being\">Well-being</a></p>",
    "description_length": 826,
    "viewCount": 237,
    "parentTagId": "practical-domains-wellbeing"
  },
  {
    "core-tag": "Practical",
    "_id": "rfZ6DY88ApBDXFpyW",
    "name": "Human Bodies",
    "slug": "human-bodies",
    "postCount": 35,
    "description_html": null,
    "description_length": null,
    "viewCount": 86,
    "parentTagId": "practical-domains-wellbeing"
  },
  {
    "core-tag": "Practical",
    "_id": "92SxJsDZ78ApAGq72",
    "name": "Nutrition",
    "slug": "nutrition",
    "postCount": 89,
    "description_html": "<p>How to optimize your food intake for various desired outcomes; nutrition science, diets, experiments.</p><p>See also: <a href=\"http://lesswrong.com/tag/cooking\">Cooking</a></p>",
    "description_length": 179,
    "viewCount": 294,
    "parentTagId": "practical-domains-wellbeing"
  },
  {
    "core-tag": "Practical",
    "_id": "Q55STnFh6gbSezRuR",
    "name": "Parenting",
    "slug": "parenting",
    "postCount": 187,
    "description_html": "<p><strong>Parenting</strong>, i.e. how to raise children well.</p><p><strong>Related pages: </strong><a href=\"https://www.lesswrong.com/tag/education\">Education</a>, <a href=\"https://www.lesswrong.com/tag/developmental-psychology\">Developmental Psychology</a>, <a href=\"https://www.lesswrong.com/tag/santa-claus\">Santa Claus</a>, <a href=\"https://www.lesswrong.com/tag/family-planning\">family planning</a></p><p><strong>External links: </strong><a href=\"https://en.wikipedia.org/wiki/Baby_sign_language\">Baby sign language</a></p>",
    "description_length": 531,
    "viewCount": 319,
    "parentTagId": "practical-domains-wellbeing"
  },
  {
    "core-tag": "Practical",
    "_id": "YSyvvi4uXvxAARX2D",
    "name": "Slack",
    "slug": "slack",
    "postCount": 41,
    "description_html": "<p><strong>Slack</strong> is absence of binding constraints on behavior. The term is usually capitalized to distinguish it from the ordinary English meaning. Not to be confused with the communication app by the same name.</p><p>From the post which introduced this usage, <a href=\"https://www.lessestwrong.com/posts/yLLkWMDbC9ZNKbjDG/slack\">Slack</a><strong>:</strong></p><blockquote><p><i>Poor is the person without Slack. Lack of Slack compounds and traps.</i></p><p><i>Slack means margin for error. You can relax.</i></p><p><i>Slack allows pursuing opportunities. You can explore. You can trade.</i></p><p><i>Slack prevents desperation. You can avoid bad trades and wait for better spots. You can be efficient.</i></p><p><i>Slack permits planning for the long term. You can invest.</i></p><p><i>Slack enables doing things for your own amusement. You can play games. You can have fun.</i></p><p><i>Slack enables doing the right thing. Stand by your friends. Reward the worthy. Punish the wicked. You can have a code.</i></p><p><i>Slack presents things as they are without concern for how things look or what others think. You can be honest.</i></p><p><i>You can do some of these things, and choose not to do others. Because you don’t have to.</i></p><p><i>Only with slack can one be a righteous dude.</i></p><p><i>Slack is life.</i></p></blockquote><p><strong>Related Sequence:</strong> <a href=\"https://www.lesswrong.com/s/HXkpm9b8o964jbQ89\">Slack and the Sabbath</a></p>",
    "description_length": 1473,
    "viewCount": 227,
    "parentTagId": "practical-domains-wellbeing"
  },
  {
    "core-tag": "Practical",
    "_id": "HLoxy2feb2PYqooom",
    "name": "Sleep",
    "slug": "sleep",
    "postCount": 42,
    "description_html": null,
    "description_length": null,
    "viewCount": 215,
    "parentTagId": "practical-domains-wellbeing"
  },
  {
    "core-tag": "Practical",
    "_id": "XqykXFKL9t38pbSEm",
    "name": "Well-being",
    "slug": "well-being",
    "postCount": 131,
    "description_html": null,
    "description_length": null,
    "viewCount": 229,
    "parentTagId": "practical-domains-wellbeing"
  },
  {
    "core-tag": "Practical",
    "_id": "ZnHkaTkxukegSrZqE",
    "name": "Cryonics",
    "slug": "cryonics",
    "postCount": 140,
    "description_html": "<p><strong>Cryonics</strong> is the practice of preserving people who are dying in liquid nitrogen soon after their heart stops. The idea is that most of your brain's information content is still intact right after you've \"died\", i.e. medical death or legal death. If humans invent molecular nanotechnology or brain emulation techniques, it may be possible to reconstruct the consciousness of cryopreserved patients.</p><p><i>Related</i>: <a href=\"https://www.lessestwrong.com/tag/life-extension\">Life Extension</a>, a more general tag about ways to avoid death.</p><h2>Cryonics-associated issues commonly raised on LessWrong</h2><p><strong>Pro-cryonics points</strong></p><ul><li>Advanced reductionism/physicalism (because of the issues associated with <a href=\"https://lessestwrong.com/tag/personal-identity\">identifying a person</a> with continuity of brain information).</li><li>Whether an extended healthy lifespan is worthwhile (relates to <a href=\"https://lessestwrong.com/tag/fun-theory\">Fun Theory</a>, religious rationalizations for 70-year lifespans, \"sour grapes\" rationalizations for why death is actually a good thing).</li><li>The \"<a href=\"https://lessestwrong.com/tag/shut-up-and-multiply\">shut up and multiply</a>\" aspect of spending $300/year (as Eliezer Yudkowsky quotes his costs for Cryonics Institute membership ($125/year) plus term life insurance ($180/year)) for a probability (how large being widely disputed) of obtaining many more years of lifespan. For this reason, cryonics advocates regard it as an <i>extreme case</i> of failure at rationality - a low-hanging fruit by which millions of deaths per year could be prevented at low cost.</li></ul><p><strong>Anti-cryonics points</strong></p><ul><li>Cognitive biases contributing to emotional prejudice in favor of cryonics (optimistic bias, motivated cognition).</li><li>The <a href=\"https://lessestwrong.com/tag/conjunction-fallacy\">multiply chained nature</a> of the probabilities involved in cryonics, and whether the final expected utility is worth the cost.</li><li>Money spent on cryonics could, arguably, be better spent on <a href=\"https://lessestwrong.com/lw/3gj/efficient_charity_do_unto_others/\">efficient charity</a>.</li><li><a href=\"https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\">S-risks</a>/<a href=\"https://arbital.com/p/hyperexistential_separation/\">hyperexistential risks</a>; The far future may turn out to be dystopian and have negative expected value.</li></ul><h2>Notable Posts</h2><ul><li><a href=\"http://www.overcomingbias.com/2008/12/we-agree-get-froze.html\">We Agree: Get Froze</a> by <a href=\"https://lessestwrong.com/tag/robin-hanson\">Robin Hanson</a>. \"My co-blogger Eliezer and I may disagree on AI fooms, but we agree on something quite contrarian and, we think, huge: More likely than not, most folks who die today didn't have to die! ... It seems far more people read this blog daily than have ever signed up for cryonics. While it is hard to justify most medical procedures using standard health economics calculations, such calculations say that at today's prices cryonics seems a good deal even if you think there's only a 5% chance it'll work.\"</li><li><a href=\"https://lessestwrong.com/lw/wq/you_only_live_twice/\">You Only Live Twice</a> by <a href=\"https://lessestwrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky</a>. \"My co-blogger Robin and I may disagree on how fast an AI can improve itself, but we agree on an issue that seems much simpler to us than that: At the point where the current legal and medical system gives up on a patient, they aren't really dead.\"</li><li><a href=\"https://lessestwrong.com/lw/z0/the_pascals_wager_fallacy_fallacy/\">The Pascal's Wager Fallacy Fallacy</a> - the fallacy of Pascal's Wager combines a high payoff with a <a href=\"https://lessestwrong.com/tag/privileging-the-hypothesis\">privileged hypothesis</a>, one with low prior probability and no particular reason to believe it. Perceptually seeing an instance of \"Pascal's Wager\" <i>just</i> from the high payoff, even when the probability is not small, is the Pascal's Wager Fallacy Fallacy.</li><li><a href=\"https://lessestwrong.com/lw/1mc/normal_cryonics/\">Normal Cryonics</a> - On the shift of perspective that came from attending a gathering of normal-seeming young cryonicists.</li><li><a href=\"https://lessestwrong.com/lw/1mh/that_magical_click/\">That Magical Click</a> - What is the unexplained process whereby some people get cryonics, or other frequently-derailed chains of thought, in a very short time?</li><li><a href=\"https://lessestwrong.com/lw/r9/quantum_mechanics_and_personal_identity/\">Quantum Mechanics and Personal Identity</a> by <a href=\"https://lessestwrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky</a>. A shortened index into the <a href=\"http://www.overcomingbias.com/2008/06/the-quantum-phy.html\">Quantum Physics Sequence</a> describing only the prerequisite knowledge to understand the statement that \"science can rule out a notion of personal identity that depends on your being composed of the same atoms - because modern physics has taken the concept of 'same atom' and thrown it out the window. There <i>are</i> no little billiard balls with individual identities. It's experimentally ruled out.\" The key post in this sequence is <a href=\"http://www.overcomingbias.com/2008/06/timeless-identi.html\">Timeless Identity</a>, in which \"Having used physics to completely trash all naive theories of identity, we reassemble a conception of persons and experiences from what is left\" but this finale might make little sense without the prior discussion.</li><li><a href=\"http://www.overcomingbias.com/2009/03/break-cryonics-down.html\">Break Cryonics Down</a> by <a href=\"https://lessestwrong.com/tag/robin-hanson\">Robin Hanson</a> - tries to identify some of the chained probabilities involved in cryonics.</li><li><a href=\"https://lessestwrong.com/lw/hv/third_alternatives_for_afterlifeism/\">Third Alternatives for Afterlife-ism</a> by <a href=\"https://lessestwrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky</a> - explains why cryonics is a <a href=\"https://lessestwrong.com/tag/third-option\">third option</a> in the dilemma about whether we should tell <a href=\"https://wiki.lesswrong.com/wiki/noble_lie\">noble lies</a> about an afterlife, to prevent people from getting depressed by not believing in an afterlife.</li><li><a href=\"https://lessestwrong.com/lw/1r0/a_survey_of_anticryonics_writing/\">A survey of anti-cryonics writing</a> by <a href=\"https://lessestwrong.com/tag/ciphergoth\">ciphergoth</a> - an attempt to find quality criticism of cryonics, with a surprising result that \"there is not one person who has ever taken the time to read and understand cryonics claims in any detail, still considers it pseudoscience, and has written a paper, article or even a blog post to rebut anything that cryonics advocates actually say\".</li></ul><h2>External links</h2><ul><li><a href=\"http://waitbutwhy.com/2016/03/cryonics.html\">Why Cryonics Makes Sense, WaitButWhy</a></li><li><a href=\"http://www.benbest.com/cryonics/CryoFAQ.html\">Cryonics Institute FAQ</a></li><li><a href=\"http://www.alcor.org/FAQs/index.html\">Alcor Life Extension Foundation FAQ</a></li><li><a href=\"http://www.alcor.org/sciencefaq.htm\">Alcor FAQ for scientists</a></li></ul><h2>See also</h2><ul><li><a href=\"https://lessestwrong.com/tag/exploratory-engineering\">Exploratory engineering</a>, <a href=\"https://lessestwrong.com/tag/absurdity-heuristic\">Absurdity heuristic</a></li><li><a href=\"https://lessestwrong.com/tag/status-quo-bias\">Status quo bias</a>, <a href=\"https://lessestwrong.com/tag/reversal-test\">Reversal test</a></li><li><a href=\"https://lessestwrong.com/tag/signaling\">Signaling</a>, <a href=\"https://lessestwrong.com/tag/near-far-thinking\">Near/far thinking</a></li><li><a href=\"https://lessestwrong.com/tag/death\">Death</a></li></ul>",
    "description_length": 7895,
    "viewCount": 364,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "3ee9k6NJfcGzL6kMS",
    "name": "Emotions",
    "slug": "emotions",
    "postCount": 193,
    "description_html": "<p>Contrary to the stereotype, <a href=\"https://www.lesswrong.com/tag/rationality\">rationality</a> doesn't mean denying <strong>emotion</strong>. When emotion is appropriate to the reality of the situation, it should be embraced; only when emotion isn't appropriate should it be suppressed.</p><h2>External links</h2><ul><li><a href=\"http://www.youtube.com/watch?v=tLgNZ9aTEwc\">The Straw Vulcan</a>, a talk introducing rationality, by <a href=\"http://lesswrong.com/user/Julia_Galef/\">Julia Galef</a> (<a href=\"https://www.lesswrong.com/lw/90n/summary_of_the_straw_vulcan/\">summary</a>) [TODO: this video has been removed,<a href=\"https://www.lesswrong.com/tag/emotions/discussion?commentId=qddytd5zQWrHRrPyh\"> find a good replacement</a>]</li><li><a href=\"http://www.overcomingbias.com/2006/12/vulcan_logic.html\">Vulcan Logic</a> by <a href=\"https://en.wikipedia.org/wiki/Hal_Finney_(cypherpunk)\">Hal Finney</a></li></ul><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/alief\">Alief</a></li><li><a href=\"https://www.lesswrong.com/tag/truth-semantics-and-meaning\">Truth</a>, <a href=\"https://www.lesswrong.com/tag/rationality\">Rationality</a></li><li><a href=\"https://www.lesswrong.com/tag/litany-of-tarski\">Litany of Tarski</a></li><li><a href=\"https://www.lesswrong.com/tag/hollywood-rationality\">Hollywood rationality</a></li></ul>",
    "description_length": 1348,
    "viewCount": 390,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "eamWQNQ2dPYWEwhqr",
    "name": "Goal Factoring",
    "slug": "goal-factoring",
    "postCount": 17,
    "description_html": "<html><head></head><body><p><strong>Goal Factoring </strong>is a rationality technique for planning which proceeds by first identifying the underlying goals motivating one or more behaviors and then searching for alternative sets of behaviors that better accomplish the goals.&nbsp;</p><p>For example, someone might refactor the two behaviors of <i>going to the gym </i>and <i>browsing Facebook </i>into the single behavior of <i>play tennis with my friend </i>which more efficiently and effectively accomplishes the underlying goals of <i>have social interaction </i>and <i>get exercise</i>.</p></body></html>",
    "description_length": 610,
    "viewCount": 410,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "5Whwix4cZ3p5otshm",
    "name": "Habits",
    "slug": "habits",
    "postCount": 53,
    "description_html": "<p>A <strong>habit</strong> is a routine of behavior that is repeated regularly and tends to occur subconsciously. Creating and maintaining useful habits is a core rationality practice as is getting rid of unhelpful habits.</p>",
    "description_length": 227,
    "viewCount": 167,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "BhrpjXqGuke5GnF6g",
    "name": "Hamming Questions",
    "slug": "hamming-questions",
    "postCount": 27,
    "description_html": null,
    "description_length": null,
    "viewCount": 453,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "iTe27Ced8s8bGuvMK",
    "name": "Intellectual Progress (Individual-Level)",
    "slug": "intellectual-progress-individual-level",
    "postCount": 50,
    "description_html": null,
    "description_length": null,
    "viewCount": 243,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "Tg9aFPFCPBHxGABRr",
    "name": "Life Improvements",
    "slug": "life-improvements",
    "postCount": 85,
    "description_html": "<p>Life-hacks, eliminating trivial inconveniences, process improvements, purchases that save you a minute a day, etc</p><p>Found most often on posts under Practical</p>",
    "description_length": 168,
    "viewCount": 439,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "AiNyf5iwbpc7mehiX",
    "name": "Meditation",
    "slug": "meditation",
    "postCount": 107,
    "description_html": null,
    "description_length": null,
    "viewCount": 332,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "FGfgzGpPTtKEqSrDm",
    "name": "More Dakka",
    "slug": "more-dakka",
    "postCount": 25,
    "description_html": "<p><strong>More Dakka </strong>is the technique of throwing more resources at a problem to see if you get better results.&nbsp;</p><p>Originally, More Dakka is the <a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Main/MoreDakka\">trope</a> of solving problems by unloading as many rounds of ammunition at them as possible. In the rationalist community it was popularized by <a href=\"https://www.lesswrong.com/posts/z8usYeKX7dtTWsEnk/more-dakka\">Zvi </a>to have the above meaning.</p>",
    "description_length": 481,
    "viewCount": 249,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "puBcCq7aRwKoa7pXX",
    "name": "Note-Taking",
    "slug": "note-taking",
    "postCount": 26,
    "description_html": "<p>Taking notes is writing that's primarily for yourself -- whether for memory, or for study, or for reference, or to generate or develop ideas.&nbsp;</p><p><strong>Related Pages: </strong><a href=\"https://www.lesswrong.com/tag/zettelkasten\">Zettelkasten</a>, <a href=\"https://www.lesswrong.com/tag/scholarship-and-learning\">Scholarship &amp; Learning</a>, <a href=\"https://www.lesswrong.com/tag/spaced-repetition\">Spaced Repetition</a></p>",
    "description_length": 440,
    "viewCount": 277,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "KoXbd2HmbdRfqLngk",
    "name": "Planning & Decision-Making",
    "slug": "planning-and-decision-making",
    "postCount": 132,
    "description_html": null,
    "description_length": null,
    "viewCount": 199,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "NfMQK5kiYKgg7r9cD",
    "name": "Sabbath",
    "slug": "sabbath",
    "postCount": 5,
    "description_html": null,
    "description_length": null,
    "viewCount": 53,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "AodfCFefLAuwDyj7Z",
    "name": "Self Experimentation",
    "slug": "self-experimentation",
    "postCount": 78,
    "description_html": null,
    "description_length": null,
    "viewCount": 104,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "fR7QfYx4JA3BnptT9",
    "name": "Skill Building",
    "slug": "skill-building",
    "postCount": 87,
    "description_html": "<p><strong>Skill Building</strong> is the meta-skill of getting good at things i.e. developing procedural knowledge.</p><p>Subtopics may include: Fast feedback loops, Choosing the correct level of challenge, studying people better than you to pick up their techniques or assimilate their style or philosophy, synthesizing advice from many tutorials, instructions, or how-tos </p>",
    "description_length": 379,
    "viewCount": 284,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "TkZ7MFwCi4D63LJ5n",
    "name": "Software Tools",
    "slug": "software-tools",
    "postCount": 203,
    "description_html": "<p>Specific pieces of software (downloadable or cloud/browser-based) that may be of interest to people on this site. The focus is on software with a practical application: for games, see <a href=\"https://www.lesswrong.com/tag/gaming-videogames-tabletop\">Gaming (videogames/tabletop)</a>.</p><p><strong>Related Sequences:</strong> <a href=\"https://www.lesswrong.com/s/vz9Zrj3oBGsttG3Jh\">Kickstarter for Coordinated Action</a></p>",
    "description_length": 428,
    "viewCount": 230,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "H2q58pKG6xFrv8bPz",
    "name": "Spaced Repetition",
    "slug": "spaced-repetition",
    "postCount": 74,
    "description_html": "<p><strong>Spaced Repetition </strong>is a technique for long-term retention of learned material where instead of attempting to memorize by ‘cramming’, memorization can be done far more efficiently by instead spacing out each review, with increasing durations as one learns the item, with the scheduling done by software.<br><br><i>See Also:</i><strong> </strong><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning\">Scholarship &amp; Learning</a></p><h1><strong>The case for Spaced Repetition</strong></h1><p>A good place to learn more about Spaced Repetition is <a href=\"https://www.gwern.net/Spaced-repetition\"><strong>Spaced Repetition for Efficient Learning</strong></a><strong> </strong>by <a href=\"https://www.lesswrong.com/users/gwern\">Gwern</a>:</p><blockquote><p><i>Spaced repetition is a centuries-old psychological technique for efficient memorization &amp; practice of skills where instead of attempting to memorize by ‘cramming’, memorization can be done far more efficiently by instead spacing out each review, with increasing durations as one learns the item, with the scheduling done by software. Because of the greater efficiency of its slow but steady approach, spaced repetition can scale to memorizing hundreds of thousands of items (while crammed items are almost immediately forgotten) and is especially useful for foreign languages &amp; medical studies.</i></p></blockquote><p>The key insight for why spaced repetition should be effective is that you forget things approximately hyperbolically-- reviewing things very soon (as in cramming-style learning) is ineffective because you have not forgotten much yet when you come to a review. In comparison, Spaced Repetition allows you to renew your knowledge precisely as you're about to forget a given fact, giving the review the maximum return-on-investment possible and (over time) flattening the 'forgetting curve' so that the interval between successive reviews gets progressively larger for a given fact.</p><p>Obviously, it's not possible to remind <i>yourself </i>of something precisely when you're about to forget it. Enter Spaced Repetition Software (SRS)! By using the forgetting curve, SRS is able to plan when you need to review each item. You can either create decks yourself, or (for some topics) download from databases. <a href=\"https://apps.ankiweb.net/\">Anki</a> and <a href=\"https://mnemosyne-proj.org/\">Mnemnosyne</a> are two popular free options, and <a href=\"https://www.supermemo.com/en\">SuperMemo</a> is a subscription-based choice.</p><h1><strong>Criticisms</strong></h1><p>Criticisms of Spaced Repetition primarily revolve around the fact that, for it to be effective, knowledge has to be broken down into individual 'pieces' to go onto cards for testing. This is difficult or impossible for some types of knowledge, and may not promote an integrated view, where the structure or hierarchy of the knowledge is clear, as well as other methods. More can be found in the post <a href=\"https://www.lesswrong.com/posts/As9E3HfgED2zkTAfB/a-vote-against-spaced-repetition\">A Vote Against Spaced Repetition</a>.</p><h1><strong>Resources</strong></h1><h2>Supermemo material</h2><ul><li><a href=\"http://supermemo.com/english/contents.htm#Articles\">Many articles on assorted related topics</a></li><li><a href=\"http://supermemo.com/english/contents.htm#Research\">Research on memory and learning</a></li><li><a href=\"http://supermemo.com/help/faq/index.htm\">Frequently asked questions about various aspects of spaced repetition</a></li></ul><h2>Spaced Repetition Decks</h2><p>Decks (links, or for Anki, the names of a deck in the Anki collection) relevant to LW.</p><ul><li><a href=\"http://www.stafforini.com/blog/anki-decks-by-lesswrong-users/\">Anki decks by LW users</a> by Pablo_Stafforini. Comprehensive and up-to-date (as of 2019) list.</li><li><a href=\"https://www.lesswrong.com/lw/3px/anki_deck_for_biases_and_fallacies/\">Anki deck for biases and fallacies</a> by phob</li><li><a href=\"https://www.lesswrong.com/r/discussion/lw/74o/anki_deck_for_cognitive_science_in_one_lesson/\">Deck for Cognitive Science in One Lesson</a></li><li><a href=\"https://www.lesswrong.com/r/discussion/lw/ee6/lesswrong_wiki_as_anki_deck\">LessWrong Wiki as an Anki deck</a> by mapnoterritory</li></ul><h2>SR cards for <a href=\"https://www.lesswrong.com/tag/sequences\">LessWrong Sequences</a></h2><ul><li><a href=\"https://www.lesswrong.com/lw/2e6/spaced_repetition_database_for_the_mysterious/\">Spaced Repetition Database for the Mysterious Answers to Mysterious Questions Sequence</a> by divia</li><li><a href=\"https://www.lesswrong.com/lw/3oq/spaced_repetition_database_for_a_humans_guide_to/\">Spaced Repetition Database for A Human's Guide to Words</a> by divia</li></ul><h2>Other Spaced Repetition Software</h2><ul><li><a href=\"https://vocapp.com/\">VocApp.com</a></li></ul>",
    "description_length": 4858,
    "viewCount": 244,
    "parentTagId": "practical-skills-techniques"
  },
  {
    "core-tag": "Practical",
    "_id": "r7qAjcbfhj2256EHH",
    "name": "Akrasia",
    "slug": "akrasia",
    "postCount": 104,
    "description_html": "<p><strong>Akrasia</strong><span> is the state of acting against one's better judgment. A canonical example is procrastination.&nbsp;</span></p><p>Increasing <a href=\"https://www.lesswrong.com/tag/willpower\">willpower</a> is seen by some as a solution to akrasia. On the other hand, many favor using tools such as <a href=\"https://www.lesswrong.com/tag/internal-double-crux?useTagName=true\">Internal Double Crux</a> to resolve internal mental conflicts until one <i>wants</i> to perform the reflectively endorsed task. The \"resolve internal conflicts\" approach is often related to viewing the mind in terms of <a href=\"https://www.lesswrong.com/tag/subagents\">parts that disagree</a> with each other.</p><h2>See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/attention\">Attention</a></li><li><a href=\"https://www.lesswrong.com/tag/motivations\">Motivations</a></li><li><a href=\"https://www.lesswrong.com/tag/prioritization\">Prioritization</a></li><li><a href=\"https://www.lesswrong.com/tag/procrastination\">Procrastination</a></li><li><a href=\"https://www.lesswrong.com/tag/productivity\">Productivity</a></li><li><a href=\"https://www.lesswrong.com/tag/willpower\">Willpower</a></li><li><a href=\"https://www.lesswrong.com/tag/adaptation-executors\">Adaptation executers</a></li><li><a href=\"https://www.lesswrong.com/tag/trivial-inconvenience\">Trivial inconvenience</a></li><li><a href=\"https://www.lesswrong.com/tag/aversion-ugh-fields\">Aversion/Ugh field</a></li><li><a href=\"https://www.lesswrong.com/tag/compartmentalization\">Compartmentalization</a></li><li><a href=\"https://www.lesswrong.com/tag/preference\">Preference</a></li><li><a href=\"https://www.lesswrong.com/tag/pica\">Pica</a></li></ul><h2>External links</h2><ul><li><a href=\"http://psychology.wikia.com/wiki/Akrasia\">Akrasia</a> at Psychology Wiki</li><li><a href=\"http://plato.stanford.edu/entries/weakness-will/\">Weakness of Will</a>, Stanford Encyclopedia of Philosophy</li><li><a href=\"http://beeminder.com/\">Beeminder</a>, community-member developed tool for commitment via self-imposed financial penalties</li></ul>",
    "description_length": 2091,
    "viewCount": 620,
    "parentTagId": "practical-productivity"
  },
  {
    "core-tag": "Practical",
    "_id": "XSeiautCrZGaQ78fx",
    "name": "Attention",
    "slug": "attention",
    "postCount": 24,
    "description_html": null,
    "description_length": null,
    "viewCount": 134,
    "parentTagId": "practical-productivity"
  },
  {
    "core-tag": "Practical",
    "_id": "iP2X4jQNHMWHRNPne",
    "name": "Motivations",
    "slug": "motivations",
    "postCount": 189,
    "description_html": "<html><head></head><body><p><strong>Motivations</strong> are the reasons why we think and do the things that we do. Related: <strong>Desire, Values</strong>. Many questions can asked about motivation such as: i) what does/could/should motivate people? ii) which stated motivations are true motivations for belief and behavior? iii) which motivations are <i>valid</i> vs <i>invalid</i>? iv) How does motivation even work?&nbsp;</p><p><i>Note: This tag is a work in progress</i></p><p>See also:</p><ul><li><a href=\"www.lesswrong.com/stub\">Inspirational</a></li><li>CEV</li><li>Utility Functions</li><li>Elephant in the Brain by Simler and Hanson</li><li>Signaling</li><li>Multi-Agent Theories of Mind</li><li><strong>Rationalization </strong>is the act of finding reasons to support a desired conclusion rather than reasoning in ways which reach the true conclusion.</li><li><strong>Motivated Cognition </strong>is when one's thinking does not purely follow processes for generating truth, and are instead influenced by desires/motivation to reach certain conclusion.</li></ul><h2>Motivation and Belief</h2><p>In the context of belief, a valid motivation for believing something might be having encountered Bayesian evidence for it; in contrast, simply wishing something were true is a poor motivation for believing and often results <i>motivated reasoning [link need].</i></p><p>The Litanies of Gendlin and Tarsky [links] are often invoked to elicit feels which motivate truth-seeking behaviors.</p><h2>Motivated Cognition, Confirmation Bias, Rationalization</h2><p>...</p><h2>Stated vs Actual Motivation</h2><p>It is no secret that often the reasons people give for their actions and beliefs are probably not the real ones driving their behavior. Is that your real objection? The work of Hanson....Signaling...</p><ul><li>Act of Charity</li><li>Player vs Character</li></ul><h2>The Cognitive Science of Motivation</h2><p>While most people can recognize the feeling of motivation, it is a much more complication question on how agents, particularly humans, implement <i>motivation.</i></p><p>In 20xx, Lukeprog wrote &lt;Neuroscience Review&gt;. Lengthy and thorough. Unknown uptodateness.</p><p>Related to the question of Motivation is subagents. Is one's overall self actually made up of subagents each with their own desires. Kaj Sotala explores this in his Multiagent Theories of Mind Sequences. CFAR techniques: Internal Double Crux are aimed harmonizing between the desires/motivations of different \"parts\" of oneself.</p><h2>Aligning Motivations</h2><ul><li>Overcoming akrasia...</li></ul><h2>Practical Techniques for Motivation</h2><ul><li>Propagating Urges</li><li>Mental Contrasting (external)</li><li>Propagating Urges</li><li>Internal Double Crux</li></ul><p>Habitual Productivity and Nate's Writing</p><p>Something to Protect</p><p>&nbsp;</p><p>See also Motivated Reasoning</p></body></html>",
    "description_length": 2902,
    "viewCount": 152,
    "parentTagId": "practical-productivity"
  },
  {
    "core-tag": "Practical",
    "_id": "dqx5k65wjFfaiJ9sQ",
    "name": "Procrastination",
    "slug": "procrastination",
    "postCount": 41,
    "description_html": "<p><strong>Procrastination</strong> is [TODO: finish tag description]</p>",
    "description_length": 73,
    "viewCount": 309,
    "parentTagId": "practical-productivity"
  },
  {
    "core-tag": "Practical",
    "_id": "udPbn9RthmgTtHMiG",
    "name": "Productivity",
    "slug": "productivity",
    "postCount": 212,
    "description_html": "<h2>Related Tags</h2><ul><li><a href=\"https://www.lesswrong.com/tag/self-improvement\">Self Improvement</a></li><li><a href=\"https://www.lesswrong.com/tag/growth-stories\">Growth Stories</a></li><li><a href=\"https://www.lesswrong.com/tag/quantified-self\">Quantified Self</a></li><li><a href=\"https://www.lesswrong.com/tag/lifelogging\">Lifelogging</a></li><li><a href=\"https://www.lesswrong.com/tag/deliberate-practice\">Deliberate Practice</a></li></ul>",
    "description_length": 450,
    "viewCount": 576,
    "parentTagId": "practical-productivity"
  },
  {
    "core-tag": "Practical",
    "_id": "YrLoz567b553YouZ2",
    "name": "Willpower",
    "slug": "willpower",
    "postCount": 36,
    "description_html": "<p><strong>Willpower</strong> is the ability to overcome urges to do or not some activity– to overcome temptation. Typically there is a sense of coercing oneself to do things despite inner resistance.</p>\n<p>Willpower is of interest those who wish to increase their productivity or otherwise do more thing that they wish to be done. The question then is \"how does one increase willpower?\"</p>\n<p>There is an argument that the use of willpower is undesirable. The use of willpower my constitute a form of <em>inner violence</em> which is in tension with <em>inner</em> <em>alignment</em> of <a href=\"https://www.lessestwrong.com/tag/subagents\">one's parts</a>– a better path to productivity and wellbeing.</p>\n<p><strong>Related:</strong> <a href=\"https://www.lessestwrong.com/tag/akrasia\">Akrasia</a></p>\n<h2>Resources</h2>\n<ul>\n<li>The writings on <a href=\"http://mindingourway.com/\">Minding Our Way</a> concerning productivity.</li>\n</ul>\n",
    "description_length": 941,
    "viewCount": 202,
    "parentTagId": "practical-productivity"
  },
  {
    "core-tag": "Practical",
    "_id": "7oXfRFCR7N22MnuY5",
    "name": "Circling",
    "slug": "circling",
    "postCount": 9,
    "description_html": "<p><strong>Circling </strong>is a group \"meditative\", \"relational\" practice. Typically, a group of people sit in a circle and deliberately focus their attention the emotions and experiences of each participant in the group. Communication is usually restricted to the topic of what the individuals in the Circle are experiencing in the present moment, particular their attitudes, feelings, and reactions to others in the group.</p><p>Circling may offer benefits in greater awareness of oneself, others, and the interpersonal dynamics between the two. Since social relations are so key to human wellbeing and at the heart of so many psychological challenges, Circling can be of key interest to anyone trying optimize themselves. It may also foster better relationships and cooperation with others.</p><p>However, Circling originated outside the LessWrong community and many feel that the practice does not have sufficient evidence behind it for it to be so widely admired within the Rationalist community.</p>",
    "description_length": 1007,
    "viewCount": 93,
    "parentTagId": "practical-interpersonal"
  },
  {
    "core-tag": "Practical",
    "_id": "ZXFpyQWPB5ideFbEG",
    "name": "Conversation (topic)",
    "slug": "conversation-topic",
    "postCount": 135,
    "description_html": "<p>A <strong>conversation </strong>is when two people talk or correspond. Most content here is about <i>how to have good conversations.</i>&nbsp;(<i>This wikitag needs work.)</i><br><br>For records of conversations, see Interviews, Debates,...</p><p>See also:</p><ul><li>Communication</li><li>Communication Cultures</li><li>Relationshops</li><li>Community</li></ul><h2><i>Conversation Halter</i></h2><p>This term was introduced on LessWrong by Eliezer in the <a href=\"https://www.lesswrong.com/posts/wqmmv6NraYv4Xoeyj/conversation-halters\">eponymous post</a>:</p><blockquote><p><i>While working on my book, I found in passing that I'd developed a list of what I started out calling \"stonewalls\", but have since decided to refer to as \"conversation halters\".&nbsp; These tactics of argument are distinguished by their being attempts to cut off the flow of debate - which is rarely the wisest way to think, and should certainly rate an alarm bell.</i></p></blockquote>",
    "description_length": 966,
    "viewCount": 251,
    "parentTagId": "practical-interpersonal"
  },
  {
    "core-tag": "Practical",
    "_id": "AADZcNS24mmSfPp2w",
    "name": "Communication Cultures",
    "slug": "communication-cultures",
    "postCount": 144,
    "description_html": "<p>A <strong>Communication Culture </strong>is a set of norms, expectations, and assumptions that a group of people adopts around communication. It is probable that some Communication Cultures are objectively better than others, but is definite that difficult clashes occur when people operating under different cultures interact.</p><p>Awareness of Communication Cultures is therefore key to getting along with others not perfectly sharing our background and preferences.</p><p>Notable Communication Cultures (these are usually contrasted along some dimension) are: <a href=\"https://www.lessestwrong.com/posts/vs3kzjLhbdKsndnBy/ask-and-guess\">Ask vs Guess</a> (and <a href=\"https://www.lessestwrong.com/posts/rEBXN3x6kXgD4pLxs/tell-culture\">Tell</a>/<a href=\"https://malcolmocean.com/2015/06/reveal-culture/\">Reveal</a>); <a href=\"https://www.lessestwrong.com/posts/LuXb6CZG4x7pDRBP8/wait-vs-interrupt-culture\">Wait vs Interrupt</a>; and <a href=\"https://www.lessestwrong.com/posts/ExssKjAaXEEYcnzPd/conversational-cultures-combat-vs-nurture-v2\">Combat vs Nurture</a>.</p><p>See also: <a href=\"https://www.lesswrong.com/tag/simulacrum-levels\">Simulacrum Levels</a></p><p><i>Tag Status: C-Class</i></p>",
    "description_length": 1202,
    "viewCount": 59,
    "parentTagId": "practical-interpersonal"
  },
  {
    "core-tag": "Practical",
    "_id": "mip7tdAN87Jarkcew",
    "name": "Relationships (Interpersonal)",
    "slug": "relationships-interpersonal",
    "postCount": 196,
    "description_html": "<html><head></head><body><p><strong>Interpersonal Relationships </strong>includes all forms of sustained interaction between people. This topic includes any discussion relating to friendship, romantic relationships, family relationships, business relationships, and so on.</p><p>Related:</p><ul><li>Communication</li><li>Communication Cultures</li><li>Circling</li></ul></body></html>",
    "description_length": 384,
    "viewCount": 594,
    "parentTagId": "practical-interpersonal"
  }
]
